[
  {
    "objectID": "qmd/training/re-train-hub-models.html",
    "href": "qmd/training/re-train-hub-models.html",
    "title": "Sam Foreman",
    "section": "",
    "text": "Re-train HF Hub Models From Scratch Using Finetuning Examples\nHF Transformers has awesome finetuning examples https://github.com/huggingface/transformers/tree/main/examples/pytorch, that cover pretty much any modality and these examples work out of box.\nBut what if you wanted to re-train from scratch rather than finetune.\nHere is a simple hack to accomplish that.\nWe will use facebook/opt-1.3b and we will plan to use bf16 training regime as an example here:\ncat &lt;&lt; EOT &gt; prep-bf16.py\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nimport torch\n\nmname = \"facebook/opt-1.3b\"\n\nconfig = AutoConfig.from_pretrained(mname)\nmodel = AutoModel.from_config(config, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(mname)\n\npath = \"opt-1.3b-bf16\"\n\nmodel.save_pretrained(path)\ntokenizer.save_pretrained(path)\nEOT\nnow run:\npython prep-bf16.py\nThis will create a folder: opt-1.3b-bf16 with everything you need to train the model from scratch. In other words you have a pretrained-like model, except it only had its initializations done and none of the training yet.\nAdjust to script above to use torch.float16 or torch.float32 if that‚Äôs what you plan to use instead.\nNow you can proceed with finetuning this saved model as normal:\npython -m torch.distributed.run \\\n--nproc_per_node=1 --nnode=1 --node_rank=0 \\\n--master_addr=127.0.0.1 --master_port=9901 \\\nexamples/pytorch/language-modeling/run_clm.py --bf16 \\\n--seed 42 --model_name_or_path opt-1.3b-bf16 \\\n--dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 \\\n--per_device_train_batch_size 12 --per_device_eval_batch_size 12 \\\n--gradient_accumulation_steps 1 --do_train --do_eval --logging_steps 10 \\\n--save_steps 1000 --eval_steps 100 --weight_decay 0.1 --num_train_epochs 1 \\\n--adam_beta1 0.9 --adam_beta2 0.95 --learning_rate 0.0002 --lr_scheduler_type \\\nlinear --warmup_steps 500 --report_to tensorboard --output_dir save_dir\nThe key entry being:\n--model_name_or_path opt-1.3b-bf16\nwhere opt-1.3b-bf16 is your local directory you have just generated in the previous step.\nSometimes it‚Äôs possible to find the same dataset that the original model was trained on, sometimes you have to use an alternative dataset.\nThe rest of the hyper-parameters can often be found in the paper or documentation that came with the model.\nTo summarize, this recipe allows you to use finetuning examples to re-train whatever model you can find on the HF hub.\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2024. ‚ÄúPersonal Website.‚Äù February 13, 2024.\nhttps://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md pencil >}}]{.red-text style='font-size: 1.0em;'} Posts",
      "Qmd",
      "Training",
      "Sam Foreman"
    ]
  },
  {
    "objectID": "qmd/training/hparams.html",
    "href": "qmd/training/hparams.html",
    "title": "",
    "section": "",
    "text": "üèãÔ∏è TrainingSelecting Training Hyper-Parameters And Model Initializations",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Selecting Training Hyper-Parameters And Model Initializations"
    ]
  },
  {
    "objectID": "qmd/training/hparams.html#glossary",
    "href": "qmd/training/hparams.html#glossary",
    "title": "",
    "section": "Glossary",
    "text": "Glossary\nTraining jargon uses a multitude of abbreviations and terms, so here are some important for this chapter.\n\nBS: Batch Size - here we mean batch size per gpu, often it is also referred to as MBS (micro-batch-size)\nGBS: Global Batch Size - total batch size per iteration - may include gradient accumulation\nGAS: Gradient Accumulation Steps - how many forward/backward cycles to perform before one full iteration is complete\nTFLOPs: Trillion FLOPs per second - FLOPS\nPP: Pipeline Parallelism",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Selecting Training Hyper-Parameters And Model Initializations"
    ]
  },
  {
    "objectID": "qmd/training/hparams.html#global-batch-size-ramp-up",
    "href": "qmd/training/hparams.html#global-batch-size-ramp-up",
    "title": "",
    "section": "Global Batch Size Ramp Up",
    "text": "Global Batch Size Ramp Up\nIf you intend to train with a very large GBS, with say 1024, or 2048 samples and even higher, when you just start training, it‚Äôs very wasteful to feed such large batch sizes to the model. At this point it‚Äôs totally random and can‚Äôt benefit from having too refined data. Therefore to save data and resources, one often ramps up the global batch size over some period of time.\nIt‚Äôs also important to not start with GBS that is too small, since otherwise the progress won‚Äôt be efficient. When there is too little data the compute (TFLOPS) is inefficient and will slow everything down. This is especially so when Pipeline Parallelism (PP) is used, since the most important thing about PP tuneup is a small GPU idleness bubble, and the smaller the GBS the larger the bubble is.\nFor example, for BLOOM-176B, where we did use PP, after doing throughput benchmarking we found that starting with GBS=16 was incredibly slow (8 TFLOPs), so we eventually started with GBS=192 (73 TFLOPs) and then we ramped up to GBS=2048 (150 TFLOPs) - we increased GBS by 16 every 9_765_625 samples.\n\nSTD Init\nThis hyper parameter is super-important and it requires math to get it right. For details see STD Init.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Selecting Training Hyper-Parameters And Model Initializations"
    ]
  },
  {
    "objectID": "qmd/training/dtype.html",
    "href": "qmd/training/dtype.html",
    "title": "",
    "section": "",
    "text": "üèãÔ∏è TrainingTensor precision / Data types",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Tensor precision / Data types"
    ]
  },
  {
    "objectID": "qmd/training/dtype.html#quarter-half-and-mixed-precision",
    "href": "qmd/training/dtype.html#quarter-half-and-mixed-precision",
    "title": "",
    "section": "Quarter, Half and Mixed Precision",
    "text": "Quarter, Half and Mixed Precision\nfp16\nbf16\nmixed fp16\nmixed bf16\nfp8\n\nGeneral OPs\nLayerNorm-like operations must not do their work in half-precision, or they may lose a lot of data. Therefore when these operations are implemented correctly they do efficient internal work in fp32 and then their outputs are downcast to half-precision. Very often it‚Äôs just the accumulation that is done in fp32, since adding up half-precision numbers is very lossy.\nexample:\n\n\nReduction collectives\nfp16: ok to do in fp16 if loss scaling is in place\nbf16: only ok in fp32\n\n\nGradient accumulation\nbest done in fp32 for both, but definitely for bf16\n\n\nOptimizer step / Vanishing gradients\nwhen adding a tiny gradient to a large number, that addition is often nullified\nfp32 master weights and fp32 optim states\nbf16 master weights and optim states can be done when using Kahan Summation and/or Stochastic rounding",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Tensor precision / Data types"
    ]
  },
  {
    "objectID": "qmd/training/dtype.html#using-fp16-pretrained-model-in-bf16-regime",
    "href": "qmd/training/dtype.html#using-fp16-pretrained-model-in-bf16-regime",
    "title": "",
    "section": "Using fp16-pretrained model in bf16 regime",
    "text": "Using fp16-pretrained model in bf16 regime\nusually fails",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Tensor precision / Data types"
    ]
  },
  {
    "objectID": "qmd/training/dtype.html#using-bf16-pretrained-model-in-fp16-regime",
    "href": "qmd/training/dtype.html#using-bf16-pretrained-model-in-fp16-regime",
    "title": "",
    "section": "Using bf16-pretrained model in fp16 regime",
    "text": "Using bf16-pretrained model in fp16 regime\nwill lose some performance on conversion, but should work - best to finetune a bit",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Tensor precision / Data types"
    ]
  },
  {
    "objectID": "qmd/training/dtype.html#fp8",
    "href": "qmd/training/dtype.html#fp8",
    "title": "",
    "section": "FP8",
    "text": "FP8\nMain paper: FP8 Formats for Deep Learning",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Tensor precision / Data types"
    ]
  },
  {
    "objectID": "qmd/network/benchmarks/results/disable-nvlink.html",
    "href": "qmd/network/benchmarks/results/disable-nvlink.html",
    "title": "",
    "section": "",
    "text": "QmdNetworkBenchmarksResultsDisabling NVLink Benchmark\n\n\n\n\n\nDisabling NVLink Benchmark\nLet‚Äôs compare the training of a gpt2 language model training over a small sample of wikitext.\nThe results are:\n\n\n\nNVlink\nTime\n\n\n\n\nY\n101s\n\n\nN\n131s\n\n\n\nYou can see that NVLink completes the training ~23% faster. In the second benchmark we use NCCL_P2P_DISABLE=1 to tell the GPUs not to use NVLink, which will use PCIe instead.\nWe will use HF Transformers examples.\nHere is the full benchmark code and outputs:\n# DDP w/ NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}\n\n# DDP w/o NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 python -m torch.distributed.launch \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}\nHardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (NV2 in nvidia-smi topo -m) Software: pytorch-1.8-to-be + cuda-11.0 / transformers==4.3.0.dev0\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam and Bekman, Stas},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam, and Stas Bekman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Network",
      "Benchmarks",
      "Results",
      "Disabling NVLink Benchmark"
    ]
  },
  {
    "objectID": "qmd/debug/underflow_overflow.html",
    "href": "qmd/debug/underflow_overflow.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingUnderflow and Overflow Detection",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Underflow and Overflow Detection"
    ]
  },
  {
    "objectID": "qmd/debug/underflow_overflow.html#specific-batch-absolute-mix-and-max-value-tracing",
    "href": "qmd/debug/underflow_overflow.html#specific-batch-absolute-mix-and-max-value-tracing",
    "title": "",
    "section": "Specific batch absolute mix and max value tracing",
    "text": "Specific batch absolute mix and max value tracing\nThe same debugging class can be used for per-batch tracing with the underflow/overflow detection feature turned off.\nLet‚Äôs say you want to watch the absolute min and max values for all the ingredients of each forward call of a given batch, and only do that for batches 1 and 3. Then you instantiate this class as:\ndebug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])\nAnd now full batches 1 and 3 will be traced using the same format as the underflow/overflow detector does.\nBatches are 0-indexed.\nThis is helpful if you know that the program starts misbehaving after a certain batch number, so you can fast-forward right to that area. Here is a sample truncated output for such configuration:\n                  *** Starting batch number=1 ***\nabs min  abs max  metadata\n                  shared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.47e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\n                  decoder.dropout Dropout\n1.60e-07 2.27e+01 input[0]\n0.00e+00 2.52e+01 output\n                  decoder T5Stack\n     not a tensor output\n                  lm_head Linear\n1.01e-06 7.92e+02 weight\n0.00e+00 1.11e+00 input[0]\n6.06e-02 8.39e+01 output\n                   T5ForConditionalGeneration\n     not a tensor output\n\n                  *** Starting batch number=3 ***\nabs min  abs max  metadata\n                  shared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.78e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\nHere you will get a huge number of frames dumped - as many as there were forward calls in your model, so it may or may not what you want, but sometimes it can be easier to use for debugging purposes than a normal debugger. For example, if a problem starts happening at batch number 150. So you can dump traces for batches 149 and 150 and compare where numbers started to diverge.\nYou can also specify the batch number after which to stop the training, with:\ndebug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Underflow and Overflow Detection"
    ]
  },
  {
    "objectID": "qmd/debug/tools.html",
    "href": "qmd/debug/tools.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingDebug Tools",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debug Tools"
    ]
  },
  {
    "objectID": "qmd/debug/tools.html#git-related-tools",
    "href": "qmd/debug/tools.html#git-related-tools",
    "title": "",
    "section": "git-related tools",
    "text": "git-related tools\n\nUseful aliases\nShow a diff of all files modified in the current branch against HEAD:\nalias brdiff=\"def_branch=\\$(git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'); git diff origin/\\$def_branch...\"\nSame, but ignore white-space differences, adding --ignore-space-at-eol or -w:\nalias brdiff-nows=\"def_branch=\\$(git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'); git diff -w origin/\\$def_branch...\"\nList all the files that were added or modified in the current branch compared to HEAD:\nalias brfiles=\"def_branch=\\$(git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'); git diff --name-only origin/\\$def_branch...\"\nOnce we have the list, we can now automatically open an editor to load just added and modified files:\nalias bremacs=\"def_branch=\\$(git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'); emacs \\$(git diff --name-only origin/\\$def_branch...) &\"\n\n\ngit-bisect\n(note to self: this is a sync from the-art-of-debugging/methodology.md which is the true source)\nThe discussed next approach should work for any revision control system that supports bisecting. We will use git bisect in this discussion.\ngit bisect helps to quickly find the commit that caused a certain problem.\nUse case: Say, you were using transformers==4.33.0 and then you needed a more recent feature so you upgraded to the bleed-edge transformers@main and your code broke. There could have been hundreds of commits between the two versions and it‚Äôd be very difficult to find the right commit that lead to the breakage by going through all the commits. Here is how you can quickly find out which commit was the cause.\nfootnote: HuggingFace Transformers is actually pretty good at not breaking often, but given its complexity and enormous size it happens nevertheless and the problems are fixed very quickly once reported. Since it‚Äôs a very popular Machine Learning library it makes for a good debugging use case.\nSolution: Bisecting all the commits between the known good and bad commits to find the one commit that‚Äôs to blame.\nWe are going to use 2 shell terminals: A and B. Terminal A will be used for git bisect and terminal B for testing your software. There is no technical reason why you couldn‚Äôt get away with a single terminal but it‚Äôs easier with 2.\n\nIn terminal A fetch the git repo and install it in devel mode (pip install -e .) into your Python environment.\n\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install -e .\nNow the code of this clone will be used automatically when you run your application, instead of the version you previously installed from PyPi or Conda or elsewhere.\nAlso for simplicity we assume that all the dependencies have already been installed.\n\nnext we launch the bisecting - In terminal A, run:\n\ngit bisect start\n\nDiscover the last known good and the first known bad commits\n\ngit bisect needs just 2 data points to do its work. It needs to know one earlier commit that is known to work (good) and one later commit that is know to break (bad). So if you look at the sequence of commits on a given branch it‚Äôd have 2 known points and many commits around these that are of an unknown quality:\n...... orig_good ..... .... .... .... ..... orig_bad ....\n-------------&gt;----------------&gt;----------------&gt; time\nSo for example if you know that transformers==4.33.0 was good and transformers@main (HEAD) is bad, find which commit is corresponding to the tag 4.33.0 by visiting the releases page and searching for 4.33.0. We find that it was commit with SHA 5a4f340d.\nfootnote: typically the first 8 hex characters are enough to have a unique identifier for a given repo, but you can use the full 40 character string.\nSo now we specify which is the first known good commit:\ngit bisect good 5a4f340d\nand as we said we will use HEAD (latest commit) as the bad one, in which case we can use HEAD instead finding out the corresponding SHA string:\ngit bisect bad HEAD\nIf however you know it broke in 4.34.0 you can find its latest commit as explained above and use that instead of HEAD.\nWe are now all set at finding out the commit that broke things for you.\nAnd after you told git bisect the good and the bad commits it has already switched to a commit somewhere in the middle:\n...... orig_good ..... .... current .... .... ..... orig_bad ........\n-------------&gt;---------------&gt;----------------&gt;----------------&gt; time\nYou can run git log to see which commit it has switched to.\nAnd to remind, we installed this repo as pip install -e . so the Python environment is instantly updated to the current commit‚Äôs code version.\n\nGood or bad\n\nThe next stage is telling git bisect if the current commit is good or bad:\nTo do so in terminal B run your program once.\nThen in terminal A run:\ngit bisect bad\nIf it fails, or:\ngit bisect good\nif it succeeds.\nIf, for example, if the result was bad, git bisect will internally flag the last commit as new bad and will half the commits again, switching to a new current commit:\n...... orig_good ..... current .... new_bad .... ..... orig_bad ....\n-------------&gt;---------------&gt;----------------&gt;----------------&gt; time\nAnd, vice versa, if the result was good, then you will have:\n...... orig_good ..... .... new_good .... current ..... orig_bad ....\n-------------&gt;---------------&gt;----------------&gt;----------------&gt; time\n\nRepeat until no more commits left\n\nKeep repeating step 4 step until the problematic commit is found.\nOnce you finished bisecting, git bisect will tell you which commit was responsible for breaking things.\n...... orig_good ..... .... last_good first_bad .... .. orig_bad ....\n-------------&gt;---------------&gt;----------------&gt;----------------&gt; time\nIf you followed the little commit diagrams, it‚Äôd correspond for thefirst_bad commit.\nYou can then go to https://github.com/huggingface/transformers/commit/ and append the commit SHA to that url which will take you to the commit, (e.g.¬†https://github.com/huggingface/transformers/commit/57f44dc4288a3521bd700405ad41e90a4687abc0 and which will then link to the PR from which it originated. And then you can ask for help by following up in that PR.\nIf your program doesn‚Äôt take too long to run even if there are thousands of commits to search, you are facing n bisecting steps from 2**n so 1024 commits can be searched in 10 steps.\nIf your program is very slow, try to reduce it to something small - ideally a small reproduction program that shows the problem really fast. Often, commenting out huge chunks of code that you deem irrelevant to the problem at hand, can be all it takes.\nIf you want to see the progress, you can ask it to show the current range of remaining commits to check with:\ngit bisect visualize --oneline\n\nClean up\n\nSo now restore the git repo clone to the same state you started from (most likely `HEAD) with:\ngit bisect reset\nand possible reinstall the good version of the library while you report the issue to the maintainers.\nSometimes, the issue emerges from intentional backward compatibility breaking API changes, and you might just need to read the project‚Äôs documentation to see what has changed. For example, if you switched from transformers==2.0.0 to transformers==3.0.0 it‚Äôs almost guaranteed that your code will break, as major numbers difference are typically used to introduce major API changes.\n\nPossible problems and their solutions:\n\n\nskipping\n\nIf for some reason the current commit cannot be tested - it can be skipped with:\ngit bisect skip\nand it git bisect will continue bisecting the remaining commits.\nThis is often helpful if some API has changed in the middle of the commit range and your program starts to fail for a totally different reason.\nYou might also try to make a variation of the program that adapts to the new API, and use it instead, but it‚Äôs not always easy to do.\n\nreversing the order\n\nNormally git expects bad to be after good.\n...... orig_good ..... .... .... .... ..... orig_bad ....\n-------------&gt;---------------&gt;----------------&gt;----------------&gt; time\nNow, if bad happens before good revision order-wise and you want to find the first revision that fixed a previously existing problem - you can reverse the definitions of good and bad - it‚Äôd be confusing to work with overloaded logic states, so it‚Äôs recommended to use a new set of states instead - for example, fixed and broken - here is how you do that.\ngit bisect start --term-new=fixed --term-old=broken\ngit bisect fixed\ngit bisect broken 6c94774\nand then use:\ngit fixed / git broken\ninstead of:\ngit good / git bad\n\ncomplications\n\nThere are sometimes other complications, like when different revisions‚Äô dependencies aren‚Äôt the same and for example one revision may require numpy=1.25 and the other numpy=1.26. If the dependency package versions are backward compatible installing the newer version should do the trick. But that‚Äôs not always the case. So sometimes one has to reinstall the right dependencies before re-testing the program.\nSometimes, it helps when there is a range of commits that are actually broken in a different way, you can either find a range of good...bad commits that isn‚Äôt including the other bad range, or you can try to git bisect skip the other bad commits as explained earlier.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debug Tools"
    ]
  },
  {
    "objectID": "qmd/debug/nccl-performance-debug.html",
    "href": "qmd/debug/nccl-performance-debug.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingNCCL: Debug and Performance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "NCCL: Debug and Performance"
    ]
  },
  {
    "objectID": "qmd/debug/nccl-performance-debug.html#nccl-environment-variables",
    "href": "qmd/debug/nccl-performance-debug.html#nccl-environment-variables",
    "title": "",
    "section": "NCCL Environment Variables",
    "text": "NCCL Environment Variables\nThe full list can be found here. That list is long but many of those variables are no longer in use.\n\nDebug Environment Variables\nThe following env vars are most useful during debugging NCCL-related issues such as hanging and crashing.\n\nNCCL_DEBUG\nThis is the most commonly used env var to debug networking issues.\nValues: - VERSION - Prints the NCCL version at the start of the program. - WARN - Prints an explicit error message whenever any NCCL call errors out. - INFO - Prints debug information - TRACE - Prints replayable trace information on every call.\nFor example:\nNCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\nThis will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported.\nAnd NCCL_DEBUG_FILE should be very useful when using NCCL_DEBUG as the information is copious especially if using many nodes.\n\n\nNCCL_DEBUG_FILE\nWhen using NCCL_DEBUG env var, redirect all NCCL debug logging output to a file.\nThe default is stdout. When using many GPUs it can be very useful to save each process‚Äô debug info into its own log file, which can be done like so:\nNCCL_DEBUG_FILE=/path/to/nccl-log.%h.%p.txt\n\n%h is replaced with the hostname\n%p is replaced with the process PID.\n\nIf you then need to analyse hundreds of these at once, here are some useful shortcuts:\n\ngrep for a specific match and also print the file and line number where it was found:\n\ngrep -n \"Init COMPLETE\" nccl-log*\n\nshow tail -1 of all nccl log files followed by the name of each file\n\nfind . -name \"nccl*\" -exec sh -c 'echo \"$(tail -1 \"$1\") ($1)\"' _ {} \\;\n\n\nNCCL_DEBUG_SUBSYS\nNCCL_DEBUG_SUBSYS used in combination with NCCL_DEBUG tells the latter which subsystems to show. Normally you don‚Äôt have to specify this variable, but sometimes the developers helping you may ask to limit the output to only some sub-systems, for example:\nNCCL_DEBUG_SUBSYS=INIT,GRAPH,ENV,TUNING\n\n\nNCCL_P2P_DISABLE\nDisables P2P comms - e.g.¬†NVLink won‚Äôt be used if there is one and the performance will be much slower as a result of that.\n\n\nNCCL_SOCKET_IFNAME\nThis one is very useful if you have multiple network interfaces and you want to choose a specific one to be used.\nBy default NCCL will try to use the fastest type of an interface, which is typically ib (InfiniBand).\nBut say you want to use an Ethernet interface instead then you can override with:\nNCCL_SOCKET_IFNAME=eth\nThis env var can be used at times to debug connectivity issues, if say one of the interfaces is firewalled, and perhaps the others aren‚Äôt and can be tried instead. Or if you are not sure whether some problem is related to the network interface or something else, so it helps to test other interfaces to invalidate that the issue comes from network.\n\n\n\nPerformance-Oriented Environment Variables\nThe following env vars are used primarily to tune up performance.\n\nNCCL_ALGO\nThis one defines which algorithms NCCL will use. Typically it‚Äôs one of tree, ring, collnetdirect and collnetchain.\nI was asking questions about how a user can do the optimization and was told at this NCCL Issue that basically the user shouldn‚Äôt try to optimize anything as NCCL has a ton of smart algorithms inside that will try to automatically switch from one algorithm to another depending on a concrete situation.\nSylvain Jeaugey shared:\n\nThere used to be a static threshold, but it‚Äôs been replaced by a more complex tuning system. The new system builds a model of the latency and bandwidth of each algorithm/protocol combination (that‚Äôs many, many combinations) and decides which one should perform best depending on the size. So there is no longer an env var and a static value, which is good because the performance of each algorithm depends on the number of nodes and number of GPUs per node and therefore we need to navigate a 2D space of algo/protocols which isn‚Äôt easy. You can always force one algorithm with NCCL_ALGO=TREE and NCCL_ALGO=RING and see what performance you get and whether NCCL switches at the right point. I know it‚Äôs hard to understand, but it‚Äôs also the best solution we found to have the best performance across all platforms and users without users having to manually tune the switch points. Downside is, if you want to manually tune things, you can‚Äôt.\n\nIf you use NCCL_ALGO you need to list the algorithms to consider, but otherwise you have no control over it. So, really, this is only useful if you want to make sure that one of the algorithms isn‚Äôt used.\nWhen asking about which algorithm is better, I received:\n\nRoughly speaking, ring is superior in terms of peak bandwidth (except on 2 nodes), tree is superior in terms of base latency (especially as we scale). Bandwidth = Size / Time, so whether you look at the time or the bandwidth for a given size, it will be a combination of both the peak bandwidth and the base latency. For a fixed size, as you scale, the base latency of ring will become prevalent and tree will be better.\n\nThere is also a new algo, named NVLS, which if NVLink SHARP is available will run faster than NVLink itself, e.g.¬†with NVLink 4.0 (450GBps) one can clock 480GBps doing all-reduce benchmarks. They are working on the inter-node version of that which requires IB or RoCE - this new algo is not documented anywhere as of this writing.\nAnd finally, if you would like to know which algo is being used - you can‚Äôt - see this answer. So if you want to know which algo gives which throughput you will have to try them all explicitly by setting NCCL_ALGO env var and then you‚Äôd know which one was chosen. Or you can edit and recompile NCCL as suggested in that same answer, but you won‚Äôt want this in production.\n\n\nNCCL_CROSS_NIC\nThe NCCL_CROSS_NIC variable controls whether NCCL should allow rings/trees to use different NICs, causing inter-node communication to use different NICs on different nodes.\nTo maximize inter-node communication performance when using multiple NICs, NCCL tries to communicate between same NICs between nodes, to allow for network design where each NIC from each node connects to a different network switch (network rail), and avoid any risk of traffic flow interference. The NCCL_CROSS_NIC setting is therefore dependent on the network topology, and in particular depending on whether the network fabric is rail-optimized or not.\nThis has no effect on systems with only one NIC.\nValues accepted:\n\n0: Always use the same NIC for the same ring/tree, to avoid crossing network rails. Suited for networks with per NIC switches (rails), with a slow inter-rail connection. Note there are corner cases for which NCCL may still cause cross-rail communication, so rails still need to be connected at the top.\n1: Do not attempt to use the same NIC for the same ring/tree. This is suited for networks where all NICs from a node are connected to the same switch, hence trying to communicate across the same NICs does not help avoiding flow collisions.\n2: (Default) Try to use the same NIC for the same ring/tree, but still allow for it if it would result in better performance.\n\n\n\n\nExtrapolating benchmarks from several nodes to many\nAs it‚Äôs often not easy to benchmark hundreds of nodes, often we try to benchmark interconnect performance using, say, 4 nodes. I wasn‚Äôt sure whether this would give the correct indication for when 40 or 400 nodes will be used so I asked about it here and the answer was:\n\nExtrapolating at scale is not that hard for ring and tree (we have a function in tuning.cc predicting it, based on the ring linear latency and the tree log latency with reduced BW). Now as you scale, there are many factors which may cause your real performance to be very far off the prediction, like routing. Also note on an IB network you‚Äôll be able to use SHARP; that way your latency stays mostly constant as you scale, your bandwidth doesn‚Äôt degrade much either, and you‚Äôre always better than both ring and tree.\n\n\n\nCounting NCCL calls\nEnable NCCL debug logging for subsystems - collectives:\nexport NCCL_DEBUG=INFO\nexport NCCL_DEBUG_SUBSYS=COLL\nif you‚Äôre working in a slurm environment with many nodes you probably want to perform this only on rank 0, like so:\nif [[ $SLURM_PROCID == \"0\" ]]; then\n  export NCCL_DEBUG=INFO\n  export NCCL_DEBUG_SUBSYS=COLL\nfi\nAssuming your logs were all sent to main_log.txt, you can then count how many of each collective call were performed with:\ngrep -a \"NCCL INFO Broadcast\" main_log.txt     | wc -l\n2590\ngrep -a \"NCCL INFO AllReduce\" main_log.txt     | wc -l\n5207\ngrep -a \"NCCL INFO AllGather\" main_log.txt     | wc -l\n1849749\ngrep -a \"NCCL INFO ReduceScatter\" main_log.txt | wc -l\n82850\nIt might be a good idea to first isolate a specific stage of the training, as loading and saving will have a very different pattern from training iterations.\nSo I typically first slice out one iteration. e.g.¬†if each iteration log starts with: iteration: ... then I‚Äôd first do:\ncsplit main_log.txt '/iteration: /' \"{*}\"\nand then analyse one of the resulting files that correspond to the iterations. By default it will be named something like xx02.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "NCCL: Debug and Performance"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/nvidia/debug.html",
    "href": "qmd/compute/accelerator/nvidia/debug.html",
    "title": "Troubleshooting NVIDIA GPUs",
    "section": "",
    "text": "No hardware is perfect, sometimes due to the manufacturing problems or due to tear and wear (especially because of exposure to high heat), GPUs are likely to encounter various hardware issues. A lot of these issues get corrected automatically without needing to really understand what‚Äôs going on. If the application continues running usually there is nothing to worry about. If the application crashes due to a hardware issue it‚Äôs important to understand why this is so and how to act on it.\nA normal user who uses a handful of GPUs is likely to never need to understand GPU-related hardware issues, but if you come anywhere close to massive ML training where you are likely to use hundreds to thousands of GPUs it‚Äôs certain that you‚Äôd want to understand about different hardware issues.\nIn your system logs you are likely to see occasionally Xid Errors like:\nNVRM: Xid (PCI:0000:10:1c): 63, pid=1896, Row Remapper: New row marked for remapping, reset gpu to activate.\nTo get those logs one of the following ways should work:\nsudo grep Xid /var/log/syslog\nsudo dmesg -T | grep Xid\nTypically, as long as the training doesn‚Äôt crash, these errors often indicate issues that automatically get corrected by the hardware.\nThe full list of Xid Errors and their interpretation can be found here.\nYou can run nvidia-smi -q and see if there are any error counts reported. For example, in this case of Xid 63, you will see something like:\nTimestamp                                 : Wed Jun  7 19:32:16 2023\nDriver Version                            : 510.73.08\nCUDA Version                              : 11.6\n\nAttached GPUs                             : 8\nGPU 00000000:10:1C.0\n    Product Name                          : NVIDIA A100-SXM4-80GB\n    [...]\n    ECC Errors\n        Volatile\n            SRAM Correctable              : 0\n            SRAM Uncorrectable            : 0\n            DRAM Correctable              : 177\n            DRAM Uncorrectable            : 0\n        Aggregate\n            SRAM Correctable              : 0\n            SRAM Uncorrectable            : 0\n            DRAM Correctable              : 177\n            DRAM Uncorrectable            : 0\n    Retired Pages\n        Single Bit ECC                    : N/A\n        Double Bit ECC                    : N/A\n        Pending Page Blacklist            : N/A\n    Remapped Rows\n        Correctable Error                 : 1\n        Uncorrectable Error               : 0\n        Pending                           : Yes\n        Remapping Failure Occurred        : No\n        Bank Remap Availability Histogram\n            Max                           : 639 bank(s)\n            High                          : 1 bank(s)\n            Partial                       : 0 bank(s)\n            Low                           : 0 bank(s)\n            None                          : 0 bank(s)\n[...]\nHere we can see that Xid 63 corresponds to:\nECC page retirement or row remapping recording event\nwhich may have 3 causes: HW Error / Driver Error / FrameBuffer (FB) Corruption\nThis error means that one of the memory rows is malfunctioning and that upon either reboot and/or a gpu reset one of the 640 spare memory rows (in A100) will be used to replace the bad row. Therefore we see in the report above that only 639 banks remain (out of 640).\nThe Volatile section of the ECC Errors report above refers to the errors recorded since last reboot/GPU reset. The Aggregate section records the same error since the GPU was first used.\nNow, there are 2 types of errors - Correctable and Uncorrectable. The correctable one is a Single Bit ECC Error (SBE) where despite memory being faulty the driver can still recover the correct value. The uncorrectable one is where more than one bit is faulty and it‚Äôs called Double Bit ECC Error (DBE). Typically, the driver will retire whole memory pages if 1 DBE or 2 SBE errors occur at the same memory address. For full information see this document\nA correctable error will not impact the application, a non-correctable one will crash the application. The memory page containing the uncorrectable ECC error will be blacklisted and not accessible until the GPU is reset.\nIf there are page scheduled to be retired you will see something like this in the output of nvidia-smi -q:\n    Retired pages\n        Single Bit ECC             : 2\n        Double Bit ECC             : 0\n        Pending Page Blacklist    : Yes\nEach retired page decreases the total memory available to applications. But each page is only 4MB large, so it doesn‚Äôt reduce the total available GPU memory by much.\nTo dive even deeper into the GPU debugging, please refer to this document - it includes a useful triage chart which helps to determine when to RMA GPUs. This document has additional information about Xid 63-like errors\nFor example it suggests:\n\nIf associated with XID 94, the application that encountered the error needs to be restarted. All other applications on the system can keep running as is until there is a convenient time to reboot for row remapping to activate. See below for guidelines on when to RMA GPUs based on row remapping failures.\n\nIf after a reboot the same condition occur for the same memory address, it means that memory remapping has failed and Xid 64 will be emitted again. If this continues it means you have a hardware issue that can‚Äôt be auto-corrected and the GPU needs to RMA‚Äôed.\nAt other times you may get Xid 63 or 64 and the application will crash. Which usually will generate additional Xid errors, but most of the time it means that the error was uncorrectable (i.e.¬†it was a DBE sort of an error and then it‚Äôll be Xid 48).\nAs mentioned earlier to reset a GPU you can either simply reboot the machine, or run:\nnvidia-smi -r -i gpu_id\nwhere gpu_id is the sequential number of the gpu you want to reset. Without -i all GPUs will be reset.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators",
      "Nvidia",
      "Troubleshooting NVIDIA GPUs"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/nvidia/debug.html#xid-errors",
    "href": "qmd/compute/accelerator/nvidia/debug.html#xid-errors",
    "title": "Troubleshooting NVIDIA GPUs",
    "section": "",
    "text": "No hardware is perfect, sometimes due to the manufacturing problems or due to tear and wear (especially because of exposure to high heat), GPUs are likely to encounter various hardware issues. A lot of these issues get corrected automatically without needing to really understand what‚Äôs going on. If the application continues running usually there is nothing to worry about. If the application crashes due to a hardware issue it‚Äôs important to understand why this is so and how to act on it.\nA normal user who uses a handful of GPUs is likely to never need to understand GPU-related hardware issues, but if you come anywhere close to massive ML training where you are likely to use hundreds to thousands of GPUs it‚Äôs certain that you‚Äôd want to understand about different hardware issues.\nIn your system logs you are likely to see occasionally Xid Errors like:\nNVRM: Xid (PCI:0000:10:1c): 63, pid=1896, Row Remapper: New row marked for remapping, reset gpu to activate.\nTo get those logs one of the following ways should work:\nsudo grep Xid /var/log/syslog\nsudo dmesg -T | grep Xid\nTypically, as long as the training doesn‚Äôt crash, these errors often indicate issues that automatically get corrected by the hardware.\nThe full list of Xid Errors and their interpretation can be found here.\nYou can run nvidia-smi -q and see if there are any error counts reported. For example, in this case of Xid 63, you will see something like:\nTimestamp                                 : Wed Jun  7 19:32:16 2023\nDriver Version                            : 510.73.08\nCUDA Version                              : 11.6\n\nAttached GPUs                             : 8\nGPU 00000000:10:1C.0\n    Product Name                          : NVIDIA A100-SXM4-80GB\n    [...]\n    ECC Errors\n        Volatile\n            SRAM Correctable              : 0\n            SRAM Uncorrectable            : 0\n            DRAM Correctable              : 177\n            DRAM Uncorrectable            : 0\n        Aggregate\n            SRAM Correctable              : 0\n            SRAM Uncorrectable            : 0\n            DRAM Correctable              : 177\n            DRAM Uncorrectable            : 0\n    Retired Pages\n        Single Bit ECC                    : N/A\n        Double Bit ECC                    : N/A\n        Pending Page Blacklist            : N/A\n    Remapped Rows\n        Correctable Error                 : 1\n        Uncorrectable Error               : 0\n        Pending                           : Yes\n        Remapping Failure Occurred        : No\n        Bank Remap Availability Histogram\n            Max                           : 639 bank(s)\n            High                          : 1 bank(s)\n            Partial                       : 0 bank(s)\n            Low                           : 0 bank(s)\n            None                          : 0 bank(s)\n[...]\nHere we can see that Xid 63 corresponds to:\nECC page retirement or row remapping recording event\nwhich may have 3 causes: HW Error / Driver Error / FrameBuffer (FB) Corruption\nThis error means that one of the memory rows is malfunctioning and that upon either reboot and/or a gpu reset one of the 640 spare memory rows (in A100) will be used to replace the bad row. Therefore we see in the report above that only 639 banks remain (out of 640).\nThe Volatile section of the ECC Errors report above refers to the errors recorded since last reboot/GPU reset. The Aggregate section records the same error since the GPU was first used.\nNow, there are 2 types of errors - Correctable and Uncorrectable. The correctable one is a Single Bit ECC Error (SBE) where despite memory being faulty the driver can still recover the correct value. The uncorrectable one is where more than one bit is faulty and it‚Äôs called Double Bit ECC Error (DBE). Typically, the driver will retire whole memory pages if 1 DBE or 2 SBE errors occur at the same memory address. For full information see this document\nA correctable error will not impact the application, a non-correctable one will crash the application. The memory page containing the uncorrectable ECC error will be blacklisted and not accessible until the GPU is reset.\nIf there are page scheduled to be retired you will see something like this in the output of nvidia-smi -q:\n    Retired pages\n        Single Bit ECC             : 2\n        Double Bit ECC             : 0\n        Pending Page Blacklist    : Yes\nEach retired page decreases the total memory available to applications. But each page is only 4MB large, so it doesn‚Äôt reduce the total available GPU memory by much.\nTo dive even deeper into the GPU debugging, please refer to this document - it includes a useful triage chart which helps to determine when to RMA GPUs. This document has additional information about Xid 63-like errors\nFor example it suggests:\n\nIf associated with XID 94, the application that encountered the error needs to be restarted. All other applications on the system can keep running as is until there is a convenient time to reboot for row remapping to activate. See below for guidelines on when to RMA GPUs based on row remapping failures.\n\nIf after a reboot the same condition occur for the same memory address, it means that memory remapping has failed and Xid 64 will be emitted again. If this continues it means you have a hardware issue that can‚Äôt be auto-corrected and the GPU needs to RMA‚Äôed.\nAt other times you may get Xid 63 or 64 and the application will crash. Which usually will generate additional Xid errors, but most of the time it means that the error was uncorrectable (i.e.¬†it was a DBE sort of an error and then it‚Äôll be Xid 48).\nAs mentioned earlier to reset a GPU you can either simply reboot the machine, or run:\nnvidia-smi -r -i gpu_id\nwhere gpu_id is the sequential number of the gpu you want to reset. Without -i all GPUs will be reset.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators",
      "Nvidia",
      "Troubleshooting NVIDIA GPUs"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/nvidia/debug.html#running-diagnostics",
    "href": "qmd/compute/accelerator/nvidia/debug.html#running-diagnostics",
    "title": "Troubleshooting NVIDIA GPUs",
    "section": "Running diagnostics",
    "text": "Running diagnostics\nIf you suspect one or mode NVIDIA GPUs are broken on a given node, dcgmi is a great tool to quickly find any bad GPUs.\nNVIDIA¬Æ Data Center GPU Manager (DCGM) is documented here and can be downloaded from here.\nHere is an example slurm script that will run very in-depth diagnostics (-r 3), which will take about 10 minutes to complete on an 8-GPU node:\n$ cat dcgmi-1n.slurm\n#!/bin/bash\n#SBATCH --job-name=dcgmi-1n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=96\n#SBATCH --gres=gpu:8\n#SBATCH --exclusive\n#SBATCH --output=%x-%j.out\n\nset -x -e\necho \"START TIME: $(date)\"\nsrun --output=%x-%j-%N.out dcgmi diag -r 3\necho \"END TIME: $(date)\"\nNow to run it on specific nodes of choice:\nsbatch --nodelist=node-115 dcgmi-1n.slurm\nsbatch --nodelist=node-151 dcgmi-1n.slurm\nsbatch --nodelist=node-170 dcgmi-1n.slurm\nedit the nodelist argument to point to the node name to run.\nIf the node is drained or downed and you can‚Äôt launch a slurm job using this node, just ssh into the node and run the command directly on the node:\ndcgmi diag -r 3\nIf the diagnostics didn‚Äôt find any issue, but the application still fails to work, re-run the diagnostics with level 4, which will now take more than 1 hour to complete:\ndcgmi diag -r 4\nFor example, if you run into a repeating Xid 64 error it‚Äôs likely that the diagnostics report will include:\n+---------------------------+------------------------------------------------+\n| Diagnostic                | Result                                         |\n+===========================+================================================+\n|-----  Deployment  --------+------------------------------------------------|\n| Error                     | GPU 3 has uncorrectable memory errors and row  |\n|                           |  remappings are pending                        |\nso you now know to RMA that problematic GPU, if remapping fails.\nThe dcgmi tool contains various other levels of diagnostics, some of which complete in a matter of a few minutes and can be run as a quick diagnostic in the epilogue of SLURM jobs to ensure that the node is ready to work for the next SLURM job, rather than discovering that after the user started their job and it crashed.\nWhen filing an RMA report you will be asked to run nvidia-bug-report script, the output of which you will need to submit with the RMA request.\nI usually save the log as well for posterity using one of:\ndcgmi diag -r 3 | tee -a dcgmi-r3-`hostname`.txt\ndcgmi diag -r 4 | tee -a dcgmi-r4-`hostname`.txt",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators",
      "Nvidia",
      "Troubleshooting NVIDIA GPUs"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/nvidia/debug.html#how-to-detect-if-a-node-is-missing-gpus",
    "href": "qmd/compute/accelerator/nvidia/debug.html#how-to-detect-if-a-node-is-missing-gpus",
    "title": "Troubleshooting NVIDIA GPUs",
    "section": "How to detect if a node is missing GPUs",
    "text": "How to detect if a node is missing GPUs\nIf you got a new VM, there are odd cases where there is less than expected number of GPUs. Here is how you can quickly test you have got 8 of them:\ncat &lt;&lt; 'EOT' &gt;&gt; test-gpu-count.sh\n#!/bin/bash\n\nset -e\n\n# test the node has 8 gpus\ntest $(nvidia-smi -q | grep UUID | wc -l) != 8 && echo \"broken node: less than 8 gpus\" && false\nEOT\nand then:\nbash test-gpu-count.sh",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators",
      "Nvidia",
      "Troubleshooting NVIDIA GPUs"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/nvidia/debug.html#how-to-detect-if-you-get-the-same-broken-node-again-and-again",
    "href": "qmd/compute/accelerator/nvidia/debug.html#how-to-detect-if-you-get-the-same-broken-node-again-and-again",
    "title": "Troubleshooting NVIDIA GPUs",
    "section": "How to detect if you get the same broken node again and again",
    "text": "How to detect if you get the same broken node again and again\nThis is mostly relevant to cloud users who rent GPU nodes.\nSo you launched a new virtual machine and discovered it has one or more broken NVIDIA GPUs. You discarded it and launched a new and the GPUs are broken again.\nChances are that you‚Äôre getting the same node with the same broken GPUs. Here is how you can know that.\nBefore discarding the current node, run and log:\n$ nvidia-smi -q | grep UUID\n    GPU UUID                              : GPU-2b416d09-4537-ecc1-54fd-c6c83a764be9\n    GPU UUID                              : GPU-0309d0d1-8620-43a3-83d2-95074e75ec9e\n    GPU UUID                              : GPU-4fa60d47-b408-6119-cf63-a1f12c6f7673\n    GPU UUID                              : GPU-fc069a82-26d4-4b9b-d826-018bc040c5a2\n    GPU UUID                              : GPU-187e8e75-34d1-f8c7-1708-4feb35482ae0\n    GPU UUID                              : GPU-43bfd251-aad8-6e5e-ee31-308e4292bef3\n    GPU UUID                              : GPU-213fa750-652a-6cf6-5295-26b38cb139fb\n    GPU UUID                              : GPU-52c408aa-3982-baa3-f83d-27d047dd7653\nThese UUIDs are unique to each GPU.\nWhen you then re-created your VM, run this command again - if the UUIDs are the same - you know you have the same broken GPUs.\nSometimes just rebooting the node will get new hardware. In some situations you get new hardware on almost every reboot, in other situations this doesn‚Äôt happen. And this behavior may change from one provider to another.\nIf you keep on getting the same broken node - one trick to overcoming this is allocating a new VM, while holding the broken VM running and when the new VM is running - discarding the broken one. That way you will surely get new GPUs - except there is no guarantee they won‚Äôt be broken as well. If the use case fits consider getting a static cluster where it‚Äôs much easier to keep the good hardware.\nThis method is extra-crucial for when GPUs don‚Äôt fail right away but after some use so it is non-trivial to see that there is a problem. Even if you reported this node to the cloud provider the technician may not notice the problem right away and put the bad node back into circulation. So if you‚Äôre not using a static cluster and tend to get random VMs on demand you may want to keep a log of bad UUIDs and know you have got a lemon immediately and not 10 hours into the node‚Äôs use.\nCloud providers usually have a mechanism of reporting bad nodes. Therefore other than discarding a bad node, it‚Äôd help yourself and other users to report bad nodes. Since most of the time users just discard the bad nodes, the next user is going to get them. I have seen users getting a very high percentage of bad nodes in some situations.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators",
      "Nvidia",
      "Troubleshooting NVIDIA GPUs"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingFaster debug and development with tiny models, tokenizers and datasets",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-model",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-model",
    "title": "",
    "section": "Making a tiny model",
    "text": "Making a tiny model\nImportant: given their popularity and the well designed simple API I will be discussing HF transformers models. But the same principle can be applied to any other model.\nTLDR: it‚Äôs trivial to make a tiny HF transformers model:\n\nFetch the config object of a full size models\nShrink the hidden size and perhaps a few other parameters\nCreate a model from that shrunken config\nSave this model. Done!\n\nfootnote: It‚Äôs critical to remember that this will generate a random model, so don‚Äôt expect any quality from its output.\nNow let‚Äôs go through the actual code and convert ‚Äúgoogle/mt5-small‚Äù into its tiny random counterpart.\nfrom transformers import MT5Config, MT5ForConditionalGeneration\n\nmname_from = \"google/mt5-small\"\nmname_very_small = \"mt5-tiny-random\"\n\nconfig = MT5Config.from_pretrained(mname_from)\n\nconfig.update(dict(\n    d_model=64,\n    d_ff=256,\n))\nprint(\"new config\", config)\n\nvery_small_model = MT5ForConditionalGeneration(config)\nprint(f\"num of params {very_small_model.num_parameters()}\")\n\nvery_small_model.save_pretrained(mname_very_small)\nAs you can see it‚Äôs trivial to do. And you can make it even smaller if you don‚Äôt need the hidden size to be at least 64. For example try 8 - you just need to make sure that the number of attention heads isn‚Äôt larger than hidden size.\nAlso please note that you don‚Äôt need any GPUs to do that and you could do this even on a huge 176B parameter model like BLOOM-176B. Since you never load the actual original model, except its config object.\nBefore modifying the config you can dump the original parameters and choose to shrinks more dimensions. For example, using less layers makes it even smaller and easier to debug. So here is what you can do instead:\nconfig.update(dict(\n    vocab_size=keep_items+12,\n    d_model=64,\n    d_ff=256,\n    d_kv=8,\n    num_layers=8,\n    num_decoder_layers=8,\n    num_heads=4,\n    relative_attention_num_buckets=32,\n))\nThe original ‚Äúgoogle/mt5-small‚Äù model file was 1.2GB. With the above changes we got it down to 126MB.\nWe can then half its size by converting the model to fp16 (or bf16) before saving it:\nvery_small_model.half()\nvery_small_model.save_pretrained(mname_very_small)\nthis takes us to 64M file.\nSo you could stop here and your program will start much much faster already.\nAnd there is one more step you could do to make it truly tiny.\nWhat we haven‚Äôt shrunken so far is the vocabulary dimension so 64x250k (hidden*vocab) is still huge. Granted this 250k vocab model is not typical - normally models models‚Äô vocab is ~30-50k, but even 30k is a lot if we want the model to be truly tiny.\nSo next we will look into various techniques to shrinking the tokenizer, as it defines our vocab size.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-tokenizer",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-tokenizer",
    "title": "",
    "section": "Making a tiny tokenizer",
    "text": "Making a tiny tokenizer\nThis task varies between a relatively simple procedure and a much more complex workout depending on the underlying tokenizer.\nThe following recipes have come from a few awesome tokenizer experts at Hugging Face, which I then adapted to my needs.\nYou probably don‚Äôt really need to understand how these work until you actually need them, therefore if you‚Äôre reading this for the first time you can safely jump over these to Making a tiny model with a tiny tokenizer.\n\nAnthony Moi‚Äôs version\nAnthony Moi‚Äôs tokenizer shrinker:\nimport json\nfrom transformers import AutoTokenizer\nfrom tokenizers import Tokenizer\n\nvocab_keep_items = 5000\nmname = \"microsoft/deberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(mname, use_fast=True)\nassert tokenizer.is_fast, \"This only works for fast tokenizers.\"\ntokenizer_json = json.loads(tokenizer._tokenizer.to_str())\nvocab = tokenizer_json[\"model\"][\"vocab\"]\nif tokenizer_json[\"model\"][\"type\"] == \"BPE\":\n    new_vocab = { token: i for token, i in vocab.items() if i &lt; vocab_keep_items }\n    merges = tokenizer_json[\"model\"][\"merges\"]\n    new_merges = []\n    for i in range(len(merges)):\n        a, b = merges[i].split()\n        new_token = \"\".join((a, b))\n        if a in new_vocab and b in new_vocab and new_token in new_vocab:\n            new_merges.append(merges[i])\n    tokenizer_json[\"model\"][\"merges\"] = new_merges\nelif tokenizer_json[\"model\"][\"type\"] == \"Unigram\":\n    new_vocab = vocab[:vocab_keep_items]\nelif tokenizer_json[\"model\"][\"type\"] == \"WordPiece\" or tokenizer_json[\"model\"][\"type\"] == \"WordLevel\":\n    new_vocab = { token: i for token, i in vocab.items() if i &lt; vocab_keep_items }\nelse:\n    raise ValueError(f\"don't know how to handle {tokenizer_json['model']['type']}\")\ntokenizer_json[\"model\"][\"vocab\"] = new_vocab\ntokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\ntokenizer.save_pretrained(\".\")\nI later discovered that gpt2 seems to have a special token \"&lt;|endoftext|&gt;\" stashed at the very end of the vocab, so it gets dropped and code breaks. So I hacked it back in with:\nif \"gpt2\" in mname:\n        new_vocab = { token: i for token, i in vocab.items() if i &lt; vocab_keep_items-1 }\n        new_vocab[\"&lt;|endoftext|&gt;\"] = vocab_keep_items-1\n    else:\n        new_vocab = { token: i for token, i in vocab.items() if i &lt; vocab_keep_items }\n\n\nLysandre Debut‚Äôs version\nLysandre Debut‚Äô shrinker using train_new_from_iterator:\nfrom transformers import AutoTokenizer\n\nmname = \"microsoft/deberta-base\" # or any checkpoint that has a fast tokenizer.\nvocab_keep_items = 5000\n\ntokenizer = AutoTokenizer.from_pretrained(mname)\nassert tokenizer.is_fast, \"This only works for fast tokenizers.\"\ntokenizer.save_pretrained(\"big-tokenizer\")\n# Should be a generator of list of texts.\ntraining_corpus = [\n    [\"This is the first sentence.\", \"This is the second one.\"],\n    [\"This sentence (contains #) over symbols and numbers 12 3.\", \"But not this one.\"],\n]\nnew_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=vocab_keep_items)\nnew_tokenizer.save_pretrained(\"small-tokenizer\")\nbut this one requires a training corpus, so I had an idea to cheat and train the new tokenizer on its own original vocab which gave me:\nfrom transformers import AutoTokenizer\n\nmname = \"microsoft/deberta-base\"\nvocab_keep_items = 5000\n\ntokenizer = AutoTokenizer.from_pretrained(mname)\nassert tokenizer.is_fast, \"This only works for fast tokenizers.\"\nvocab = tokenizer.get_vocab()\ntraining_corpus = [ vocab.keys() ] # Should be a generator of list of texts.\nnew_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=vocab_keep_items)\nnew_tokenizer.save_pretrained(\"small-tokenizer\")\nwhich is almost perfect, except it now doesn‚Äôt have any information about the frequency for each word/char (that‚Äôs how most tokenizers compute their vocab, which if you need this info you can fix by having each key appearing len(vocab) - ID times, i.e.:\ntraining_corpus = [ (k for i in range(vocab_len-v)) for k,v in vocab.items() ]\nwhich will make the script much much longer to complete.\nBut for the needs of a tiny model (testing) the frequency doesn‚Äôt matter at all.\n\n\nHack the tokenizer file approach\nSome tokenizers can be be just manually truncated at the file level, e.g.¬†let‚Äôs shrink Llama2‚Äôs tokenizer to 3k items:\n# Shrink the orig vocab to keep things small (just enough to tokenize any word, so letters+symbols)\n# ElectraTokenizerFast is fully defined by a tokenizer.json, which contains the vocab and the ids,\n# so we just need to truncate it wisely\nimport subprocess\nimport shlex\nfrom transformers import LlamaTokenizerFast\n\nmname = \"meta-llama/Llama-2-7b-hf\"\nvocab_keep_items = 3000\n\ntokenizer_fast = LlamaTokenizerFast.from_pretrained(mname)\ntmp_dir = f\"/tmp/{mname}\"\ntokenizer_fast.save_pretrained(tmp_dir)\n# resize tokenizer.json (vocab.txt will be automatically resized on save_pretrained)\n# perl  -0777 -pi -e 's|(2999).*|$1},\"merges\": []}}|msg' tokenizer.json # 0-indexed, so vocab_keep_items-1!\nclosing_pat = '},\"merges\": []}}'\ncmd = (f\"perl -0777 -pi -e 's|({vocab_keep_items-1}).*|$1{closing_pat}|msg' {tmp_dir}/tokenizer.json\")\n#print(f\"Running:\\n{cmd}\")\nresult = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n# reload with modified tokenizer\ntokenizer_fast_tiny = LlamaTokenizerFast.from_pretrained(tmp_dir)\ntokenizer_fast_tiny.save_pretrained(\".\")\nPlease remember that the outcome is only useful for functional testing - not quality work.\nHere is the full version of make_tiny_model.py which includes both the model and the tokenizer shrinking.\n\n\nSentencePiece vocab shrinking\nFirst clone SentencePiece into a parent dir:\ngit clone https://github.com/google/sentencepiece\nNow to the shrinking:\n# workaround for fast tokenizer protobuf issue, and it's much faster too!\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\nfrom transformers import XLMRobertaTokenizerFast\n\nmname = \"xlm-roberta-base\"\n\n# Shrink the orig vocab to keep things small\nvocab_keep_items = 5000\ntmp_dir = f\"/tmp/{mname}\"\nvocab_orig_path = f\"{tmp_dir}/sentencepiece.bpe.model\" # this name can be different\nvocab_short_path = f\"{tmp_dir}/spiece-short.model\"\n# HACK: need the sentencepiece source to get sentencepiece_model_pb2, as it doesn't get installed\nsys.path.append(\"../sentencepiece/python/src/sentencepiece\")\nimport sentencepiece_model_pb2 as model\ntokenizer_orig = XLMRobertaTokenizerFast.from_pretrained(mname)\ntokenizer_orig.save_pretrained(tmp_dir)\nwith open(vocab_orig_path, 'rb') as f: data = f.read()\n# adapted from https://blog.ceshine.net/post/trim-down-sentencepiece-vocabulary/\nm = model.ModelProto()\nm.ParseFromString(data)\nprint(f\"Shrinking vocab from original {len(m.pieces)} dict items\")\nfor i in range(len(m.pieces) - vocab_keep_items): _ = m.pieces.pop()\nprint(f\"new dict {len(m.pieces)}\")\nwith open(vocab_short_path, 'wb') as f: f.write(m.SerializeToString())\nm = None\n\ntokenizer_fast_tiny = XLMRobertaTokenizerFast(vocab_file=vocab_short_path)\ntokenizer_fast_tiny.save_pretrained(\".\")",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-model-with-a-tiny-tokenizer",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-model-with-a-tiny-tokenizer",
    "title": "",
    "section": "Making a tiny model with a tiny tokenizer",
    "text": "Making a tiny model with a tiny tokenizer\nSo now you can shrink the vocab size to as small as the tokenizer allows, that is you need to have at least enough tokens to cover the target alphabet and special characters, and usually 3-5k tokens is more than enough. Sometimes you could make it even small, after all the original ASCII charset has only 128 characters.\nIf we continue the MT5 code from earlier in this chapter and add the tokenizer shrinking code from the previous section, we end up with this script mt5-make-tiny-model.py and when we run it - our end model file is truly tiny - 3.34 MB in size! As you can see the script also has code to validate that the model can actually work with the modified tokenizer. The results will be garbage, but the intention is to test that the new model and the tokenizer are functional.\nHere is another example fsmt-make-super-tiny-model.py - here you can see I‚Äôm creating a totally new tiny vocab from scratch.\nI also recommend to always store the building scripts with the model, so that you could quickly fix things or make similar versions of the model.\nAlso be aware that since HF transformers needs tiny models for their testing, you are very likely to already find one for each architecture available mostly from https://huggingface.co/hf-internal-testing (except they didn‚Äôt include the code of how they were made, but you can now figure it out based on these notes).\nAnother hint: if you need a slightly different tiny model, you can also start with an already existing tiny model and adapt it instead. Since it‚Äôs random it‚Äôs really only about getting the right dimensions. For example if the tiny model you found has 2 layers but you need 8, just resave it with this larger dimension and you‚Äôre done.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-dataset",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#making-a-tiny-dataset",
    "title": "",
    "section": "Making a tiny dataset",
    "text": "Making a tiny dataset\nSimilar to models and tokenizers it helps to have a handy tiny version of a dataset you work with a lot. As usual this won‚Äôt help with quality testing, but it‚Äôs perfect for launching your program really fast.\nfootnote: the impact of using a tiny dataset won‚Äôt be as massive as using a tiny model, if you‚Äôre using already pre-indexed Arrow file datasets, since those are already extremely fast. But say you want the iterator to finish an epoch in 10 steps. Instead of editing your code to truncate the dataset, you could just use a tiny dataset instead.\nThis process of making a tiny dataset is somewhat more difficult to explain because it‚Äôd depend on the builder of the original model, which can be quite different from each other, but perhaps you can correlate my recipes to your datasets.\nBut the concept is still very simple:\n\nClone the full dataset git repo\nReplace its full data tarball with a tiny one that contains just a few samples\nSave it - Done!\n\nHere are some examples:\n\nstas/oscar-en-10k\nstas/c4-en-10k\nstas/openwebtext-10k\n\nIn all of these I took the original tarball, grabbed the first 10k records, tarred it back, used this smaller tarball and that was that. The rest of the builder script remained mostly the same.\nAnd here are some examples of synthetic datasets, where instead of just shrinking the original tarball, I untar‚Äôed it, manually chose the representative examples and then wrote a script to build any size of desired dataset based on those few representative samples: - stas/general-pmd-synthetic-testing and the unpacker - stas/cm4-synthetic-testing - and the unpacker\nThese are also the complex examples where each sample is more than a text entry, but may have multiple text entries and images as well.\nThe unpacker is what expands each complex multi-record sample into its own sub-directory, so that now you can easily go and tweak it to your liking. You can add image, remove them, make text records smaller, etc.. You will also notice that I‚Äôm shrinking the large images into tiny 32x32 images, so again I‚Äôm applying the important principle of tiny across all dimensions that don‚Äôt break the requirements of the target codebase.\nAnd then the main script uses that structure to build a dataset of any desired length.\nAnd here is for example the instructions of deploying these scripts for stas/general-pmd-synthetic-testing:\n# prep dataset repo\nhttps://huggingface.co/new-dataset =&gt; stas/general-pmd-synthetic-testing\ngit clone https://huggingface.co/datasets/stas/general-pmd-synthetic-testing\ncd general-pmd-synthetic-testing\n\n# select a few seed records so there is some longer and shorter text, records with images and without,\n# a few variations of each type\nrm -rf data\npython general-pmd-ds-unpack.py --dataset_name_or_path \\\ngeneral_pmd/image/localized_narratives__ADE20k/train/00000-00002 --ids 1-10 --target_path data\n\ncd data\n\n# shrink to 32x32 max, keeping ratio\nmogrify -format jpg -resize 32x32\\&gt; */*jpg\n\n# adjust one record to have no image and no text\ncd 1\nrm image.jpg text.txt\ntouch image.null text.null\ncd -\n\ncd ..\n\n# create tarball\ntar -cvzf data.tar.gz data\n\n# complete the dataset repo\necho \"This dataset is designed to be used in testing. It's derived from general-pmd/localized_narratives__ADE20k \\\ndataset\" &gt;&gt; README.md\n\n# test dataset\ncd ..\ndatasets-cli test general-pmd-synthetic-testing/general-pmd-synthetic-testing.py --all_configs\nI also recommend to always store the building scripts with the dataset, so that you could quickly fix things or make similar versions of the dataset.\nSimilar to tiny models, you will find many tiny datasets under https://huggingface.co/hf-internal-testing.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#conclusion",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nWhile in the domain of ML we have the dataset, the model and the tokenizer - each of which can be made tiny and enable super-speed development with low resource requirements, if you‚Äôre coming from a different industry you can adapt the ideas discussed in this chapter to your particular domain‚Äôs artifacts/payloads.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/make-tiny-models-tokenizers-datasets.html#backup-of-all-scripts-in-this-chapter",
    "href": "qmd/debug/make-tiny-models-tokenizers-datasets.html#backup-of-all-scripts-in-this-chapter",
    "title": "",
    "section": "Backup of all scripts in this chapter",
    "text": "Backup of all scripts in this chapter\nShould the original scripts this chapter is pointing to disappear or the HF hub is down while you‚Äôre reading this, here is the local back up of all of them.\nnote-to-self: to make the latest backup of files linked to in this chapter run:\nperl -lne 'while (/(https.*?.py)\\)/g) { $x=$1; $x=~s/blob/raw/; print qq[wget $x] }' make-tiny-models.md",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Faster debug and development with tiny models, tokenizers and datasets"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html",
    "href": "qmd/debug/pytorch.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingDebugging PyTorch programs",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#getting-nodes-to-talk-to-each-other",
    "href": "qmd/debug/pytorch.html#getting-nodes-to-talk-to-each-other",
    "title": "",
    "section": "Getting nodes to talk to each other",
    "text": "Getting nodes to talk to each other\nOnce you need to use more than one node to scale your training, e.g., if you want to use DDP to train faster, you have to get the nodes to talk to each other, so that communication collectives could send data to each other. This is typically done via a comms library like NCCL. And in our DDP example, at the end of training step all GPUs have to perform an all_reduce call to synchronize the gradients across all ranks.\nIn this section we will discuss a very simple case of just 2 nodes (with 8 GPUs each) talking to each other and which can then be easily extended to as many nodes as needed. Let‚Äôs say that these nodes have the IP addresses 10.0.0.1 and 10.0.0.2.\nOnce we have the IP addresses we then need to choose a port for communications.\nIn Unix there are 64k ports. The first 1k are reserved for common services so that any computer on the Internet could connect to any other computer knowing ahead of time which port to connect to. For example, port 22 is reserved for SSH. So that whenever you do ssh example.com in fact the program open a connection to example.com:22.\nAs there are thousands of services out there, the reserved 1k ports is not enough, and so various services could use pretty much any port. But fear not, when you get your Linux box on the cloud or an HPC, you‚Äôre unlikely to have many preinstalled services that could use a high number port, so most ports should be available.\nTherefore let‚Äôs choose port 6000.\nNow we have: 10.0.0.1:6000 and 10.0.0.2:6000 that we want to be able to communicate with each other.\nThe first thing to do is to open port 6000 for incoming and outgoing connections on both nodes. It might be open already or you might have to read up the instructions of your particular setup on how to open a given port.\nHere are multiple ways that you could use to test whether port 6000 is already open.\ntelnet localhost:6000\nnmap -p 6000 localhost\nnc -zv localhost 6000\ncurl -v telnet://localhost:6000\nMost of these should be available via apt install or whatever your package manager uses.\nLet‚Äôs use nmap in this example. If I run:\n$ nmap -p 22 localhost\n[...]\nPORT   STATE SERVICE\n22/tcp open  ssh\nWe can see the port is open and it tells us which protocol and service is allocated as a bonus.\nNow let‚Äôs run:\n$ nmap -p 6000 localhost\n[...]\n\nPORT     STATE  SERVICE\n6000/tcp closed X11\nHere you can see port 6000 is closed.\nNow that you understand how to test, you can proceed to test the 10.0.0.1:6000 and 10.0.0.2:6000.\nFirst ssh to the first node in terminal A and test if port 6000 is opened on the second node:\nssh 10.0.0.1\nnmap -p 6000 10.0.0.2\nif all is good, then in terminal B ssh to the second node and do the same check in reverse:\nssh 10.0.0.2\nnmap -p 6000 10.0.0.1\nIf both ports are open you can now use this port. If either or both are closed you have to open these ports. Since most clouds use a proprietary solution, simply search the Internet for ‚Äúopen port‚Äù and the name of your cloud provider.\nThe next important thing to understand is that compute nodes will typically have multiple network interface cards (NICs). You discover those interfaces by running:\n$ sudo ifconfig\nOne interface is typically used by users to connecting to nodes via ssh or for various other non-compute related services - e.g., sending an email or download some data. Often this interface is called eth0, with eth standing for Ethernet, but it can be called by other names.\nThen there is the inter-node interface which can be Infiniband, EFA, OPA, HPE Slingshot, etc. (more information). There could be one or dozens of those interfaces.\nHere are some examples of ifconfig‚Äôs output:\n$ sudo ifconfig\nenp5s0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 10.0.0.23  netmask 255.255.255.0  broadcast 10.0.0.255\n        [...]\nI removed most of the output showing only some of the info. Here the key information is the IP address that is listed after inet. In the example above it‚Äôs 10.0.0.23. This is the IP address of interface enp5s0.\nIf there is another node, it‚Äôll probably be 10.0.0.24 or 10.0.0.21 or something of sorts - the last segment will be the one with a different number.\nLet‚Äôs look at another example:\n$ sudo ifconfig\nib0     Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00\n        inet addr:172.0.0.50  Bcast: 172.0.0.255  Mask:255.255.255.0\n        [...]\nHere ib typically tells us it‚Äôs an InfiniBand card, but really it can be any other vendor. I have seen OmniPath using ib for example. Again inet tells us the IP of this interface is 172.0.0.50.\nIf you lost me, we want the IP addresses so that we could test if ip:port is open on each node in question.\nFinally, going back to our pair of 10.0.0.1:6000 and 10.0.0.2:6000 let‚Äôs do an all_reduce test using 2 terminals, where we choose 10.0.0.1 as the master host which will coordinate other nodes. For testing we will use this helper debug program torch-distributed-gpu-test.py.\nIn terminal A:\n$ ssh 10.0.0.1\n$ python -m torch.distributed.run --role $(hostname -s): --tee 3 --nnodes 2 --nproc_per_node 8 \\\n --master_addr 10.0.0.1 --master_port 6000 torch-distributed-gpu-test.py\nIn terminal B:\n$ ssh 10.0.0.2\n$ python -m torch.distributed.run --role $(hostname -s): --tee 3 --nnodes 2 --nproc_per_node 8 \\\n --master_addr 10.0.0.1 --master_port 6000 torch-distributed-gpu-test.py\nNote that I‚Äôm using the same --master_addr 10.0.0.1 --master_port 6000 in both cases because we checked port 6000 is open and we use 10.0.0.1 as the coordinating host.\nThis approach of running things manually from each node is painful and so there are tools that automatically launch the same command on multiple nodes\npdsh\npdsh is one such solution - which is like ssh but will automatically run the same command on multiple nodes:\nPDSH_RCMD_TYPE=ssh pdsh -w 10.0.0.1,10.0.0.2 \\\n\"python -m torch.distributed.run --role $(hostname -s): --tee 3 --nnodes 2 --nproc_per_node 8 \\\n --master_addr 10.0.0.1 --master_port 6000 torch-distributed-gpu-test.py\"\nYou can see how I folded the 2 sets of commands into 1. If you have more nodes, just add more nodes as -w argument.\nSLURM\nIf you use SLURM, it‚Äôs almost certain that whoever set things up already have all the ports opened for you, so it should just work. But if it doesn‚Äôt the information in this section should help debug things.\nHere is how you‚Äôd use this with SLURM.\n#!/bin/bash\n#SBATCH --job-name=test-nodes        # name\n#SBATCH --nodes=2                    # nodes\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=10           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 0:05:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#\nexport GPUS_PER_NODE=8\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=6000\n#\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n--nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n--master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\ntorch-distributed-gpu-test.py'\nIf you have more than 2 nodes you just need to change the number of nodes and the above script will automatically work for any number of them.\nMPI:\nAnother popular way is to use Message Passing Interface (MPI). There are a few open source implementations of it available.\nTo use this tool you first create a hostfile that contains your target nodes and the number of processes that should be run on each host. In the example of this section, with 2 nodes and 8 gpus each it‚Äôd be:\n$ cat hostfile\n10.0.0.1:8\n10.0.0.2:8\nand to run, it‚Äôs just:\n$ mpirun --hostfile  -np 16 -map-by ppr:8:node python my-program.py\nNote that I used my-program.py here because torch-distributed-gpu-test.py was written to work with torch.distributed.run (also known as torchrun). With mpirun you will have to check your specific implementation to see which environment variable it uses to pass the rank of the program and replace LOCAL_RANK with it, the rest should be mostly the same.\nNuances: - You might have to explicitly tell it which interface to use by adding --mca btl_tcp_if_include 10.0.0.0/24 to match our example. If you have many network interfaces it might use one that isn‚Äôt open or just the wrong interface. - You can also do the reverse and exclude some interfaces. e.g.¬†say you have docker0 and lo interfaces - to exclude those add --mca btl_tcp_if_exclude docker0,lo.\nmpirun has a gazillion of flags and I will recommend reading its manpage for more information. My intention was only to show you how you could use it. Also different mpirun implementations may use different CLI options.\n\nSolving the Infiniband connection between multiple nodes\nIn one situation on Azure I got 2 nodes on a shared subnet and when I tried to run the 2 node NCCL test:\nNCCL_DEBUG=INFO python -u -m torch.distributed.run --nproc_per_node=1 --nnodes 2 --rdzv_endpoint 10.2.0.4:6000  --rdzv_backend c10d torch-distributed-gpu-test.py\nI saw in the debug messages that Infiniband interfaces got detected:\nnode-2:5776:5898 [0] NCCL INFO NET/IB : Using [0]ibP111p0s0:1/IB [1]rdmaP1111p0s2:1/RoCE [RO]; OOB eth0:10.2.0.4&lt;0&gt;\nBut the connection would then time out with the message:\nnode-2:5776:5902 [0] transport/net_ib.cc:1296 NCCL WARN NET/IB : Got completion from peer 10.2.0.5&lt;33092&gt; with error 12, opcode 0, len\n0, vendor err 129 (Recv)\nnode-2:5776:5902 [0] NCCL INFO transport/net.cc:1134 -&gt; 6\nnode-2:5776:5902 [0] NCCL INFO proxy.cc:679 -&gt; 6\nnode-2:5776:5902 [0] NCCL INFO proxy.cc:858 -&gt; 6 [Proxy Thread]\nand nothing works. So here the Ethernet connectivity between 2 nodes works but not the IB interface.\nThere could be a variety of reason for this failing, but of the most likely one is when you‚Äôre on the cloud and the 2 nodes weren‚Äôt provisioned so that their IB is connected. So your Ethernet inter-node connectivity works, but it‚Äôs too slow. Chances are that you need to re-provision the nodes so that they are allocated together. For example, on Azure this means you have to allocate nodes within a special availability set\nGoing back to our case study, once the nodes were deleted and recreated within an availability set the test worked out of the box.\nThe individual nodes are often not meant for inter-node communication and often the clouds have the concept of clusters, which are designed for allocating multiple nodes as a group and are already preconfigured to work together.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#prefixing-logs-with-noderank-interleaved-asserts",
    "href": "qmd/debug/pytorch.html#prefixing-logs-with-noderank-interleaved-asserts",
    "title": "",
    "section": "Prefixing logs with node:rank, interleaved asserts",
    "text": "Prefixing logs with node:rank, interleaved asserts\nIn this section we will use torchrun (torch.distributed.run) during the demonstration and at the end of this section similar solutions for other launchers will be listed.\nWhen you have warnings and tracebacks (or debug prints), it helps a lot to prefix each log line with its hostname:rank prefix, which is done by adding --role $(hostname -s): --tee 3 to torchrun:\npython -m torch.distributed.run --role $(hostname -s): --tee 3 --nnodes 1 --nproc_per_node 2 \\\ntorch-distributed-gpu-test.py\nNow each log line will be prefixed with [hostname:rank]\nNote that the colon is important.\nIf you‚Äôre in a SLURM environment the above command line becomes:\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n--nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n--master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\n--role $(hostname -s): --tee 3 \\\ntorch-distributed-gpu-test.py'\nOf course adjust your environment variables to match, this was just an example.\nImportant! Note, that I‚Äôm using a single quoted string of commands passed to bash -c. This way hostname -s command is delayed until it‚Äôs run on each of the nodes. If you‚Äôd use double quotes above, hostname -s will get executed on the starting node and then all nodes will get the same hostname as the prefix, which defeats the purpose of using these flags. So if you use double quotes you need to rewrite the above like so:\nsrun --jobid $SLURM_JOBID bash -c \"python -m torch.distributed.run \\\n--nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank \\$SLURM_PROCID \\\n--master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\n--role \\$(hostname -s): --tee 3 \\\ntorch-distributed-gpu-test.py\"\n$SLURM_PROCID is escaped too as it needs to be specific to each node and it‚Äôs unknown during the launch of the slurm job on the main node. So there are 2 \\$ escapes in this version of the command.\nThis prefixing functionality is also super-helpful when one gets the distributed program fail and which often results in interleaved tracebacks that are very difficult to interpret. So by greping for one node:rank string of choice, it‚Äôs now possible to reconstruct the real error message.\nFor example, if you get a traceback that looks like:\n  File \"/path/to/training/dataset.py\", line 785, in __init__\n  File \"/path/to/training/dataset.py\", line 785, in __init__\n    if self.dataset_proba.sum() != 1:\nAttributeError: 'list' object has no attribute 'sum'\n    if self.dataset_proba.sum() != 1:\n  File \"/path/to/training/dataset.py\", line 785, in __init__\n  File \"/path/to/training/dataset.py\", line 785, in __init__\n    if self.dataset_proba.sum() != 1:\n    if self.dataset_proba.sum() != 1:\nAttributeError: 'list' object has no attribute 'sum'\nAttributeError: 'list' object has no attribute 'sum'\nAttributeError: 'list' object has no attribute 'sum'\nand when it‚Äôs dozens of frames over 8 nodes it can‚Äôt be made sense of, but the above -tee + --role addition will generate:\n[host1:0]  File \"/path/to/training/dataset.py\", line 785, in __init__\n[host1:1]  File \"/path/to/training/dataset.py\", line 785, in __init__\n[host1:0]    if self.dataset_proba.sum() != 1:\n[host1:0]AttributeError: 'list' object has no attribute 'sum'\n[host1:1]    if self.dataset_proba.sum() != 1:\n[host1:2]  File \"/path/to/training/dataset.py\", line 785, in __init__\n[host1:3]  File \"/path/to/training/dataset.py\", line 785, in __init__\n[host1:3]    if self.dataset_proba.sum() != 1:\n[host1:2]    if self.dataset_proba.sum() != 1:\n[host1:1]AttributeError: 'list' object has no attribute 'sum'\n[host1:2]AttributeError: 'list' object has no attribute 'sum'\n[host1:3]AttributeError: 'list' object has no attribute 'sum'\nand you can grep this output for just one host:rank prefix, which gives us:\n$ grep \"[host1:0]\" log.txt\n[host1:0]  File \"/path/to/training/dataset.py\", line 785, in __init__\n[host1:0]    if self.dataset_proba.sum() != 1:\n[host1:0]AttributeError: 'list' object has no attribute 'sum'\nand voila, you can now tell what really happened. And as I mentioned earlier there can be easily a hundred to thousands of interleaved traceback lines there.\nAlso, if you have just one node, you can just pass -tee 3 and there is no need to pass --role.\nIf hostname -s is too long, but you have each host with its own sequence number like:\n[really-really-really-long-hostname-5:0]\n[really-really-really-long-hostname-5:1]\n[really-really-really-long-hostname-5:2]\nyou can of course make it shorter by replacing hostname -s with hostname -s | tr -dc '0-9', which would lead to much shorter prefixes:\n[5:0]\n[5:1]\n[5:2]\nAnd, of course, if you‚Äôre doing debug prints, then to solve this exact issue you can use printflock.\nHere is how you accomplish the same feat with other launchers:\n\nsrun in SLURM: add --label\nopenmpi: add --tag-output\naccelerate: you can just pass the same -tee + --role flags as in torchrun",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#dealing-with-async-cuda-bugs",
    "href": "qmd/debug/pytorch.html#dealing-with-async-cuda-bugs",
    "title": "",
    "section": "Dealing with Async CUDA bugs",
    "text": "Dealing with Async CUDA bugs\nWhen using CUDA, failing pytorch programs very often produce a python traceback that makes no sense or can‚Äôt be acted upon. This is because due to CUDA‚Äôs async nature - when a CUDA kernel is executed, the program has already moved on and when the error happened the context of the program isn‚Äôt there. The async functionality is there to make things faster, so that while the GPU is churning some matmul the program on CPU could already start doing something else.\nAt other times some parts of the system will actually tell you that they couldn‚Äôt generate the correct traceback, as in this error:\n[E ProcessGroupNCCL.cpp:414] Some NCCL operations have failed or timed out. Due to the\nasynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/\nincomplete data. To avoid this inconsistency, we are taking the entire process down.\nThere are a few solutions.\nIf the failure is instant and can be reproduced on CPU (not all programs work on CPU), simply re-rerun it after hiding your GPUs. This is how you do it:\nCUDA_VISIBLE_DEVICES=\"\" python my-pytorch-program.py\nThe env var CUDA_VISIBLE_DEVICES is used to manually limit the visibility of GPUs to the executed program. So for example if you have 8 gpus and you want to run program1.py with first 4 gpus and program2.py with the remaining 2 gpus you can do:\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" python my-pytorch-program1.py\nCUDA_VISIBLE_DEVICES=\"4,5,6,7\" python my-pytorch-program2.py\nand the second program won‚Äôt be the wiser that it‚Äôs not using GPUs 0-3.\nBut in the case of debug we are hiding all GPUs, by setting CUDA_VISIBLE_DEVICES=\"\".\nNow the program runs on CPU and you will get a really nice traceback and will fix the problem in no time.\nBut, of course, if you your program requires multiple GPUs this won‚Äôt work. And so here is another solution.\nRerun your program after setting this environment variable:\nCUDA_LAUNCH_BLOCKING=1 python my-pytorch-program.py\nThis variable tells pytorch (or any other CUDA-based program) to turn its async nature off everywhere and now all operations will be synchronous. So when the program crashes you should now get a perfect traceback and you will know exactly what ails your program.\nIn theory enabling this variable should make everything run really slow, but in reality it really depends on your software. We did the whole of BLOOM-176B training using CUDA_LAUNCH_BLOCKING=1 with Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) and had zero slowdown - we had to use it as pytorch was hanging without it and we had no time to figure the hanging out.\nSo, yes, when you switch from async to sync nature, often it can hide some subtle race conditions, so there are times that a hanging disappears as in the example I shared above. So measure your throughput with and without this flag and sometimes it might actual not only help with getting an in-context traceback but actually solve your problem altogether.\nNote: NCCL==2.14.3 coming with pytorch==1.13 hangs when CUDA_LAUNCH_BLOCKING=1 is used. So don‚Äôt use it with that version of pytorch. The issue has been fixed in nccl&gt;=2.17 which should be included in pytorch==2.0.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#segfaults-and-getting-a-backtrace-from-a-core-file",
    "href": "qmd/debug/pytorch.html#segfaults-and-getting-a-backtrace-from-a-core-file",
    "title": "",
    "section": "segfaults and getting a backtrace from a core file",
    "text": "segfaults and getting a backtrace from a core file\nIt‚Äôs not uncommon for a complex pytorch program to segfault and drop a core file. Especially if you‚Äôre using complex extensions like NCCL.\nThe corefile is what the program generates when it crashes on a low-level - e.g.¬†when using a python extension - such as a CUDA kernel or really any library that is coded directly in some variant of C or another language and made accessible in python through some binding API. The most common cause of a segfault is when such software accesses memory it has not allocated. For example, a program may try to free memory it hasn‚Äôt allocated. But there could be many other reasons.\nWhen a segfault event happens Python can‚Äôt do anything, as the proverbial carpet is pulled out from under its feet, so it can‚Äôt generate an exception or even write anything to the output.\nIn these situation one must go and analyse the libC-level calls that lead to the segfault, which is luckily saved in the core file.\nIf your program crashed, you will often find a file that will look something like: core-python-3097667-6\nBefore we continue make sure you have gdb installed:\nsudo apt-get install gdb\nNow make sure you know the path to the python executable that was used to run the program that crashed. If you have multiple python environment you have to activate the right environment first. If you don‚Äôt gdb may fail to unpack the core file.\nSo typically I‚Äôd go:\nconda activate my-env\ngdb python core-python-3097667-6\n\nadjust my-env to whatever env you use, or instead of conda use whatever way you use to activate your python environment - and perhaps you‚Äôre using the system-wise python and then you don‚Äôt need to activate anything.\nadjust the name of the core file to the file you have gotten - it‚Äôs possible that there are many - pick the latest then.\n\nNow gdb will churn for a bit and will give you a prompt where you type: bt. We will use an actual core file here:\n(gdb) bt\n#0  0x0000147539887a9f in raise () from /lib64/libc.so.6\n#1  0x000014753985ae05 in abort () from /lib64/libc.so.6\n#2  0x000014751b85a09b in __gnu_cxx::__verbose_terminate_handler() [clone .cold.1] () from /lib64/libstdc++.so.6\n#3  0x000014751b86053c in __cxxabiv1::__terminate(void (*)()) () from /lib64/libstdc++.so.6\n#4  0x000014751b860597 in std::terminate() () from /lib64/libstdc++.so.6\n#5  0x000014751b86052e in std::rethrow_exception(std::__exception_ptr::exception_ptr) () from /lib64/libstdc++.so.6\n#6  0x000014750bb007ef in c10d::ProcessGroupNCCL::WorkNCCL::handleNCCLGuard() ()\n   from .../python3.8/site-packages/torch/lib/libtorch_cuda_cpp.so\n#7  0x000014750bb04c69 in c10d::ProcessGroupNCCL::workCleanupLoop() ()\n   from.../python3.8/site-packages/torch/lib/libtorch_cuda_cpp.so\n#8  0x000014751b88cba3 in execute_native_thread_routine () from /lib64/libstdc++.so.6\n#9  0x000014753a3901cf in start_thread () from /lib64/libpthread.so.0\n#10 0x0000147539872dd3 in clone () from /lib64/libc.so.6\nand there you go. How do you make sense of it?\nWell, you go from the bottom of the stack to the top. You can tell that a clone call was made in libc which then called start_thread in libpthread and then if you keep going there are a bunch of calls in the torch libraries and finally we can see that the program terminated itself, completing with raise from libc which told the Linux kernel to kill the program and create the core file.\nThis wasn‚Äôt an easy to understand backtrace.\nfootnote: Yes, python calls it a traceback and elsewhere it‚Äôs called a backtrace - it‚Äôs confusing, but it‚Äôs more or less the same thing.\nActually I had to ask pytorch devs for help and received:\n\nPyTorch ProcessGroup watchdog thread caught an asynchronous error from NCCL\nThis error is an ‚Äúunhandled system error‚Äù which in this particular case turned out to be an IB-OPA error\nThe ProcessGroup‚Äôs WorkCleanUp thread rethrew the error so that the main process would crash and the user would get notified (otherwise this async error would not surface)\n\nTrust me there are times when even if you‚Äôre inexperienced the backtrace can give you enough of a hint to where you should look for troubleshooting.\nBut fear not - most of the time you won‚Äôt need to understand the traceback. Ideally you‚Äôd just attach the core file to your filed Issue. But it can easily be 5GB large. So the developers that will be trying to help you will ask you to generate a gdb backtrace and now you know how to do that.\nI didn‚Äôt promise it‚Äôll be easy, I just showed you where to start.\nNow another useful details is that many programs these days run multiple threads. And bt only shows the main thread of the process. But, often, it can be helpful to see where other threads in the process were when segfault has happened. For that you simply type 2 commands at the (gdb) prompt:\n(gdb) thread apply all bt\n(gdb) bt\nand this time around you typically will get a massive report, one backtrace per thread.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#py-spy",
    "href": "qmd/debug/pytorch.html#py-spy",
    "title": "",
    "section": "py-spy",
    "text": "py-spy\nThis is a super-useful tool for analysing hanging programs. For example, when a you have a resource deadlock or there is an issue with a network connection.\nYou will find an exhaustive coverage of this tool here.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#strace",
    "href": "qmd/debug/pytorch.html#strace",
    "title": "",
    "section": "strace",
    "text": "strace\nSimilar to py-spy, strace is a super-useful tool which traces any running application at the low-level system calls - e.g.¬†libC and alike.\nFor example, run:\nstrace python -c \"print('strace')\"\nand you will see everything that is done at the system call level as the above program runs.\nBut usually it‚Äôs more useful when you have a stuck program that spins all CPU cores at 100% but nothing happens and you want to see what‚Äôs it doing. In this situation you simply attached to the running program like so:\nstrace --pid PID\nwhere you get the PID for example from the output of top or ps. Typically I just copy-n-paste the PID of the program that consumes the most CPU - top usually shows it at the very top of its listing.\nSame as py-spy you may need sudo perms to attached to an already running process - it all depends on your system setup. But you can always start a program with strace as I have shown in the original example.\nLet‚Äôs look at a small sub-snippet of the output of strace python -c \"print('strace')\"\nwrite(1, \"strace\\n\", 7strace\n)                 = 7\nHere we can see that a write call was executed on filedescriptor 1, which almost always is stdout (stdin being 0, and stderr being 2).\nIf you‚Äôre not sure what a filedescriptor is pointing to, normally you can tell from strace‚Äôs output itself. But you can also do:\nls -l /proc/PID/fd\nwhere PID is the pid of the currently running program you‚Äôre trying to investigate.\nFor example, when I run the above while running a pytest test with gpus, I got (partial output):\nl-wx------ 1 stas stas 64 Mar  1 17:22 5 -&gt; /dev/null\nlr-x------ 1 stas stas 64 Mar  1 17:22 6 -&gt; /dev/urandom\nlrwx------ 1 stas stas 64 Mar  1 17:22 7 -&gt; /dev/nvidiactl\nlrwx------ 1 stas stas 64 Mar  1 17:22 8 -&gt; /dev/nvidia0\nlr-x------ 1 stas stas 64 Mar  1 17:22 9 -&gt; /dev/nvidia-caps/nvidia-cap2\nso you can see that a device /dev/null is open as FD (file descriptor) 5, /dev/urandom as FD 6, etc.\nNow let‚Äôs go look at another snippet from our strace run.\naccess(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\nHere it tried to see if file /etc/ld.so.preload exists, but as we can see it doesn‚Äôt - this can be useful if some shared library is missing - you can see where it‚Äôs trying to load it from.\nLet‚Äôs try another one:\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libpthread.so.0\", O_RDONLY|O_CLOEXEC) = 3\nread(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 832) = 832\nnewfstatat(3, \"\", {st_mode=S_IFREG|0644, st_size=21448, ...}, AT_EMPTY_PATH) = 0\nmmap(NULL, 16424, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f8028807000\nmmap(0x7f8028808000, 4096, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1000) = 0x7f8028808000\nmmap(0x7f8028809000, 4096, PROT_READ, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x2000) = 0x7f8028809000\nmmap(0x7f802880a000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x2000) = 0x7f802880a000\nclose(3)\nhere we can see that it opens /lib/x86_64-linux-gnu/libpthread.so.0 and assigns it FD 3, it then reads 832 chars from FD 3, (we can also see that the first chars are ELF - which stands for a shared library format), then memory maps it and closes that file.\nIn this following example, we see a python cached file is opened, its filepointer is moved to 0, and then it‚Äôs read and closed.\nopenat(AT_FDCWD, \"/home/stas/anaconda3/envs/py38-pt113/lib/python3.8/__pycache__/abc.cpython-38.pyc\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0664, st_size=5329, ...}) = 0\nlseek(3, 0, SEEK_CUR)                   = 0\nlseek(3, 0, SEEK_CUR)                   = 0\nfstat(3, {st_mode=S_IFREG|0664, st_size=5329, ...}) = 0\nbrk(0x23bf000)                          = 0x23bf000\nread(3, \"U\\r\\r\\n\\0\\0\\0\\0\\24\\216\\177c\\211\\21\\0\\0\\343\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 5330) = 5329\nread(3, \"\", 1)                          = 0\nclose(3)\nIt‚Äôs important to notice that file descriptors are re-used, so we have seen the same FD 3 twice, but each time it was open to a different file.\nIf your program is for example trying to reach to the Internet, you can also tell these calls from strace as the program would be reading from a socket file descriptor.\nSo let‚Äôs run an example on a program that downloads files from the HF hub:\nstrace python -c 'import sys; from transformers import AutoConfig; AutoConfig.from_pretrained(sys.argv[1])' t5-small\nhere is some relevant to this discussion snippet:\nsocket(AF_INET6, SOCK_STREAM|SOCK_CLOEXEC, IPPROTO_TCP) = 3\nsetsockopt(3, SOL_TCP, TCP_NODELAY, [1], 4) = 0\nioctl(3, FIONBIO, [1])                  = 0\nconnect(3, {sa_family=AF_INET6, sin6_port=htons(443), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"2600:1f18:147f:e850:e203:c458:10cd:fc3c\n\", &sin6_addr), sin6_scope_id=0}, 28) = -1 EINPROGRESS (Operation now in progress)\npoll([{fd=3, events=POLLOUT|POLLERR}], 1, 10000) = 1 ([{fd=3, revents=POLLOUT}])\ngetsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\n[...]\nwrite(3, \"\\26\\3\\3\\0F\\20\\0\\0BA\\4\\373m\\244\\16\\354/\\334\\205\\361j\\225\\356\\202m*\\305\\332\\275\\251\\17J\"..., 126) = 126\nread(3, 0x2f05c13, 5)                   = -1 EAGAIN (Resource temporarily unavailable)\npoll([{fd=3, events=POLLIN}], 1, 9903)  = 1 ([{fd=3, revents=POLLIN}])\nread(3, \"\\24\\3\\3\\0\\1\", 5)               = 5\nread(3, \"\\1\", 1)                        = 1\nread(3, \"\\26\\3\\3\\0(\", 5)                = 5\nread(3, \"\\0\\0\\0\\0\\0\\0\\0\\0\\344\\v\\273\\225`\\4\\24m\\234~\\371\\332%l\\364\\254\\34\\3472&lt;\\0356s\\313\"..., 40) = 40\nioctl(3, FIONBIO, [1])                  = 0\npoll([{fd=3, events=POLLOUT}], 1, 10000) = 1 ([{fd=3, revents=POLLOUT}])\nwrite(3, \"\\27\\3\\3\\1.\\0\\374$\\361\\217\\337\\377\\264g\\215\\364\\345\\256\\260\\211$\\326pkR\\345\\276,\\321\\221`-\"..., 307) = 307\nioctl(3, FIONBIO, [1])                  = 0\nread(3, 0x2ef7283, 5)                   = -1 EAGAIN (Resource temporarily unavailable)\npoll([{fd=3, events=POLLIN}], 1, 10000) = 1 ([{fd=3, revents=POLLIN}])\nYou can see where that again it uses FD 3 but this time it opens a INET6 socket instead of a file. You can see that it then connects to that socket, polls, reads and writes from it.\nThere are many other super useful understandings one can derive from using this tool.\nBTW, if you don‚Äôt want to scroll up-down, you can also save the output to a file:\nstrace -o strace.txt python -c \"print('strace')\"\nNow, since you‚Äôre might want to strace the program from the very beginning, for example to sort out some race condition on a distributed filesystem, you will want to tell it to follow any forked processes. This what the -f flag is for:\nstrace -o log.txt -f python -m torch.distributed.run --nproc_per_node=4 --nnodes=1 --tee 3 test.py\nSo here we launch 4 processes and will end up running strace on at least 5 of them - the launcher plus 4 processes (each of which may spawn further child processes).\nIt will conveniently prefix each line with the pid of the program so it should be easy to tell which system was made by which process.\nBut if you want separate logs per process, then use -ff instead of -f.\nThe strace manpage has a ton of other useful options.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#invoke-pdb-on-a-specific-rank-in-multi-node-training",
    "href": "qmd/debug/pytorch.html#invoke-pdb-on-a-specific-rank-in-multi-node-training",
    "title": "",
    "section": "Invoke pdb on a specific rank in multi-node training",
    "text": "Invoke pdb on a specific rank in multi-node training\nOnce pytorch 2.2 is released you will have a new handy debug feature:\nimport torch.distributed as dist\n[...]\n\ndef mycode(...):\n\n   dist.breakpoint(0)\nThis is the same as ForkedPdb (below) but will automatically break for you on the rank of your choice - rank0 in the example above. Just make sure to call up;;n right away when the breakpoint hits to get into your normal code.\nHere is what it does underneath:\nimport sys\nimport pdb\n\nclass ForkedPdb(pdb.Pdb):\n    \"\"\"\n    PDB Subclass for debugging multi-processed code\n    Suggested in: https://stackoverflow.com/questions/4716533/how-to-attach-debugger-to-a-python-subproccess\n    \"\"\"\n    def interaction(self, *args, **kwargs):\n        _stdin = sys.stdin\n        try:\n            sys.stdin = open('/dev/stdin')\n            pdb.Pdb.interaction(self, *args, **kwargs)\n        finally:\n            sys.stdin = _stdin\n\n\ndef mycode():\n\n    if dist.get_rank() == 0:\n        ForkedPdb().set_trace()\n    dist.barrier()\nso you can code it yourself as well.\nAnd you can use that ForkedPdb code for normal forked applications, minus the dist calls.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/pytorch.html#floating-point-math-discrepancies-on-different-devices",
    "href": "qmd/debug/pytorch.html#floating-point-math-discrepancies-on-different-devices",
    "title": "",
    "section": "Floating point math discrepancies on different devices",
    "text": "Floating point math discrepancies on different devices\nIt‚Äôs important to understand that depending on which device the floating point math is performed on the outcomes can be different. For example doing the same floating point operation on a CPU and a GPU may lead to different outcomes, similarly when using 2 different GPU architectures, and even more so if these are 2 different types of accelerators (e.g.¬†NVIDIA vs.¬†AMD GPUs).\nHere is an example of discrepancies I was able to get doing the same simple floating point math on an 11 Gen Intel i7 CPU and an NVIDIA A100 80GB (PCIe) GPU:\nimport torch\n\ndef do_math(device):\n    inv_freq = (10 ** (torch.arange(0, 10, device=device) / 10))\n    print(f\"{inv_freq[9]:.20f}\")\n    return inv_freq.cpu()\n\na = do_math(torch.device(\"cpu\"))\nb = do_math(torch.device(\"cuda\"))\n\ntorch.testing.assert_close(a, b, rtol=0.0, atol=0.0)\nwhen we run it we get 2 out of 10 elements mismatch:\n7.94328212738037109375\n7.94328308105468750000\n[...]\nAssertionError: Tensor-likes are not equal!\n\nMismatched elements: 2 / 10 (20.0%)\nGreatest absolute difference: 9.5367431640625e-07 at index (9,)\nGreatest relative difference: 1.200604771156577e-07 at index (9,)\nThis was a simple low-dimensional example, but in reality the tensors are much bigger and will typically end up having more mismatches.\nNow you might say that the 1e-6 discrepancy can be safely ignored. And it‚Äôs often so as long as this is a final result. If this tensor from the example above is now fed through a 100 layers of matmuls, this tiny discrepancy is going to compound and spread out to impact many other elements with the final outcome being quite different from the same action performed on another type of device.\nFor example, see this discussion - the users reported that when doing Llama-2-7b inference they were getting quite different logits depending on how the model was initialized. To clarify the initial discussion was about Deepspeed potentially being the problem, but in later comments you can see that it was reduced to just which device the model‚Äôs buffers were initialized on. The trained weights aren‚Äôt an issue they are loaded from the checkpoint, but the buffers are recreated from scratch when the model is loaded, so that‚Äôs where the problem emerges.\nIt‚Äôs uncommon that small variations make much of a difference, but sometimes the difference can be clearly seen, as in this example where the same image is produced on a CPU and an MPS device.\n\nThis snapshot and the commentary come from this PyTorch Issue thread.\nIf you‚Äôre curious where I pulled this code from - this is a simplified reduction of this original code in modeling_llama.py:\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Debugging PyTorch programs"
    ]
  },
  {
    "objectID": "qmd/debug/torch-distributed-hanging-solutions.html",
    "href": "qmd/debug/torch-distributed-hanging-solutions.html",
    "title": "",
    "section": "",
    "text": "üêõ DebuggingDiagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs"
    ]
  },
  {
    "objectID": "qmd/debug/torch-distributed-hanging-solutions.html#helper-tools",
    "href": "qmd/debug/torch-distributed-hanging-solutions.html#helper-tools",
    "title": "",
    "section": "Helper tools",
    "text": "Helper tools\nTry to use the following script torch-distributed-gpu-test.py to diagnose the situation.\nThis will help primarily with discovering network-related issues. And also to quickly understand how multi-gpu communications work.\nFor code-related issues read the rest of this document.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs"
    ]
  },
  {
    "objectID": "qmd/debug/torch-distributed-hanging-solutions.html#approaches-to-diagnosing-multi-gpu-hanging-deadlocks",
    "href": "qmd/debug/torch-distributed-hanging-solutions.html#approaches-to-diagnosing-multi-gpu-hanging-deadlocks",
    "title": "",
    "section": "Approaches to diagnosing multi-gpu hanging / deadlocks",
    "text": "Approaches to diagnosing multi-gpu hanging / deadlocks\n\npy-spy\nFirst do pip install py-spy.\nNow you can attach to each process with:\npy-spy dump -n -p PID\nand it will tell you where the process hangs (very often it‚Äôs a nccl collective function or a barrier).\n\nPID is the process id of the hanging python process.\n-n is useful if you want to see stack traces from python extensions written in C, C++, etc., as the program may hang in one of the extensions\nyou may need to add sudo before the command - for more details see this note.\n\nIf you have no sudo access your sysadmin might be able to perform this for you:\nsudo echo 0 &gt; /proc/sys/kernel/yama/ptrace_scope\nwhich will allow you running py-spy (and strace) without needing sudo. Beware of the possible security implications - but typically if your compute node is inaccessible from the Internet it‚Äôs less likely to be a risk.\nTo make this change permanent edit /etc/sysctl.d/10-ptrace.conf and set:\nkernel.yama.ptrace_scope = 0\nHere is an example of py-spy dump python stack trace:\nThread 835995 (active): \"MainThread\"\n    broadcast (torch/distributed/distributed_c10d.py:1191)\n    _aggregate_total_loss (deepspeed/runtime/pipe/engine.py:540)\n    train_batch (deepspeed/runtime/pipe/engine.py:330)\n    train_step (megatron/training.py:436)\n    train (megatron/training.py:851)\n    pretrain (megatron/training.py:187)\n    &lt;module&gt; (pretrain_gpt.py:239)\nThe very first line is where the program is stuck.\nIf the hanging happens inside a CPP extension, add --native py-spy and it‚Äôll show the non-python code if any.\n\nmulti-process py-spy\nNow, how do you do it for multiple processes. Doing it one-by-one is too slow. So let‚Äôs do it at once.\nIf the launch command was python, what you do is:\npgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}\nif deepspeed:\npgrep -P $(pgrep -o deepspeed) | xargs -I {} py-spy dump --pid {}\nfor accelerate:\npgrep -P $(pgrep -o accelerate) | xargs -I {} py-spy dump --pid {}\nyou get the idea.\nThis particular approach will only analyse the main processes and not various other sub-processes/threads spawned by these processes. So if you have 8 gpus and 8 processes, the above will generate 8 stack traces.\nIf you want all processes and their subprocesses, then you‚Äôd just run:\npgrep -f python | xargs -I {} py-spy dump --pid {}\n(and as before replace python with the name of the launcher program if it‚Äôs not python)\n\n\nmulti-node py-spy via srun\nWhat if you have multiple nodes?\nYou can of course ssh to each node interactively and dump the stack traces.\nIf you‚Äôre using the SLURM environment you can use srun to do it on all nodes for you.\nNow in another console get the SLURM_JOBID (or get it from salloc log):\nsqueue -u `whoami` -o \"%.16i %9P %26j %.8T %.10M %.8l %.6D %.20S %R\"\nNow use the following srun command after adjusting jobid with SLURM_JOBID from the outcome of the command above this sentence:\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 --output=trace-%N.out sh -c 'ps aux | grep python | egrep -v \"grep|srun\" | grep `whoami` | awk \"{print \\$2}\" | xargs -I {} py-spy dump --native --pid {}' || echo \"failed\"\nNotes: - One must use --gres=gpu:0 for the monitor srun or otherwise it will block until the main srun (the one running the training) exits. - Each node will generate its unique log file named trace-nodename.out - so this would help to identify which node(s) are problematic. You can remove --output=trace-%N.out if you want it all being dumped to stdout - In some SLURM versions you may also need to add --overlap - In some SLURM versions the jobid might not match that of reported in squeue, so you have to get the correct SLURM_JOB_ID from the logs of the job you‚Äôre trying to ‚Äúattach‚Äù to - i.e.¬†your srun job that allocated the GPUs. - Sometimes bash doesn‚Äôt work, but sh does. I think it has to do with what dot files get sourced - You might need to also activate a custom python environment, which you can do like so:\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 --output=trace-%N.out sh -c 'conda activate myenvname; ps auxc | ... ' || echo \"failed\"\nor you can do it inside ~/.bashrc or whatever shell‚Äôs rc file you decide to use.\nAs mentioned before if you want just the main processes you‚Äôd use this instead:\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 --output=trace-%N.out sh -c 'pgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}' || echo \"failed\"\nAdjust python if need be as explained in the multi-gpu section above.\nThe previous longer command will deliver traces for all python processes.\nIf you‚Äôre not getting anything, start with the basic debug like:\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 --output=trace-%N.out sh -c 'date'\nonce you know you‚Äôre talking to all the nodes, then you can progressively unravel the depth of calls, as in:\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 sh -c 'date'\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 sh -c 'pgrep -o python'\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 sh -c 'pgrep -P $(pgrep -o python) '\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 sh -c 'pgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}'\nand at each stage check that the output makes sense - e.g.¬†the 2nd and 3rd call you should be getting the PIDs of the processes.\n\n\nmulti-node py-spy via pdsh\npdsh seems to be a good easy tool to use to accomplish remote work on multiple nodes. Say, you‚Äôre running on 2 nodes with hostnames nodename-5 and nodename-8, then you can quickly test that remote execution is working by getting the date on all of these hosts with just:\n$ PDSH_RCMD_TYPE=ssh pdsh -w nodename-[5,8] \"date\"\nnodename-5: Wed Oct 25 04:32:43 UTC 2023\nnodename-8: Wed Oct 25 04:32:45 UTC 2023\nfootnote: pdsh should be available via a normal OS package installer\nOnce you tested that date works it‚Äôs time to move to py-spy.\nTo do py-spy on all python processes that are sub-processes, it‚Äôd be:\nPDSH_RCMD_TYPE=ssh pdsh -w nodename-[5,8] 'pgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}'\nbut as you‚Äôre likely to need to have the ~/.bashrc run, you will need to clone it into ~/.pdshrc, reduce that clone to what is needed to be run (e.g.¬†modify PATH, activate conda) and then source it, like:\nPDSH_RCMD_TYPE=ssh pdsh -w nodename-[5,8] 'source ~/.pdshrc; pgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}\"'\nThe reason you need a startup script is because usually ~/.bashrc starts with:\n# If not running interactively, don't do anything\ncase $- in\n    *i*) ;;\n      *) return;;\nesac\nso when you run such non-interactive workflows Bash won‚Äôt process its ~/.bashrc normally (exit early) and thus anything relying on this startup script won‚Äôt work. So you can either remove the non-interactive exiting code above or fork ~/.bashrc into a startup file that only contains what‚Äôs needed for the remote command to succeed.\nfootnote: there is nothing special about ~/.pdshrc - any other name would do, since you‚Äôre manually sourceing it.\nAnd if your system isn‚Äôt setup to run py-spy w/o sudo as explained a few sections up, you‚Äôd need something like this:\nPDSH_RCMD_TYPE=ssh pdsh -w nodename-[5,8] 'sudo bash -c \"source ~/.pdshrc; pgrep -P $(pgrep -o python) | xargs -I {} py-spy dump --pid {}\"'\nOf course, you may need to edit the pgrep section to narrow down which processes you want to watch.\nAdditionally, to avoid being prompted with:\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nfor every new node you haven‚Äôt logged into yet, you can disable this check with:\necho \"Host *\" &gt;&gt; ~/.ssh/config\necho \"  StrictHostKeyChecking no\" &gt;&gt; ~/.ssh/config\nHere I assume you‚Äôre on an isolated cluster so you don‚Äôt need to worry about security issues and thus bypassing such check is most likely OK.\n\n\nmulti-node py-spy via ds_ssh\nThis is yet another way, but please make sure to read the pdsh section above first.\nThe following notes require pip install deepspeed.\nIn one SLURM environment I also attempted using pdsh via ds_ssh, but somehow I wasn‚Äôt able to run py-spy remotely - the main issue was that remote ssh command wasn‚Äôt giving the same env as when I was logged in interactively via ssh. But if you have sudo access on the compute nodes then you could do:\nFirst prepare hostfile:\nfunction makehostfile() {\nperl -e '$slots=split /,/, $ENV{\"SLURM_STEP_GPUS\"};\n$slots=8 if $slots==0; # workaround 8 gpu machines\n@nodes = split /\\n/, qx[scontrol show hostnames $ENV{\"SLURM_JOB_NODELIST\"}];\nprint map { \"$_ slots=$slots\\n\" } @nodes'\n}\nmakehostfile &gt; hostfile\nAdapt $slots to the number of gpus per node. You may have to adapt this script if your scontrol produces a different output.\nNow run the py-spy extraction command over all participating nodes:\nds_ssh -f hostfile \"source ~/.pdshrc; ps aux | grep python | grep -v grep | grep `whoami` | awk '{print \\$2}' | xargs -I {} sudo py-spy dump --pid {} \"\nNotes: - Put inside ~/.pdshrc whatever init code that you may need to run. If you don‚Äôt need any you can remove source ~/.pdshrc; from the command line. - If you don‚Äôt have it already ds_ssh is installed when you do pip install deepspeed. - you might need to export PDSH_RCMD_TYPE=ssh if you get rcmd: socket: Permission denied error\n\n\n\nNetwork-level hanging\nThe hanging could be happening at the network level. NCCL_DEBUG=INFO can help here.\nRun the script with NCCL_DEBUG=INFO env var and try to study the outcome for obvious errors. It will tell you which device it‚Äôs using, e.g.:\nDeepWhite:21288:21288 [0] NCCL INFO NET/Socket : Using [0]enp67s0:192.168.50.21&lt;0&gt;\nSo it‚Äôs using interface enp67s0 over 192.168.50.21\nIs your 192.168.50.21 firewalled? or is it somehow a misconfigured network device?\nDoes it work if you use a loopback device 127.0.0.1?\nNCCL_DEBUG=INFO NCCL_SOCKET_IFNAME=lo python -m torch.distributed.run --nproc_per_node 4 --nnodes 1 torch-distributed-gpu-test.py\nif not, see what other local network devices you have via ifconfig - try that instead of lo if any.\nIt‚Äôs currently using enp67s0 in the above example.\n\n\nIsolate problematic GPUs\nYou can also try to see if only some GPUs fail\nFor example, does it work if you use the first 2 or the last 2 gpus:\nCUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\nthen the 2nd pair:\nCUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n\n\npython trace\nNow what happens when the training doesn‚Äôt just hang, but the hanging process stops responding? e.g.¬†this happens when there is a serious hardware issue. But what if it is recurrent and py-spy won‚Äôt help here, since it won‚Äôt be able to attach to a process that is not responding.\nSo next came the idea of tracing all calls like one does with strace(1), I researched python calls tracing facilities and have discovered that python has a trace sub-system.\nThe following code will trace all python calls and log them to the console and into a dedicated per process log file, via a custom Tee module I added.\nThis then can help to understand where some processes stopped responding, since we will have the log of the last call and all the previous calls before it went unresponsive.\n$ cat train.py\n[...]\n\ndef main():\n    # [...]\n    train()\n\nimport re\nclass Tee:\n    \"\"\"\n    A helper class to tee print's output into a file.\n    Usage:\n    sys.stdout = Tee(filename)\n    \"\"\"\n\n    def __init__(self, filename):\n        self.stdout = sys.stdout\n        self.file = open(filename, \"a\")\n\n    def __getattr__(self, attr):\n        return getattr(self.stdout, attr)\n\n    def write(self, msg):\n        self.stdout.write(msg)\n        self.file.write(msg)\n        self.file.flush()\n\n    def flush(self):\n        self.stdout.flush()\n        self.file.flush()\n\nif __name__ == \"__main__\":\n\n    import sys\n    import trace\n    import socket\n    import os\n\n    # enable the trace\n    if 0:\n        cwd = os.path.realpath('.')\n        pid = os.getpid()\n        hostname = socket.gethostname()\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        trace_output_file = f\"{cwd}/trace-{hostname}-{local_rank}-{pid}.txt\"\n\n        # create a Trace object, telling it what to ignore, and whether to\n        # do tracing or line-counting or both.\n        tracer = trace.Trace(\n            ignoredirs=[sys.prefix, sys.exec_prefix],\n            trace=1,\n            count=1,\n            timing=True,\n        )\n\n        # run the new command using the given tracer\n        sys.stdout = Tee(trace_output_file)\n        tracer.run('main()')\n    else:\n        main()\nThis code doesn‚Äôt require any special handing other than enabling the trace by changing if 0 to if 1.\nIf you don‚Äôt set ignoredirs, this will now dump all python calls. Which means expect a lot of GBs of data logged, especially if you have hundreds of GPUs.\nOf course, you don‚Äôt have to start tracing from main - if you suspect a specific are you can start tracing there instead and it‚Äôll be much faster and less data to save.\nI wish I could tell trace which packages to follow, but alas it only supports dirs to ignore, which is much more difficult to set, and thus you end up with a lot more data than needrf. But still this is a super useful tool for debugging hanging processes.\nAlso, your code will now run much much slower and the more packages you trace the slower it will become.\n\nNicerTrace\nAs Trace proved to provide very limited usability when debugging a complex multi-node multi-hour run crash, I have started on working on a better version of the trace python module.\nYou can find it here: NicerTrace\nI added multiple additional flags to the constructor and made the output much more useful. You fill find a full working example in that same file, just run:\npython trace/NicerTrace.py\nand you should see:\n        trace/NicerTrace.py:1 &lt;module&gt;\n0:00:00 &lt;string&gt;:     1:         trace/NicerTrace.py:185 main\n0:00:00 NicerTrace.py:   186:     img = Image.new(\"RGB\", (4, 4))\n        PIL.Image:2896 new\n0:00:00 Image.py:  2912:     _check_size(size)\n        PIL.Image:2875 _check_size\n0:00:00 Image.py:  2883:     if not isinstance(size, (list, tuple)):\n0:00:00 Image.py:  2886:     if len(size) != 2:\n0:00:00 Image.py:  2889:     if size[0] &lt; 0 or size[1] &lt; 0:\nas you will see in the example I set:\n            packages_to_include=[\"PIL\"],\nso it‚Äôll trace PIL plus anything that is not under site-packages. If you need to trace another package, just add it to that list.\nThis is a very fresh work-in-progress package, so it‚Äôs evolving as we are trying to make it help us resolve a very complex crashing situation.\n\n\nWorking with generated trace files\nWhen the per-node-rank trace files has been generated the following might be helpful to quickly analyse the situation:\n\ngrep for a specific match and also print the file and line number where it was found:\n\ngrep -n \"backward\" trace*\n\nshow tail -1 of all trace files followed by the name of each file:\n\nfind . -name \"trace*\" -exec sh -c 'echo \"$1: $(tail -3 \"$1\")\"' _ {} \\;\n\nor similar to the above, but print 5 last lines with the leading filename and some vertical white space for an easier reading:\n\nfind . -name \"trace*\" -exec sh -c 'echo; echo $1; echo \"$(tail -5 \"$1\")\"' _ {} \\;\n\ncount how many times grep matched a given pattern in each ifle and print the matched file (in this example matching the pattern backward):\n\nfind . -name \"trace*\" -exec sh -c 'echo \"$1: $(grep \"backward\" $1 | wc -l)\"' _ {} \\;\n\n\n\ngood old print\nNow once you discovered where the hanging happens to further understand why this is happening, a debugger would ideally be used, but more often than not debugging multi-process (multi-node) issues can be very difficult.\nIn such situations a good old print works. You just need to add some debug prints before the calls where things hang, things that would help understand what lead to the deadlock. For example, some barrier was missing and one or a few processes skipped some code and while the rest of processes are still blocking waiting for everybody to send some data (for example in NCCL collective functions like gather or reduce).\nYou of course, want to prefix each print with the rank of the process so that you could tell which is which. For example:\nimport torch.distributed as dist\nprint(f\"{dist.get_rank()}: passed stage 0\")\nWhat you will quickly discover is that if you have multiple GPUs these prints will be badly interleaved and you will have a hard time making sense of the debug data. So let‚Äôs fix this. We are going to override print with a custom version of the same, but which uses flock to ensure that only one process can write to stdout at the same time.\nThe helper module printflock.py is included here. To activate it just run this at the top of the module you‚Äôre debugging:\nfrom printflock import printflock as print\nand now all your print calls in that module will magically be non-iterleaved. You can of course, just use printflock directly:\nfrom printflock import printflock\nimport torch.distributed as dist\nprintflock(f\"{dist.get_rank()}: passed stage 0\")\n\n\ncore files\nIf the hanging happens inside non-python code, and py-spy --native isn‚Äôt enough for some reason you can make the hanging program dump a core file, which is done with one of these approaches:\ngcore &lt;pid&gt;\nkill -ABRT &lt;pid&gt;\nand then you can introspect the core file as explained here.\nIf you don‚Äôt get the core file dumped you need to configure your system to allow so and also specify where the core files should be saved to.\nTo ensure the file is dumped in bash run (other shells may use a different command):\nulimit -c unlimited\nTo make this persistent run:\necho '* soft core unlimited' &gt;&gt; /etc/security/limits.conf\nOn some systems like Ubuntu the core files are hijacked by apport, check the contents of /proc/sys/kernel/core_pattern to see where they are sent. You can override where they are sent with:\nsudo sysctl -w kernel.core_pattern=/tmp/core-%e.%p.%h.%t\nChange the directory if you want to, but make sure that the user the program is running under can write to that directory. To make this change permanent edit /etc/sysctl.conf and add kernel.core_pattern=/tmp/core-%e.%p.%h.%t (or modify if it‚Äôs already there).\nfootnote: see man core for all the different templates available\nIf on Ubuntu by default it sends core files to apport, which may save the core to /var/lib/apport/coredump or /var/crash. But you can change it explained above.\nA quick way to test if your setup can generate a core file is:\nsleep 10 &\nkillall -SIGSEGV sleep\nNormally SIGSEGV isn‚Äôt recommended for a real situation of diagnosing a hanging program, because SIGSEGV is likely to launch a sighandler, but for this test it‚Äôs good enough.\n\n\nCode loops\nCode loops can be tricky to debug in hanging scenarios. If you have code like the following:\nfor i, d in enumerate(data):\n    some_hanging_call(d)\nit‚Äôs possible that one process hangs in the first iteration, and another process in the second iteration, which makes things very confusing. But the stack trace won‚Äôt give such indication, as the line numbers would be the same, even though the processes aren‚Äôt in the same place code progression-wise.\nIn such situations unroll the loop to be:\nd_iter = iter(data)\nsome_hanging_call(next(d_iter)\nsome_hanging_call(next(d_iter)\nand now when you run py-spy the line numbers will be correct. The processes hanging in the first iteration will report the first some_hanging_call and those in the second iteration in the second call - as each now has its own line.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs"
    ]
  },
  {
    "objectID": "qmd/debug/torch-distributed-hanging-solutions.html#hardware-specific-issues",
    "href": "qmd/debug/torch-distributed-hanging-solutions.html#hardware-specific-issues",
    "title": "",
    "section": "Hardware-specific issues",
    "text": "Hardware-specific issues\n\nAMD/ROCm hangs or slow with IOMMU enabled\nAMD Instinct users may need to either Disable IOMMU or set it to:\nGRUB_CMDLINE_LINUX_DEFAULT=\"iommu=soft\"\nin /etc/default/grub (the grub config file could be elsewhere depending on the OS).\nDisabling is GRUB_CMDLINE_LINUX=\"amd_iommu=off\"",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging",
      "Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html",
    "href": "qmd/insights/ai-battlefield.html",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "This chapter is one person‚Äôs opinionated overview of the ML/AI Engineering reality, which may or may not be another person‚Äôs reality. The intention is to help you start asking the right questions and get your ML Engineering needs met.\n\n\n\n\nTraining:\n\nHow fast one can train a better model (first to market advantage)\nHow much $$ was spent (do we still have money left to pay salaries to talent after training?)\n\nInference:\n\nFast latency (users are used to msec response times and will leave if the response takes seconds)\nFast throughput (how many concurrent queries can be processed)\nHow much $$ is being spent per user (can we rent more GPUs to acquire more users and/or improve (1) and (2)?)\n\n\n\n\n\nFast compute massively dominated by matrix multiplications\nFast enough memory, IO, network and CPU to feed the compute\n\nCorollary: If when you buy or rent hardware you invest in the fastest accelerators, but cheap out on any of the other components you wasted $$ and you might not win the race as it‚Äôll take longer to train.\n\n\n\n\nAn accelerator or a processing unit is what does most of the work.\nSince ML does a lot of parallel processing (SIMD) GPUs were used at the beginning, but now you additionally have TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs, etc. Recent CPUs are becoming used as accelerators as well, especially for inference.\n\nMore details.\n\n\n\n\nAI companies - train models/build products around self-trained or trained-by-others‚Äô models, in-house research.\nAcademia - does massive research and write papers. Lots of new ideas are generated.\nAI enthusiasts - lots of good will available, some pull resources/talents together to train open access models, with donated compute by HPCs and an occasional cloud, or a university cluster.\nEntrepreneurs - lots of low hanging fruit to pick - creative reselling of services, making ML-driven apps, and using various ingenious combinations of available resources to create amazing outcomes.\n\n\n\n\n\nIt‚Äôs very surprising that almost everybody involved in the domain of AI shares a lot of the discoveries with the community.\nSurely, companies don‚Äôt disclose all of their IP, but a lot of it does get shared in the form of knowledge or model weights\nCompanies that publish a lot of IP and models tend to attract higher quality talent.\nTwitter seems to be the central platform where one must be to follow what‚Äôs going on\n\n\n\n\n\nThe Dot-com bubble occurred during 1995-2000. And a very similar situation is happening right now in the AI space.\nThere is a lot of money available to create new startups or boost the existing companies. It‚Äôs relatively easy to raise millions of dollars.\nAs we are in the wild-wild-west stage of the AI industry it‚Äôs very difficult to predict the future, and so pretty much anything goes as far as startup ideas go, as long as it sounds reasonable.\nWhat distinguishes the AI bubble from the Dot-com bubble, is that one didn‚Äôt actually need much money to operate a Dot-com company - most of the raised money went to marketing and some to staff, barely any to compute. AI companies need millions of dollars because training LLMs requires an insane amount of compute, and that compute is very expensive. e.g.¬†1x NVIDIA H100 costs ~$30k and a company may need 512 of those, which is $15M (not counting the other hardware components and related costs)!\n\n\n\n\n\nThis is my personal LLM/VLM trainings-based heaven and hell. YMMV.\n\n\n\nA well built HPC, or a full service cloud based cluster, where someone diligently and timely takes care of the hardware and the systems.\nI just need to bring my training software and do the training, which is already an insanely complicated job requiring special skills.\nLots of nodes available for exclusive unlimited use\nFast inter-node connectivity that doesn‚Äôt bottleneck the accelerators and which isn‚Äôt shared with other users\nHuge local super-fast NVME based shared filesystem that can fit datasets and checkpoints\nBarebones Linux w/ SLURM and minimal software to be able to launch training jobs\nsudoer access to ease the work with a team of people\n\n\n\n\n\nA cloud or in-house cluster, where you have to do everything - sysadmining, replacing hardware, dealing with outages, etc. And to do the training on top of that.\nA smallish slow shared filesystem (NFS?), with cloud to draw data from and checkpoint to\nSlow inter-node leading to low accelerator utilization\nInter-node shared with other users which make the network erratic and unpredictable\nSuper-complicated cloud console with gazillion of screens and steps to set even simple things up\nNot being able to swap out failing hardware fast\nNeeding to timeshare the nodes - with wait times between training jobs\nHaving other concurrent users who might use up the whole disk, leading to trainings crashing\nNot being able to kill jobs others on the team started and went to sleep\n\n\n\n\n\nThere are 3 main choices to where one gets compute:\n\nRent on the cloud\nGet a timeshare on an HPC\nBuy it\n\n\n\nThis is currently the prevalent way of getting compute.\nPros:\n\nEasy to expand or contract the size of the cluster\nEasy to upgrade from the old hardware generation to the new one in a few years\nCluster management could be easily outsourced\n\nCons:\n\nExpensive, unless you negotiate a long term (1-3 year) contract for hundreds of accelerators\nYou will be tempted to buy many tools and services that you may or may not need\nYou always get charged whether you use your cluster fully or not\n\n\n\n\nThere aren‚Äôt that many HPCs out there and so the amount of available resources is limited.\nPros: - Managed for you - all you need is your software to do the training and a bit of SLURM know-how to launch jobs - Often sponsored by the local government/university - probably could get the job done for less $$ or even free (e.g.¬†we trained BLOOM-176B for free on JeanZay HPC!)\nCons: - needing to time share compute with other teams == short job times with possible long wait times in between - could be difficult to finish training quickly - The inter-node network is likely to be unstable as it‚Äôll be used by other teams - Have to abide by the HPC‚Äôs rules (e.g.¬†no sudo access and various other rules to follow) - In a way the HPC cluster will be what it‚Äôll be - you can‚Äôt make the network faster and often even getting some software installed can be tricky.\n\n\n\nIt‚Äôs mainly universities that buy and build their own clusters, and some big companies do that too.\nPros:\n\nIf you can deploy the hardware 24/7 for more than a few years the total cost will be cheaper than renting\nEasy to provide fast local storage - a good NVME raid would be much cheaper and faster than online storage\n\nCons:\n\nYou‚Äôre stuck with the outdated hardware just a few years after it was purchased - might be able to resell\nMust buy more than needed - Hardware tends to break, especially when it‚Äôs used 24/7, RMA could take weeks\nHave to hire talent to manage the in-house solution\nHave to figure out cooling, electric costs, insurance, etc.\n\n\n\n\n\n\n\nImagine a steam locomotive - the engine is great, but if the fireman isn‚Äôt fast enough to shovel the coal in, the train won‚Äôt move fast.\n\nsource\nThis is the current state of ML hardware: The bottleneck is in moving bits and not the compute.\n\nAccelerators get ~2x faster every 2 years (Moore‚Äôs law)\nNetwork and memory are not! Already now both are compute bottlenecks\nIO can be another bottleneck if your DataLoader has to pull data from the cloud\nCPU is fine as long as it has enough cpu-cores for DataLoader workers, and main processes\n\nCorollary: research the whole machine and not just its engine.\na crazy idea: the older GPUs might do fine if you can actually feed them as fast as they can compute. And if you can get 3x of them at the same cost as the next generation GPU you might finish training sooner and a lower cost.\n\n\n\n\nOnce you choose the architecture and the size of the model and how many tokens you want to train the model for you immediately know how much compute will be required to accomplish this goal. Specifically you can now calculate how many floating point operations will be needed.\nAll that is missing is comparing different compute providers to how many floating point operations their hardware can computes per secs (TFLOPS) and their cost per unit and now you can tell the total approximate cost of the training.\n\n\nCalculate the time needed to train given the TFLOPS of the considered solution:\ntotal_tflops_required / tflops_of_this_compute_unit = time_in_seconds\nLet‚Äôs say it came to be 604800 secs or 7 days.\nLook at the cost of using this compute solution for 7 days and now you know the total $$ to train this model.\nLook at other proposals and calculate the same - chose the best option.\n\n\nAs mentioned earlier, time is of a huge importance, so you might still choose a more expensive solution if finishing the training sooner is important because you want to be first to market.\n\nUnfortunately, this math is only partially correct because the advertised peak TFLOPS are typically unachievable. The MFU section delves into it.\n\n\n\nAs mentioned in the previous section, some (most?) vendors publish unrealistic peak performance TFLOPS - they aren‚Äôt possible to achieve.\nModel Flops Utilization (MFU) is the metric that tells us how well the accelerator is utilized. Here is how it is calculated:\n\nMeasure the actual TFLOPS by calculating how many floating point operations a single training iteration takes and dividing that number by the number of seconds this iteration took.\nDivide the actual TFLOPS by advertised TFLOPS to get the MFU\n\nExample: Let‚Äôs say you‚Äôre training in BFLOAT16 precision:\n\nIf a single iteration requires 624 Tera floating point operations and it took 4 secs to run then we know that we get: 624/4=156 actual TFLOPS\nnow BF16@A100 is advertised as 312TFLOPS so 156/312=0.5 gives us 50% MFU.\n\nPractically: - with NVIDIA GPUs if you‚Äôre above 50% MFU on a multi-node setup with a large model you‚Äôre already doing fantastic - recent advancements in more efficient scalability solutions keep on increasing MFU - slow networks and inefficient frameworks or untuned configuration lower MFU\nTherefore once you know the MFU you can now adjust the cost estimate from the previous section. In the example there we said it‚Äôll take 7 days to train, but if MFU is 50%, it means it‚Äôll take 14 days to train.\n\n\n\nWhy can‚Äôt the advertised TFLOPS achieved? It‚Äôs because it takes time to move data between accelerator memory and compute and additionally it takes even more time to move data from disk and other gpus to the accelerator‚Äôs memory.\n\nThere is not much can be done about the accelerator memory since its bandwidth is what it is - one can only write more efficient software to make data move faster to/from the accelerator - hint: fused and custom written kernels (like torch.compile and flash attention)\nIf you only have a single GPU and the model fits its memory, you don‚Äôt need to worry about the network - accelerator memory is the only bottleneck. But if you have to shard the model across multiple GPUs network becomes the bottleneck.\nIntra-node Network - is very fast, but difficult to take advantage of for large models - Tensor parallelism and sequence parallelism address part of this problem. (more).\nInter-node Network - typically is too slow on most server setups - thus this is the key component to research! Efficient frameworks succeed to partially hide the comms overhead by overlapping compute and comms. But if comms take longer than compute, the comms are still the bottleneck. more.\nStorage IO is important primarily for feeding the DataLoader workers and saving the checkpoints. more.\n\nTypically with enough DL workers the DataLoader adds very little overhead.\nWhile checkpoints are being saved the accelerators idle unless some async saving solution is used, so fast IO is crucial here\n\n\n\n\n\n\n\n\nAs of this writing here are the most common accelerators that can be used for training, finetuning and inference ML models:\nWidely available:\n\nNVIDIA A100 - huge availability across all clouds, but is already gradually being replaced by H100\n\nAvailable, but locks you in:\n\nGoogle TPUs - fast! but the cost is a lock-in into a single vendor and cloud\n\nEmerging to general availability:\n\nNVIDIA H100 - 2-3x faster than A100 (half precision), 6x faster for fp8\nAMD MI250 ~= A100 - very few clouds have them\nAMD MI300 ~= H100 - don‚Äôt expect until 1.5-2 years from now to be GA\nIntel Gaudi2 ~= H100 - starting to slowly emerge on Intel‚Äôs cloud\nGraphCore IPU - very difficult to find, paperspace has them\nCerebras WaferScale Engine - available on Cerebras‚Äô cloud\n\n\n\nIn general most (all?) accelerators are supported by major frameworks like PyTorch or TensorFlow and the same code should run everywhere with small modifications as long as it doesn‚Äôt use any accelerator-specific functionality.\nFor example, if your PyTorch application includes custom CUDA kernels it‚Äôll only work on NVIDIA GPUs and may be on AMD MI-series.\n\nNVIDIA GPUs: all based on CUDA, which most training frameworks support. You can easily moved between different NVIDIA GPUs and most things would work the same.\nAMD MI250/MI300: with PyTorch using ROCm you can run most CUDA-based software as is. This is really the only inter-operable accelerator with the NVIDIA stack.\nGaudi2: if you use HF Transformers/Diffusers you can use optimum-habana. If you use HF Trainer with NVIDIA GPUs it should be relatively easy to switch to train/infer on Gaudi2.\nGraphCore IPU: can also be run via PyTorch via poptorch\nCerebras: is also working on PyTorch support via Cerebras Software Platform (CSoft) via XLA.\n\nAlso in general most ML code could be compiled into cross-platform formats like Open Neural Network Exchange (ONNX) which can be run on a variety of accelerators. This approach is typically used more often for inference workloads.\n\n\n\n\n\nIf you want to train a large model that doesn‚Äôt fit onto a single accelerator‚Äôs memory you have to rely on the intra- and inter-node networks to synchronize multiple accelerators.\nThe biggest issue right now is that compute hardware advancements move faster than networking hardware, e.g.¬†for NVIDIA NVLink intra-node:\n\n\n\n\n\n\n\n\n\n\n\nGPU\nComputefp16TFLOPS\nComputespeedup\nIntra-nodeGBps\nIntra-nodespeedup\n\n\n\n\nV100\n125\n1\n300\n1\n\n\nA100\n312\n2.5\n600\n2\n\n\nH100\n989\n8\n900\n3\n\n\n\n\nYou can see that A100 was 2.5 faster than V100, and H100 is ~3x faster than A100. But the intra-node speed of NVLink has only increased by 300GBps each generation.\nMoreover, all 3 generations of NVLink use identical NICs of the same 50GBps duplex throughput. They have just doubled and tripled the number of links to speed things up. So there was 0 progress in that technology.\nThe inter-node situation isn‚Äôt any better with most NICs there doing 100 or 200Gbps, and some 400Gbps are starting to emerge. (correspondingly in GBps: 12.5, 25 and 50). It‚Äôs the same story here, some solutions provide dozens of NICs to get to higher speeds.\nAlso typically with LLMs the payload is so large that network latency is often negligible for training. It‚Äôs still quite important for inference.\n\n\n\n\nPay attention to bytes vs bits. 1Byte = 8bits. 1GBps = 8Gbps.\nIf you need to reduce bits (e.g.¬†gradients) across multiple nodes, it‚Äôs the slowest link (Inter-node) that defines the overall throughput, so intra-node speed doesn‚Äôt matter then\nTensor parallelism and sequence parallelism have to remain within the node to be efficient - only makes sense with fast intra-node speed\n\nNVIDIA:\n\nNVIDIA-based compute nodes come with 50GBps duplex NVLInk\nSome have a lot of NVLinks, others less but typically plenty w/ at least 900GBps (5.6Tbps) duplex for H100, 600GBps for A100 nodes\n\nIntel Gaudi2:\n\n8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps\n\nMore details\n\n\n\n\nAn order of magnitude slower than Intra-node\nYou will see a wide range of speeds from 50Gbps to 3200 Gbps\nYou need to reduce gradients and other bits faster than compute to avoid idling accelerators\nYou typically get at most 80% of advertised speed. e.g., if you are told you get 800Gbps, expect ~480Gbps.\nIf moving to fp8 H100 is 18x faster than V100\nWe are yet to see if 3200Gbps for H100s will be enough to keep high MFU.\nPractically less than 3x but it‚Äôs a good estimate\n\nMore details.\n\n\n\n\nThere are 3 distinct Storage IO needs in the ML workload:\n\nYou need to be able to feed the DataLoader fast - (super fast read, don‚Äôt care about fast write) - requires sustainable load for hours and days\nYou need to be able to write checkpoints fast - (super fast write, fastish read as you will be resuming a few times) - requires burst writing - you want super fast to not block the training for long (unless you use some sort of cpu offloading to quickly unblock the training)\nYou need to be able to load and maintain your codebase - (medium speed for both reading and writing) - this also needs to be shared since you want all nodes to see the same codebase - as it happens only during the start or resume it‚Äôll happen infrequently\n\n\nMost of the time you‚Äôre being sold 80% of what you paid. If you want a reliable 100TBs you need to rent 125TBs or your application may fail to write long before the disk is full.\nShared Distributed Filesystem:\n\nnon-parallel shared file systems can be extremely slow if you have a lot of small files (=Python!)\nYou want Parallel FS like GPFS (IBM Spectrum Scale) or Lustre (Open Source)\n\n\nMore details.\n\n\n\nYou need enough memory for:\n\n2-3 possibly DL workers per Accelerator (so 16-24 processes with 8 accelerators per node)\nEven more memory for DL workers if you pull data from the cloud\nEnough memory to load the model if you can‚Äôt load to accelerator directly\nOften used for accelerator memory offloading - extends accelerator‚Äôs memory by swapping out the currently unused layers - if that‚Äôs the target use, then the more cpu memory is available - the better!\n\n\n\n\nThis is probably the least worrisome component.\n\nMost clouds provide beefy CPUs with plenty of cpu cores\nYou need to have enough cores to run 2-3 DL workers +1 per gpu - so at least 30 cores\nEven more cores for DL workers if you have complex and/or slow DL transforms (CV)\nMost of the compute happens on GPUs\n\n\n\n\n\n\n\n\nTraining in half mixed-precision: model_size_in_B * 18 * 1.25 / gpu_size_in_GB\nInference in half precision: model_size_in_B * 2 * 1.25 /  gpu_size_in_GB\n\nThat‚Äôs the minimum, more to have a bigger batch size and longer sequence length.\nHere is the breakdown:\n\nTraining: 8 bytes for AdamW states, 4 bytes for grads, 4+2 bytes for weights\nInference: 2 bytes for weights (1 byte if you use quantization)\n1.25 is 25% for activations (very very approximate)\n\nFor example: Let‚Äôs take an 80B param model and 80GB GPUs and calculate how many of them we will need for:\n\nTraining: at least 23 GPUs 80*18*1.25/80\nInference: at least 3 GPUs 80*2*1.25/80\n\nMore details.\n\n\n\n\nAs you navigate this very complex AI industry here are some thing to be aware of:\n\n\n\nIf you contract doesn‚Äôt have clear deliverables (time and performance) don‚Äôt be surprised if you paid for something you won‚Äôt receive in time you need it or not at all\nBe very careful before you sign a contract that includes clauses that start with ‚Äúwe will make a reasonable effort to ‚Ä¶‚Äù.\nWhen was the last time you went to the bread section of the supermarket and found a lump of half-baked dough with a note ‚Äúwe made a reasonable effort to bake this bread, but alas, what you see is what you get‚Äù?\nBut for whatever reason it‚Äôs acceptable to create a legal contract where the provider provides neither delivery dates nor performance metrics and doesn‚Äôt provide stipulations for what will they do in recompense when those promises aren‚Äôt fulfilled.\n\n\n\n\n\nSome cloud providers will make you use very proprietary tools or hardware that will make it very difficult for you to leave down the road because you will have to retool everything if you leave\nConsider what would be the cost of moving to a different provider should this provider prove to be not satisfactory or if they don‚Äôt have a capacity to fulfill your growing needs.\nIf you rent a cluster with a generic Linux box with generic open source tools it should be trivial to move from one provider to another as almost everything would work out of the box\nObviously if you choose compute that requires custom software that works for that hardware only and you can‚Äôt rent this hardware anywhere else you‚Äôre setting yourself up for a lock-in\n\n\n\n\n\nThe cloud providers have mostly the same generic hardware, which leads to a very slim  margin and so in order to make big  they invent products and then try to convince you that you need to buy them. Sometimes you actually need those products, but very often not. See also the previous section on lock-in, since proprietary products usually mean a partial lock-in.\nOften it‚Äôs easy to observe the 3 step marketing technique for solutions that seek a problem to solve:\n\n\nConvince a couple of well respected customers to use the provider‚Äôs proprietary products by giving them huge discounts or even pay them to use them\nUse those in step 1 as the social approval lever to reel in more converts\nThen scoop the rest of the strugglers by telling them that 80% of your customers (1+2) use these amazing products\n\nWhen marketing these products it‚Äôs important: - to mention how well they work with a dozen of other products, since now you‚Äôre not buying into a single product but into a whole proprietary product-sphere. - to use really nice looking complicated diagrams of how things plug into each other, and move really fast to the next slide before someone asks a difficult question.\nHPCs are probably a good group of compute providers to learn from - they have no funds to create new products and so they creatively address all their needs using mostly generic open source tools with some custom written software added when absolutely needed.\n\n\n\n\nTo conclude I thought I‚Äôd share some insights to how one could slightly improve their daily AI battlefield experience.\n\n\nIf you read Twitter and other similar ML-related feeds you‚Äôre guaranteed to feel the fear of missing out, since there is probably at least one new great model getting released weekly and multiple papers are getting published daily and your peers will publish their cool achievements hours.\nWe are dealing with very complicated technology and there is a small handful of people who can absorb that much new material and understand / integrate it.\nThis can be extremely depressing and discouraging.\nI deal with it by looking at twitter about once or twice a week. I mostly use Twitter in broadcast mode - that is if I have something to share I post it and only watch for possible follow up questions.\nUsually all the important news reach me through other people.\n\n\n\nThe pace of innovation in the field of AI is insane. It‚Äôs not possible to know all-things-AI. I‚Äôd dare to say it‚Äôs not possible to know even 10% of it for most of us.\nI realized this very early one and I stopped paying attention to most announcements, tutorials, keynotes, etc. Whenever I have a new need I research it and I discover what I need and I have to be careful not to try to learn other things not pertinent to the goal at hand.\nSo I actually know very little, but what I have researched in depth I know quite well for some time and later I forget even that (that‚Äôs why I write these notes - so that I can easily find what I have already researched).\nSo if you ask me something, chances are that I don‚Äôt know it, but the saving grace for me is that if you give me time I can figure it out and give the answer or develop a solution.\n\n\n\nBecause the ML field is in a huge race, a lot of the open source software is half-baked, badly documented, badly tested, at times poorly supported. So if you think you can save time by re-using software written by others expect spending hours to weeks trying to figure out how to make it work. And then keeping it working when the updates break it.\nThe next problem is that most of this software depends on other software which often can be just as bad. It‚Äôs not uncommon where I start fixing some integration problem, just to discover a problem in a dependent package, which in its turn has another problem from another package. This can be extremely frustrating and discouraging. Once excepts to save time by reuse, but ends up spending a long time figuring out how to make it work. At least if I write my own software I have fun and it‚Äôs a creative process, trying to make other people‚Äôs software work is not.\nSo at the end of the day we are still better off re-using other people‚Äôs software, except it comes at an emotional price and exhaustion.\nSo first of all, try to find a way not to beat yourself up if the software you didn‚Äôt write doesn‚Äôt work. If you think about it, those problems aren‚Äôt of your creation.\nLearning how to debug efficiently should also make this process much less painful.\n\n\n\n\nMark Saroufim,",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#basics",
    "href": "qmd/insights/ai-battlefield.html#basics",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "Training:\n\nHow fast one can train a better model (first to market advantage)\nHow much $$ was spent (do we still have money left to pay salaries to talent after training?)\n\nInference:\n\nFast latency (users are used to msec response times and will leave if the response takes seconds)\nFast throughput (how many concurrent queries can be processed)\nHow much $$ is being spent per user (can we rent more GPUs to acquire more users and/or improve (1) and (2)?)\n\n\n\n\n\nFast compute massively dominated by matrix multiplications\nFast enough memory, IO, network and CPU to feed the compute\n\nCorollary: If when you buy or rent hardware you invest in the fastest accelerators, but cheap out on any of the other components you wasted $$ and you might not win the race as it‚Äôll take longer to train.\n\n\n\n\nAn accelerator or a processing unit is what does most of the work.\nSince ML does a lot of parallel processing (SIMD) GPUs were used at the beginning, but now you additionally have TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs, etc. Recent CPUs are becoming used as accelerators as well, especially for inference.\n\nMore details.\n\n\n\n\nAI companies - train models/build products around self-trained or trained-by-others‚Äô models, in-house research.\nAcademia - does massive research and write papers. Lots of new ideas are generated.\nAI enthusiasts - lots of good will available, some pull resources/talents together to train open access models, with donated compute by HPCs and an occasional cloud, or a university cluster.\nEntrepreneurs - lots of low hanging fruit to pick - creative reselling of services, making ML-driven apps, and using various ingenious combinations of available resources to create amazing outcomes.\n\n\n\n\n\nIt‚Äôs very surprising that almost everybody involved in the domain of AI shares a lot of the discoveries with the community.\nSurely, companies don‚Äôt disclose all of their IP, but a lot of it does get shared in the form of knowledge or model weights\nCompanies that publish a lot of IP and models tend to attract higher quality talent.\nTwitter seems to be the central platform where one must be to follow what‚Äôs going on\n\n\n\n\n\nThe Dot-com bubble occurred during 1995-2000. And a very similar situation is happening right now in the AI space.\nThere is a lot of money available to create new startups or boost the existing companies. It‚Äôs relatively easy to raise millions of dollars.\nAs we are in the wild-wild-west stage of the AI industry it‚Äôs very difficult to predict the future, and so pretty much anything goes as far as startup ideas go, as long as it sounds reasonable.\nWhat distinguishes the AI bubble from the Dot-com bubble, is that one didn‚Äôt actually need much money to operate a Dot-com company - most of the raised money went to marketing and some to staff, barely any to compute. AI companies need millions of dollars because training LLMs requires an insane amount of compute, and that compute is very expensive. e.g.¬†1x NVIDIA H100 costs ~$30k and a company may need 512 of those, which is $15M (not counting the other hardware components and related costs)!",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#ml-engineers-heaven-and-hell",
    "href": "qmd/insights/ai-battlefield.html#ml-engineers-heaven-and-hell",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "This is my personal LLM/VLM trainings-based heaven and hell. YMMV.\n\n\n\nA well built HPC, or a full service cloud based cluster, where someone diligently and timely takes care of the hardware and the systems.\nI just need to bring my training software and do the training, which is already an insanely complicated job requiring special skills.\nLots of nodes available for exclusive unlimited use\nFast inter-node connectivity that doesn‚Äôt bottleneck the accelerators and which isn‚Äôt shared with other users\nHuge local super-fast NVME based shared filesystem that can fit datasets and checkpoints\nBarebones Linux w/ SLURM and minimal software to be able to launch training jobs\nsudoer access to ease the work with a team of people\n\n\n\n\n\nA cloud or in-house cluster, where you have to do everything - sysadmining, replacing hardware, dealing with outages, etc. And to do the training on top of that.\nA smallish slow shared filesystem (NFS?), with cloud to draw data from and checkpoint to\nSlow inter-node leading to low accelerator utilization\nInter-node shared with other users which make the network erratic and unpredictable\nSuper-complicated cloud console with gazillion of screens and steps to set even simple things up\nNot being able to swap out failing hardware fast\nNeeding to timeshare the nodes - with wait times between training jobs\nHaving other concurrent users who might use up the whole disk, leading to trainings crashing\nNot being able to kill jobs others on the team started and went to sleep",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#getting-compute",
    "href": "qmd/insights/ai-battlefield.html#getting-compute",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "There are 3 main choices to where one gets compute:\n\nRent on the cloud\nGet a timeshare on an HPC\nBuy it\n\n\n\nThis is currently the prevalent way of getting compute.\nPros:\n\nEasy to expand or contract the size of the cluster\nEasy to upgrade from the old hardware generation to the new one in a few years\nCluster management could be easily outsourced\n\nCons:\n\nExpensive, unless you negotiate a long term (1-3 year) contract for hundreds of accelerators\nYou will be tempted to buy many tools and services that you may or may not need\nYou always get charged whether you use your cluster fully or not\n\n\n\n\nThere aren‚Äôt that many HPCs out there and so the amount of available resources is limited.\nPros: - Managed for you - all you need is your software to do the training and a bit of SLURM know-how to launch jobs - Often sponsored by the local government/university - probably could get the job done for less $$ or even free (e.g.¬†we trained BLOOM-176B for free on JeanZay HPC!)\nCons: - needing to time share compute with other teams == short job times with possible long wait times in between - could be difficult to finish training quickly - The inter-node network is likely to be unstable as it‚Äôll be used by other teams - Have to abide by the HPC‚Äôs rules (e.g.¬†no sudo access and various other rules to follow) - In a way the HPC cluster will be what it‚Äôll be - you can‚Äôt make the network faster and often even getting some software installed can be tricky.\n\n\n\nIt‚Äôs mainly universities that buy and build their own clusters, and some big companies do that too.\nPros:\n\nIf you can deploy the hardware 24/7 for more than a few years the total cost will be cheaper than renting\nEasy to provide fast local storage - a good NVME raid would be much cheaper and faster than online storage\n\nCons:\n\nYou‚Äôre stuck with the outdated hardware just a few years after it was purchased - might be able to resell\nMust buy more than needed - Hardware tends to break, especially when it‚Äôs used 24/7, RMA could take weeks\nHave to hire talent to manage the in-house solution\nHave to figure out cooling, electric costs, insurance, etc.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#the-needs-of-technology",
    "href": "qmd/insights/ai-battlefield.html#the-needs-of-technology",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "Imagine a steam locomotive - the engine is great, but if the fireman isn‚Äôt fast enough to shovel the coal in, the train won‚Äôt move fast.\n\nsource\nThis is the current state of ML hardware: The bottleneck is in moving bits and not the compute.\n\nAccelerators get ~2x faster every 2 years (Moore‚Äôs law)\nNetwork and memory are not! Already now both are compute bottlenecks\nIO can be another bottleneck if your DataLoader has to pull data from the cloud\nCPU is fine as long as it has enough cpu-cores for DataLoader workers, and main processes\n\nCorollary: research the whole machine and not just its engine.\na crazy idea: the older GPUs might do fine if you can actually feed them as fast as they can compute. And if you can get 3x of them at the same cost as the next generation GPU you might finish training sooner and a lower cost.\n\n\n\n\nOnce you choose the architecture and the size of the model and how many tokens you want to train the model for you immediately know how much compute will be required to accomplish this goal. Specifically you can now calculate how many floating point operations will be needed.\nAll that is missing is comparing different compute providers to how many floating point operations their hardware can computes per secs (TFLOPS) and their cost per unit and now you can tell the total approximate cost of the training.\n\n\nCalculate the time needed to train given the TFLOPS of the considered solution:\ntotal_tflops_required / tflops_of_this_compute_unit = time_in_seconds\nLet‚Äôs say it came to be 604800 secs or 7 days.\nLook at the cost of using this compute solution for 7 days and now you know the total $$ to train this model.\nLook at other proposals and calculate the same - chose the best option.\n\n\nAs mentioned earlier, time is of a huge importance, so you might still choose a more expensive solution if finishing the training sooner is important because you want to be first to market.\n\nUnfortunately, this math is only partially correct because the advertised peak TFLOPS are typically unachievable. The MFU section delves into it.\n\n\n\nAs mentioned in the previous section, some (most?) vendors publish unrealistic peak performance TFLOPS - they aren‚Äôt possible to achieve.\nModel Flops Utilization (MFU) is the metric that tells us how well the accelerator is utilized. Here is how it is calculated:\n\nMeasure the actual TFLOPS by calculating how many floating point operations a single training iteration takes and dividing that number by the number of seconds this iteration took.\nDivide the actual TFLOPS by advertised TFLOPS to get the MFU\n\nExample: Let‚Äôs say you‚Äôre training in BFLOAT16 precision:\n\nIf a single iteration requires 624 Tera floating point operations and it took 4 secs to run then we know that we get: 624/4=156 actual TFLOPS\nnow BF16@A100 is advertised as 312TFLOPS so 156/312=0.5 gives us 50% MFU.\n\nPractically: - with NVIDIA GPUs if you‚Äôre above 50% MFU on a multi-node setup with a large model you‚Äôre already doing fantastic - recent advancements in more efficient scalability solutions keep on increasing MFU - slow networks and inefficient frameworks or untuned configuration lower MFU\nTherefore once you know the MFU you can now adjust the cost estimate from the previous section. In the example there we said it‚Äôll take 7 days to train, but if MFU is 50%, it means it‚Äôll take 14 days to train.\n\n\n\nWhy can‚Äôt the advertised TFLOPS achieved? It‚Äôs because it takes time to move data between accelerator memory and compute and additionally it takes even more time to move data from disk and other gpus to the accelerator‚Äôs memory.\n\nThere is not much can be done about the accelerator memory since its bandwidth is what it is - one can only write more efficient software to make data move faster to/from the accelerator - hint: fused and custom written kernels (like torch.compile and flash attention)\nIf you only have a single GPU and the model fits its memory, you don‚Äôt need to worry about the network - accelerator memory is the only bottleneck. But if you have to shard the model across multiple GPUs network becomes the bottleneck.\nIntra-node Network - is very fast, but difficult to take advantage of for large models - Tensor parallelism and sequence parallelism address part of this problem. (more).\nInter-node Network - typically is too slow on most server setups - thus this is the key component to research! Efficient frameworks succeed to partially hide the comms overhead by overlapping compute and comms. But if comms take longer than compute, the comms are still the bottleneck. more.\nStorage IO is important primarily for feeding the DataLoader workers and saving the checkpoints. more.\n\nTypically with enough DL workers the DataLoader adds very little overhead.\nWhile checkpoints are being saved the accelerators idle unless some async saving solution is used, so fast IO is crucial here",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#key-hardware-components",
    "href": "qmd/insights/ai-battlefield.html#key-hardware-components",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "As of this writing here are the most common accelerators that can be used for training, finetuning and inference ML models:\nWidely available:\n\nNVIDIA A100 - huge availability across all clouds, but is already gradually being replaced by H100\n\nAvailable, but locks you in:\n\nGoogle TPUs - fast! but the cost is a lock-in into a single vendor and cloud\n\nEmerging to general availability:\n\nNVIDIA H100 - 2-3x faster than A100 (half precision), 6x faster for fp8\nAMD MI250 ~= A100 - very few clouds have them\nAMD MI300 ~= H100 - don‚Äôt expect until 1.5-2 years from now to be GA\nIntel Gaudi2 ~= H100 - starting to slowly emerge on Intel‚Äôs cloud\nGraphCore IPU - very difficult to find, paperspace has them\nCerebras WaferScale Engine - available on Cerebras‚Äô cloud\n\n\n\nIn general most (all?) accelerators are supported by major frameworks like PyTorch or TensorFlow and the same code should run everywhere with small modifications as long as it doesn‚Äôt use any accelerator-specific functionality.\nFor example, if your PyTorch application includes custom CUDA kernels it‚Äôll only work on NVIDIA GPUs and may be on AMD MI-series.\n\nNVIDIA GPUs: all based on CUDA, which most training frameworks support. You can easily moved between different NVIDIA GPUs and most things would work the same.\nAMD MI250/MI300: with PyTorch using ROCm you can run most CUDA-based software as is. This is really the only inter-operable accelerator with the NVIDIA stack.\nGaudi2: if you use HF Transformers/Diffusers you can use optimum-habana. If you use HF Trainer with NVIDIA GPUs it should be relatively easy to switch to train/infer on Gaudi2.\nGraphCore IPU: can also be run via PyTorch via poptorch\nCerebras: is also working on PyTorch support via Cerebras Software Platform (CSoft) via XLA.\n\nAlso in general most ML code could be compiled into cross-platform formats like Open Neural Network Exchange (ONNX) which can be run on a variety of accelerators. This approach is typically used more often for inference workloads.\n\n\n\n\n\nIf you want to train a large model that doesn‚Äôt fit onto a single accelerator‚Äôs memory you have to rely on the intra- and inter-node networks to synchronize multiple accelerators.\nThe biggest issue right now is that compute hardware advancements move faster than networking hardware, e.g.¬†for NVIDIA NVLink intra-node:\n\n\n\n\n\n\n\n\n\n\n\nGPU\nComputefp16TFLOPS\nComputespeedup\nIntra-nodeGBps\nIntra-nodespeedup\n\n\n\n\nV100\n125\n1\n300\n1\n\n\nA100\n312\n2.5\n600\n2\n\n\nH100\n989\n8\n900\n3\n\n\n\n\nYou can see that A100 was 2.5 faster than V100, and H100 is ~3x faster than A100. But the intra-node speed of NVLink has only increased by 300GBps each generation.\nMoreover, all 3 generations of NVLink use identical NICs of the same 50GBps duplex throughput. They have just doubled and tripled the number of links to speed things up. So there was 0 progress in that technology.\nThe inter-node situation isn‚Äôt any better with most NICs there doing 100 or 200Gbps, and some 400Gbps are starting to emerge. (correspondingly in GBps: 12.5, 25 and 50). It‚Äôs the same story here, some solutions provide dozens of NICs to get to higher speeds.\nAlso typically with LLMs the payload is so large that network latency is often negligible for training. It‚Äôs still quite important for inference.\n\n\n\n\nPay attention to bytes vs bits. 1Byte = 8bits. 1GBps = 8Gbps.\nIf you need to reduce bits (e.g.¬†gradients) across multiple nodes, it‚Äôs the slowest link (Inter-node) that defines the overall throughput, so intra-node speed doesn‚Äôt matter then\nTensor parallelism and sequence parallelism have to remain within the node to be efficient - only makes sense with fast intra-node speed\n\nNVIDIA:\n\nNVIDIA-based compute nodes come with 50GBps duplex NVLInk\nSome have a lot of NVLinks, others less but typically plenty w/ at least 900GBps (5.6Tbps) duplex for H100, 600GBps for A100 nodes\n\nIntel Gaudi2:\n\n8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps\n\nMore details\n\n\n\n\nAn order of magnitude slower than Intra-node\nYou will see a wide range of speeds from 50Gbps to 3200 Gbps\nYou need to reduce gradients and other bits faster than compute to avoid idling accelerators\nYou typically get at most 80% of advertised speed. e.g., if you are told you get 800Gbps, expect ~480Gbps.\nIf moving to fp8 H100 is 18x faster than V100\nWe are yet to see if 3200Gbps for H100s will be enough to keep high MFU.\nPractically less than 3x but it‚Äôs a good estimate\n\nMore details.\n\n\n\n\nThere are 3 distinct Storage IO needs in the ML workload:\n\nYou need to be able to feed the DataLoader fast - (super fast read, don‚Äôt care about fast write) - requires sustainable load for hours and days\nYou need to be able to write checkpoints fast - (super fast write, fastish read as you will be resuming a few times) - requires burst writing - you want super fast to not block the training for long (unless you use some sort of cpu offloading to quickly unblock the training)\nYou need to be able to load and maintain your codebase - (medium speed for both reading and writing) - this also needs to be shared since you want all nodes to see the same codebase - as it happens only during the start or resume it‚Äôll happen infrequently\n\n\nMost of the time you‚Äôre being sold 80% of what you paid. If you want a reliable 100TBs you need to rent 125TBs or your application may fail to write long before the disk is full.\nShared Distributed Filesystem:\n\nnon-parallel shared file systems can be extremely slow if you have a lot of small files (=Python!)\nYou want Parallel FS like GPFS (IBM Spectrum Scale) or Lustre (Open Source)\n\n\nMore details.\n\n\n\nYou need enough memory for:\n\n2-3 possibly DL workers per Accelerator (so 16-24 processes with 8 accelerators per node)\nEven more memory for DL workers if you pull data from the cloud\nEnough memory to load the model if you can‚Äôt load to accelerator directly\nOften used for accelerator memory offloading - extends accelerator‚Äôs memory by swapping out the currently unused layers - if that‚Äôs the target use, then the more cpu memory is available - the better!\n\n\n\n\nThis is probably the least worrisome component.\n\nMost clouds provide beefy CPUs with plenty of cpu cores\nYou need to have enough cores to run 2-3 DL workers +1 per gpu - so at least 30 cores\nEven more cores for DL workers if you have complex and/or slow DL transforms (CV)\nMost of the compute happens on GPUs",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#impress-others-with-your-ml-instant-math",
    "href": "qmd/insights/ai-battlefield.html#impress-others-with-your-ml-instant-math",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "Training in half mixed-precision: model_size_in_B * 18 * 1.25 / gpu_size_in_GB\nInference in half precision: model_size_in_B * 2 * 1.25 /  gpu_size_in_GB\n\nThat‚Äôs the minimum, more to have a bigger batch size and longer sequence length.\nHere is the breakdown:\n\nTraining: 8 bytes for AdamW states, 4 bytes for grads, 4+2 bytes for weights\nInference: 2 bytes for weights (1 byte if you use quantization)\n1.25 is 25% for activations (very very approximate)\n\nFor example: Let‚Äôs take an 80B param model and 80GB GPUs and calculate how many of them we will need for:\n\nTraining: at least 23 GPUs 80*18*1.25/80\nInference: at least 3 GPUs 80*2*1.25/80\n\nMore details.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#traps-to-be-aware-of",
    "href": "qmd/insights/ai-battlefield.html#traps-to-be-aware-of",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "As you navigate this very complex AI industry here are some thing to be aware of:\n\n\n\nIf you contract doesn‚Äôt have clear deliverables (time and performance) don‚Äôt be surprised if you paid for something you won‚Äôt receive in time you need it or not at all\nBe very careful before you sign a contract that includes clauses that start with ‚Äúwe will make a reasonable effort to ‚Ä¶‚Äù.\nWhen was the last time you went to the bread section of the supermarket and found a lump of half-baked dough with a note ‚Äúwe made a reasonable effort to bake this bread, but alas, what you see is what you get‚Äù?\nBut for whatever reason it‚Äôs acceptable to create a legal contract where the provider provides neither delivery dates nor performance metrics and doesn‚Äôt provide stipulations for what will they do in recompense when those promises aren‚Äôt fulfilled.\n\n\n\n\n\nSome cloud providers will make you use very proprietary tools or hardware that will make it very difficult for you to leave down the road because you will have to retool everything if you leave\nConsider what would be the cost of moving to a different provider should this provider prove to be not satisfactory or if they don‚Äôt have a capacity to fulfill your growing needs.\nIf you rent a cluster with a generic Linux box with generic open source tools it should be trivial to move from one provider to another as almost everything would work out of the box\nObviously if you choose compute that requires custom software that works for that hardware only and you can‚Äôt rent this hardware anywhere else you‚Äôre setting yourself up for a lock-in\n\n\n\n\n\nThe cloud providers have mostly the same generic hardware, which leads to a very slim  margin and so in order to make big  they invent products and then try to convince you that you need to buy them. Sometimes you actually need those products, but very often not. See also the previous section on lock-in, since proprietary products usually mean a partial lock-in.\nOften it‚Äôs easy to observe the 3 step marketing technique for solutions that seek a problem to solve:\n\n\nConvince a couple of well respected customers to use the provider‚Äôs proprietary products by giving them huge discounts or even pay them to use them\nUse those in step 1 as the social approval lever to reel in more converts\nThen scoop the rest of the strugglers by telling them that 80% of your customers (1+2) use these amazing products\n\nWhen marketing these products it‚Äôs important: - to mention how well they work with a dozen of other products, since now you‚Äôre not buying into a single product but into a whole proprietary product-sphere. - to use really nice looking complicated diagrams of how things plug into each other, and move really fast to the next slide before someone asks a difficult question.\nHPCs are probably a good group of compute providers to learn from - they have no funds to create new products and so they creatively address all their needs using mostly generic open source tools with some custom written software added when absolutely needed.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#unsolicited-advice",
    "href": "qmd/insights/ai-battlefield.html#unsolicited-advice",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "To conclude I thought I‚Äôd share some insights to how one could slightly improve their daily AI battlefield experience.\n\n\nIf you read Twitter and other similar ML-related feeds you‚Äôre guaranteed to feel the fear of missing out, since there is probably at least one new great model getting released weekly and multiple papers are getting published daily and your peers will publish their cool achievements hours.\nWe are dealing with very complicated technology and there is a small handful of people who can absorb that much new material and understand / integrate it.\nThis can be extremely depressing and discouraging.\nI deal with it by looking at twitter about once or twice a week. I mostly use Twitter in broadcast mode - that is if I have something to share I post it and only watch for possible follow up questions.\nUsually all the important news reach me through other people.\n\n\n\nThe pace of innovation in the field of AI is insane. It‚Äôs not possible to know all-things-AI. I‚Äôd dare to say it‚Äôs not possible to know even 10% of it for most of us.\nI realized this very early one and I stopped paying attention to most announcements, tutorials, keynotes, etc. Whenever I have a new need I research it and I discover what I need and I have to be careful not to try to learn other things not pertinent to the goal at hand.\nSo I actually know very little, but what I have researched in depth I know quite well for some time and later I forget even that (that‚Äôs why I write these notes - so that I can easily find what I have already researched).\nSo if you ask me something, chances are that I don‚Äôt know it, but the saving grace for me is that if you give me time I can figure it out and give the answer or develop a solution.\n\n\n\nBecause the ML field is in a huge race, a lot of the open source software is half-baked, badly documented, badly tested, at times poorly supported. So if you think you can save time by re-using software written by others expect spending hours to weeks trying to figure out how to make it work. And then keeping it working when the updates break it.\nThe next problem is that most of this software depends on other software which often can be just as bad. It‚Äôs not uncommon where I start fixing some integration problem, just to discover a problem in a dependent package, which in its turn has another problem from another package. This can be extremely frustrating and discouraging. Once excepts to save time by reuse, but ends up spending a long time figuring out how to make it work. At least if I write my own software I have fun and it‚Äôs a creative process, trying to make other people‚Äôs software work is not.\nSo at the end of the day we are still better off re-using other people‚Äôs software, except it comes at an emotional price and exhaustion.\nSo first of all, try to find a way not to beat yourself up if the software you didn‚Äôt write doesn‚Äôt work. If you think about it, those problems aren‚Äôt of your creation.\nLearning how to debug efficiently should also make this process much less painful.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/insights/ai-battlefield.html#contributors",
    "href": "qmd/insights/ai-battlefield.html#contributors",
    "title": "ü™ñ The AI Battlefield",
    "section": "",
    "text": "Mark Saroufim,",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights",
      "ü™ñ The AI Battlefield"
    ]
  },
  {
    "objectID": "qmd/storage/benchmarks/results/hope-2023-12-20-14-37-02-331702-summary.html",
    "href": "qmd/storage/benchmarks/results/hope-2023-12-20-14-37-02-331702-summary.html",
    "title": "",
    "section": "",
    "text": "üì¶ StorageBenchmarksResultsfio benchmark results for hope on 2023-12-20-14:37:02\n\n\n\n\n\nfio benchmark results for hope on 2023-12-20-14:37:02\npartition /mnt/nvme0/fio/fio-test\n\nfilesize=16k read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n4.0\n1006.3\n257614\n16\n\n\n\n\nfilesize=16k write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n3.2\n1239.1\n317200\n16\n\n\n\n\nfilesize=1m read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.7\n2400.1\n614419\n16\n\n\n\n\nfilesize=1m write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1940.5\n496765\n16\n\n\n\n\nfilesize=1g read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.4\n2762.0\n707062\n16\n\n\n\n\nfilesize=1g write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1943.9\n497638\n16\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage",
      "Benchmarks",
      "Results",
      "fio benchmark results for hope on 2023-12-20-14:37:02"
    ]
  },
  {
    "objectID": "qmd/training/emulate-multi-node.html",
    "href": "qmd/training/emulate-multi-node.html",
    "title": "",
    "section": "",
    "text": "üèãÔ∏è TrainingEmulate a multi-node setup using just a single node",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Emulate a multi-node setup using just a single node"
    ]
  },
  {
    "objectID": "qmd/training/emulate-multi-node.html#larger-set-ups",
    "href": "qmd/training/emulate-multi-node.html#larger-set-ups",
    "title": "",
    "section": "Larger set ups",
    "text": "Larger set ups\nNow, let‚Äôs say you have 4 GPUs and you want to emulate 2x2 nodes. Then simply change the hostfile to be:\n$ cat hostfile\nworker-0 slots=2\nworker-1 slots=2\nand the CUDA_VISIBLE_DEVICES hack to:\nif os.environ[\"RANK\"] in [\"2\", \"3\"]:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\nEverything else should be the same.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Emulate a multi-node setup using just a single node"
    ]
  },
  {
    "objectID": "qmd/training/emulate-multi-node.html#automating-the-process",
    "href": "qmd/training/emulate-multi-node.html#automating-the-process",
    "title": "",
    "section": "Automating the process",
    "text": "Automating the process\nIf you want an automatic approach to handle any shape of topology, you could use something like this:\ndef set_cuda_visible_devices():\n    \"\"\"\n    automatically assign the correct groups of gpus for each emulated node by tweaking the\n    CUDA_VISIBLE_DEVICES env var\n    \"\"\"\n\n    global_rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    emulated_node_size = int(os.environ[\"LOCAL_SIZE\"])\n    emulated_node_rank = int(global_rank // emulated_node_size)\n    gpus = list(map(str, range(world_size)))\n    emulated_node_gpus = \",\".join(gpus[emulated_node_rank*emulated_node_size:(emulated_node_rank+1)*emulated_node_size])\n    print(f\"Setting CUDA_VISIBLE_DEVICES={emulated_node_gpus}\")\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = emulated_node_gpus\n\nset_cuda_visible_devices()",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Emulate a multi-node setup using just a single node"
    ]
  },
  {
    "objectID": "qmd/training/emulate-multi-node.html#emulating-multiple-gpus-with-a-single-gpu",
    "href": "qmd/training/emulate-multi-node.html#emulating-multiple-gpus-with-a-single-gpu",
    "title": "",
    "section": "Emulating multiple GPUs with a single GPU",
    "text": "Emulating multiple GPUs with a single GPU\nThe following is an orthogonal need to the one discussed in this document, but it‚Äôs related so I thought it‚Äôd be useful to share some insights here:\nWith NVIDIA A100 you can use MIG to emulate up to 7 instances of GPUs on just one real GPU, but alas you can‚Äôt use those instances for anything but standalone use - e.g.¬†you can‚Äôt do DDP or any NCCL comms over those GPUs. I hoped I could use my A100 to emulate 7 instances and add one more real GPU and to have 8x GPUs to do development with - but nope it doesn‚Äôt work. Asking NVIDIA engineers about it, there are no plans to have this use-case supported.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Emulate a multi-node setup using just a single node"
    ]
  },
  {
    "objectID": "qmd/training/emulate-multi-node.html#acknowledgements",
    "href": "qmd/training/emulate-multi-node.html#acknowledgements",
    "title": "",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Jeff Rasley for helping me to set this up.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Emulate a multi-node setup using just a single node"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/training-loss-patterns.html",
    "href": "qmd/training/instabilities/training-loss-patterns.html",
    "title": "Sam Foreman",
    "section": "",
    "text": "Training loss plot is similar to the heart beat pattern - there is the good, the bad and you-should-worry one. After studying many training loss trajectories one develops an intuition to explain various loss behaviors during one‚Äôs training and how to act on those.\nI warn you that the ‚ÄúUnderstanding‚Äù in the title of this section is overloaded since very often we don‚Äôt really understand why certain types of spikes happen. Here ‚Äúunderstanding‚Äù refers to recognizing various patterns. We then usually have techniques to overcome the bad patterns and bring the training successfully to the finish line.\nThus you will find here a gallery of training loss patterns sometimes with real explanations, but more often than not educated guesses to what might be happening.\nPlease excuse the plot snapshots looking wildly different from each other as they have come from many sources over multiple years.\n\n\nLet‚Äôs look at some good, bad and unusual patterns.\n\n\nPrior to starting BLOOM-176B training we did multiple experiments with the 104B model. We failed to figure out how to not diverge very early on.\n\nAs you can see many attempts were made, many techniques were applied (see chronicles. We think the 2 main obstacles were using fp16 and data that had a lot of garbage in it. For BLOOM-176B we switched to bf16, used much cleaner data and also added an embedding layer-norm and that made all the difference.\n\n\n\n\nThe BLOOM-176B training had a close to perfect training loss trajectory, with a single spike that has recovered in 200 steps.\nYou can inspect the TB to zoom in and check other plots.\nThis was the almost perfect training indeed. Lots of hard work was put into achieving this.\n\n\n\nRecently I was doing some performance testing and run a tiny global batch size of 8 on 8x A100 nodes on llama-2-7b trained from scratch. (w/ Deepspeed ZeRO-3 DP using HF Transformers Llama implementation)\n\nHere one can observe a rapid loss improvement from 4 to 2.5 in just 480 samples after a very steady much slower improvements. My colleague Gautam Mittal called it the grokking moment. In just a handful of steps the model suddenly generalized to much better predict the masked tokens.\nNormally one doesn‚Äôt see such a dramatic improvement when using a much larger batch size.\nIf we zoom in it took about 60 8-sample per iteration steps:\n\n\n\n\n\nIn general there are 3 types of loss spikes:\n\nFast recovering spikes\nSlow recovering spikes\nNot fully recovering spikes\n\nThe spikes usually happen because of a bad data pocket, either due to badly shuffled data or because it hasn‚Äôt been cleaned from some garbage scraped from the websites.\nWhile one would suspect that the batch before the spike was the trigger, but if you were to study that batch‚Äôs contents you are likely to find nothing unusual - quite often the problem starts developing many steps before and then most of the sudden it happens. But also it might not be easy to study the batch, since it could amount to a size of a book when the global batch size and the sequence lengths are huge.\n\n\nLoss spikes can happen often and as long as they quickly bounce back to where they left off the training usually continues as if nothing happened:\nHere is an example of the 13B pre-BLOOM training experiment:\n\nAs you can see there are many spikes, some of a huge magnitude but they have all quickly recovered.\n\n\n\nHere is a slow recovering spike from the IDEFICS-80B training:\n\n\n\n\nThis 104B model attempt spiked, started recovering but decided to not recover fully and instead started diverging\n\nHere is another example from the IDEFICS-80B training:\n\n\n\n\nHere are a few examples of diverging that didn‚Äôt go through a spike\n\nand here are a few more:\n\nas you can see each restart makes a bit of progress and then the model diverges.\nAll these are from the 104B model attempts.\n\n\n\nDuring the IDEFICS-80B training we were using 2 different dataset types mixed together:\n\nLegend: cm4 (high), average (mid) and pmd (low)\nYou can see that the loss spikes were sometimes happening simultaneously on both datasets and at other times only one of the datasets loss would spike.\nHere the model was learning two different data distributions and as you can see it was not reporting the same loss and the spike behaviors on both data distributions. The pmd datasets loss was much easier for the model than the cm4 one.\n\n\n\n\nTraining resume due to a hardware crash or because a need to rollback to an earlier checkpoint due to encountering a divergence is pretty much guaranteed to happen. If your training software doesn‚Äôt resume perfectly so that the model doesn‚Äôt notice there was a resume various problems could be encountered.\nThe most complicated challenge of resume is restoring various RNGs, getting to the DataLoader index where the previous training was restored, and dealing with various other requirements if you use complex DataLoaders that are specific to your setup.\n\n\nDuring IDEFICS-80B training we had a very complicated DataLoader which was suffering from image to text ratio fluctuations when the DataLoader was getting restored on resume, so we ended up having a small spike on each resume which would then recover:\n\nYou can see the loss and ratio plots correlation here. As we had to resume about a dozen times we saw a lot of those spikes.\n\n\n\nI was training a variation of Llama2 and saw this super unusual spike that didn‚Äôt diverge or recover but which switched to a new higher loss level:\n\nI rolled back to just before the weird behavior occurred and restarted. The loss training progressed at the same loss level for a bit and then again spiked and shifted to a higher loss.\n\nI have never seen this type of divergence before. I was scratching my head for a while and then decided to look at the bigger picture.\nAs of this writing Wandb doesn‚Äôt handle resume data plotting correctly if a rollback was performed, that is it ignores all new data after the rollback until the steps of the old data have been overcome. This forces us to start a new wandb plot for every resume with a rollback so that new data is shown. And if you needs to see the whole plot you have to stitch them and which includes dead data points that are no longer true. So I did the stitching and saw this puzzle:\n\nThere was no real spike in the two earlier runs. The loss never went up in the first place. In both resumes it was under-reporting loss due to an exactly repeated data and then it reached data it hasn‚Äôt seen before and started reporting correctly. In other words it was overfitting and reporting a false loss.\nThe cause of the problem is data repetition, and since it clearly memorised some of it it was reporting a better loss.\nThe problem comes from pytorch-lightning not handling resumes correctly wrt DataSampler automatically - basically every time you resume you start your data stream from scratch. This, of course, requires a user to somehow fix the situation. You could change the seed to somewhat ameliorate the situation and avoid the exact data sequence, but it still leaves you with repeat data, which isn‚Äôt what you want for any serious training (or ablation experiments, since your observation will be invalid, if they assume IID data distribution.\nfootnote: I discussed this issue with the PTL developers and they said that they tried hard to come up with a generic solution but it wasn‚Äôt meant to be. So the user needs to figure it out.\nMake sure to check your training framework documentation whether it handles the DataSampler resuming correctly. Make sure you didn‚Äôt discover this problem after the training has finished and you ended up training 6x times the same 50B of tokens from the planned 300B tokens seen only once each.\nDoing a couple of resumes early on before embarking on the real training should also expose if there is a problem. Albeit, if the data gets reshuffled on each resume you are unlikely to see it. It‚Äôll only be seen if the seed is the same.",
    "crumbs": [
      "[{{< iconify line-md pencil >}}]{.red-text style='font-size: 1.0em;'} Posts",
      "Qmd",
      "Training",
      "Instabilities",
      "Sam Foreman"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/training-loss-patterns.html#the-good-the-bad-and-the-unexpected",
    "href": "qmd/training/instabilities/training-loss-patterns.html#the-good-the-bad-and-the-unexpected",
    "title": "Sam Foreman",
    "section": "",
    "text": "Let‚Äôs look at some good, bad and unusual patterns.\n\n\nPrior to starting BLOOM-176B training we did multiple experiments with the 104B model. We failed to figure out how to not diverge very early on.\n\nAs you can see many attempts were made, many techniques were applied (see chronicles. We think the 2 main obstacles were using fp16 and data that had a lot of garbage in it. For BLOOM-176B we switched to bf16, used much cleaner data and also added an embedding layer-norm and that made all the difference.\n\n\n\n\nThe BLOOM-176B training had a close to perfect training loss trajectory, with a single spike that has recovered in 200 steps.\nYou can inspect the TB to zoom in and check other plots.\nThis was the almost perfect training indeed. Lots of hard work was put into achieving this.\n\n\n\nRecently I was doing some performance testing and run a tiny global batch size of 8 on 8x A100 nodes on llama-2-7b trained from scratch. (w/ Deepspeed ZeRO-3 DP using HF Transformers Llama implementation)\n\nHere one can observe a rapid loss improvement from 4 to 2.5 in just 480 samples after a very steady much slower improvements. My colleague Gautam Mittal called it the grokking moment. In just a handful of steps the model suddenly generalized to much better predict the masked tokens.\nNormally one doesn‚Äôt see such a dramatic improvement when using a much larger batch size.\nIf we zoom in it took about 60 8-sample per iteration steps:",
    "crumbs": [
      "[{{< iconify line-md pencil >}}]{.red-text style='font-size: 1.0em;'} Posts",
      "Qmd",
      "Training",
      "Instabilities",
      "Sam Foreman"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/training-loss-patterns.html#main-types-of-loss-spikes",
    "href": "qmd/training/instabilities/training-loss-patterns.html#main-types-of-loss-spikes",
    "title": "Sam Foreman",
    "section": "",
    "text": "In general there are 3 types of loss spikes:\n\nFast recovering spikes\nSlow recovering spikes\nNot fully recovering spikes\n\nThe spikes usually happen because of a bad data pocket, either due to badly shuffled data or because it hasn‚Äôt been cleaned from some garbage scraped from the websites.\nWhile one would suspect that the batch before the spike was the trigger, but if you were to study that batch‚Äôs contents you are likely to find nothing unusual - quite often the problem starts developing many steps before and then most of the sudden it happens. But also it might not be easy to study the batch, since it could amount to a size of a book when the global batch size and the sequence lengths are huge.\n\n\nLoss spikes can happen often and as long as they quickly bounce back to where they left off the training usually continues as if nothing happened:\nHere is an example of the 13B pre-BLOOM training experiment:\n\nAs you can see there are many spikes, some of a huge magnitude but they have all quickly recovered.\n\n\n\nHere is a slow recovering spike from the IDEFICS-80B training:\n\n\n\n\nThis 104B model attempt spiked, started recovering but decided to not recover fully and instead started diverging\n\nHere is another example from the IDEFICS-80B training:\n\n\n\n\nHere are a few examples of diverging that didn‚Äôt go through a spike\n\nand here are a few more:\n\nas you can see each restart makes a bit of progress and then the model diverges.\nAll these are from the 104B model attempts.\n\n\n\nDuring the IDEFICS-80B training we were using 2 different dataset types mixed together:\n\nLegend: cm4 (high), average (mid) and pmd (low)\nYou can see that the loss spikes were sometimes happening simultaneously on both datasets and at other times only one of the datasets loss would spike.\nHere the model was learning two different data distributions and as you can see it was not reporting the same loss and the spike behaviors on both data distributions. The pmd datasets loss was much easier for the model than the cm4 one.",
    "crumbs": [
      "[{{< iconify line-md pencil >}}]{.red-text style='font-size: 1.0em;'} Posts",
      "Qmd",
      "Training",
      "Instabilities",
      "Sam Foreman"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/training-loss-patterns.html#resume-related-spikes",
    "href": "qmd/training/instabilities/training-loss-patterns.html#resume-related-spikes",
    "title": "Sam Foreman",
    "section": "",
    "text": "Training resume due to a hardware crash or because a need to rollback to an earlier checkpoint due to encountering a divergence is pretty much guaranteed to happen. If your training software doesn‚Äôt resume perfectly so that the model doesn‚Äôt notice there was a resume various problems could be encountered.\nThe most complicated challenge of resume is restoring various RNGs, getting to the DataLoader index where the previous training was restored, and dealing with various other requirements if you use complex DataLoaders that are specific to your setup.\n\n\nDuring IDEFICS-80B training we had a very complicated DataLoader which was suffering from image to text ratio fluctuations when the DataLoader was getting restored on resume, so we ended up having a small spike on each resume which would then recover:\n\nYou can see the loss and ratio plots correlation here. As we had to resume about a dozen times we saw a lot of those spikes.\n\n\n\nI was training a variation of Llama2 and saw this super unusual spike that didn‚Äôt diverge or recover but which switched to a new higher loss level:\n\nI rolled back to just before the weird behavior occurred and restarted. The loss training progressed at the same loss level for a bit and then again spiked and shifted to a higher loss.\n\nI have never seen this type of divergence before. I was scratching my head for a while and then decided to look at the bigger picture.\nAs of this writing Wandb doesn‚Äôt handle resume data plotting correctly if a rollback was performed, that is it ignores all new data after the rollback until the steps of the old data have been overcome. This forces us to start a new wandb plot for every resume with a rollback so that new data is shown. And if you needs to see the whole plot you have to stitch them and which includes dead data points that are no longer true. So I did the stitching and saw this puzzle:\n\nThere was no real spike in the two earlier runs. The loss never went up in the first place. In both resumes it was under-reporting loss due to an exactly repeated data and then it reached data it hasn‚Äôt seen before and started reporting correctly. In other words it was overfitting and reporting a false loss.\nThe cause of the problem is data repetition, and since it clearly memorised some of it it was reporting a better loss.\nThe problem comes from pytorch-lightning not handling resumes correctly wrt DataSampler automatically - basically every time you resume you start your data stream from scratch. This, of course, requires a user to somehow fix the situation. You could change the seed to somewhat ameliorate the situation and avoid the exact data sequence, but it still leaves you with repeat data, which isn‚Äôt what you want for any serious training (or ablation experiments, since your observation will be invalid, if they assume IID data distribution.\nfootnote: I discussed this issue with the PTL developers and they said that they tried hard to come up with a generic solution but it wasn‚Äôt meant to be. So the user needs to figure it out.\nMake sure to check your training framework documentation whether it handles the DataSampler resuming correctly. Make sure you didn‚Äôt discover this problem after the training has finished and you ended up training 6x times the same 50B of tokens from the planned 300B tokens seen only once each.\nDoing a couple of resumes early on before embarking on the real training should also expose if there is a problem. Albeit, if the data gets reshuffled on each resume you are unlikely to see it. It‚Äôll only be seen if the seed is the same.",
    "crumbs": [
      "[{{< iconify line-md pencil >}}]{.red-text style='font-size: 1.0em;'} Posts",
      "Qmd",
      "Training",
      "Instabilities",
      "Sam Foreman"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Engineering Open Book",
    "section": "",
    "text": "This is an open collection of methodologies, tools and step by step instructions to help with successful training of large language models and multi-modal models.\nThis is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs.\nThis repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source BLOOM-176B model in 2022 and IDEFICS-80B multi-modal model in 2023. Currently, I‚Äôm working on developing/training open-source Retrieval Augmented Generation (RAG) models at Contextual.AI.\nI‚Äôve been compiling this information mostly for myself so that I could quickly find solutions I have already researched in the past and which have worked, but as usual I‚Äôm happy to share these with the wider ML community."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "ML Engineering",
    "section": " Projects",
    "text": "Projects\n\n\n\n ezpz\n\nDistributed training, ezpz.\n\n Megatron-DeepSpeed\n\nMegatron-LM + DeepSpeed, for the largest of large language models.\n\n wordplay [web]\n\nBuilt on  [nanoGPT] with support for2\n{ :hugging_face: datasets, DeepSpeed }\n\n ai-science-training-series [web]\n\nStudent training series on AI-driven Science on Supercomputers\n\n\n\n\n\n enrich\n\nPython‚Äôs logging, with Rich\n\n ambivalent [web]\n\nMinimal, beautiful (+ highly-customizable) styles for Matplotlib3.\n\n l2hmc-qcd [web]\n\nApplication of the L2HMC algorithm to simulations in lattice QCD.\n\n climate-analysis [web]\n\nClimate Analysis project using ClimRR data"
  },
  {
    "objectID": "index.html#recent-work",
    "href": "index.html#recent-work",
    "title": "ML Engineering",
    "section": " Recent Work",
    "text": "Recent Work\n\nDeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery [‚Ä¶], NeurIPS 2023 AI For Science Workshop, Oct 2023\n\n DeepSpeed4Science.ai Blog Post \n\nA Comprehensive Performance Study of Large Language Models on Novel AI Accelerators, M. Emani, S. Foreman, et al., IPDPS 2024, Oct 2023\nExploratory Analysis of Climate Data with ClimRR, S. Foreman, Intro to HPC Bootcamp @ NERSC, August 7, 2023\n\nGenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics, M. Zvyagin et. al., Oct 2022\n\n ACM Gordon Bell Special Prize for HPC-Based COVID-19 Research\n\n\nLattice QCD and Particle Physics, A.S. Kronfeld et al., July 15, 2022\n\nApplications of ML to Lattice QFT, arXiv:2202.05838, D. Boyda, S. Cal√≠, S. Foreman, et al., Feb 2022\n\nLeapFrogLayers: Trainable Framework for Effective Sampling, S. Foreman, X.Y. Jin, J.C. Osborn, Lattice, 2021\n\nHMC with Normalizing Flows, slides, S. Foreman et al., Lattice, 2021\n\nDeep Learning Hamiltonian Monte Carlo (+ poster), S. Foreman, X.Y. Jin, & J.C. Osborn, @ SimDL Workshop @ ICLR, 2021\n\nMachine Learning and Neural Networks for Field Theory, S. Foreman, X.Y. Jin, & J.C. Osborn, SnowMass, 2020\n\nExamples of renormalization group transformations for image sets, S. Foreman et al., Physical Review E., 2018\n\nRG inspired Machine Learning for lattice field theory S. Foreman et al., arXiv:1710.02079, 2017\n\nLarge Energy Density in Three-Plate Nanocapacitors due to Coulomb Blockade, S. Foreman et al., J. Appl. Phys, 2018"
  },
  {
    "objectID": "index.html#recent-talks",
    "href": "index.html#recent-talks",
    "title": "ML Engineering",
    "section": " Recent Talks",
    "text": "Recent Talks\n\nMLMC: Machine Learning Monte Carlo for Lattice Gauge Theory, at Lattice 2023, July 2023\nGenerative Modeling and Efficient Sampling, at PASC23, July 2023\nEfficient Sampling for Lattice Gauge Theory, at Deep Fridays @ U. Bologna, April 2023\nLarge Scale Training, at Introduction to AI-driven Science on Supercomputers: A Student Training Series, November 2022\nHyperparameter Management, at 2022 ALCF Simulation, Data, and Learning Workshop, October 2022\nStatistical Learning, at ATPESC 2022, August 2022 üìï accompanying notebook\nScientific Data Science: An Emerging Symbiosis, at Argonne National Laboratory, May 2022\nMachine Learning in HEP, at UNC Greensboro, March 2022\nAccelerated Sampling Methods for Lattice Gauge Theory, at BNL-HET& RBRC Joint Workshop ‚ÄúDWQ @ 25‚Äù, Dec 2021\nTraining Topological Samplers for Lattice Gauge Theory, ML4HEP, on and off the Lattice @ ECT* Trento, Sep 2021\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\nDeep Learning HMC for Improved Gauge Generation to the Machine Learning Techniques in Lattice QCD Workshop, 2021\nMachine Learning for Lattice QCD at the University of Iowa, 2020\nMachine learning inspired analysis of the Ising model transition to Lattice, 2018\nMachine Learning Analysis of Ising Worms at Brookhaven National Laboratory, 2017"
  },
  {
    "objectID": "index.html#active-projects",
    "href": "index.html#active-projects",
    "title": "ML Engineering",
    "section": " Active Projects",
    "text": "Active Projects\n\n\n\n\n\n\n   GitHub Stats (saforem2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd\n\n\n\n\n\n\n\n GitHub repo"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "ML Engineering",
    "section": " Experience",
    "text": "Experience\n\n\n\nAssistant Computational Scientist\nALCF\n2022\n‚Äì\n\n\n\nPostdoc\nALCF\n2019\n2022\n\n\n\nGraduate Researcher\nANL\n2018\n2019"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "ML Engineering",
    "section": " Education",
    "text": "Education\n\n\n\nPhD\nPhysics\nUniversity of Iowa\n2019\n\n\nB.Sc\nPhysics\nUIUC\n2015\n\n\nB.Sc\nMath\nUIUC\n2015"
  },
  {
    "objectID": "index.html#events",
    "href": "index.html#events",
    "title": "ML Engineering",
    "section": " Events",
    "text": "Events\n\nOrganizer for Machine Learning and Quantum Computing for Earth Sciences at 17th U. S. National Congress on Computational Mechanics, July 2023\nOrganizer for SC23 Workshop: High Performance Python for Science at Scale (HPPSS), November 2023"
  },
  {
    "objectID": "index.html#appendix",
    "href": "index.html#appendix",
    "title": "ML Engineering",
    "section": " Appendix",
    "text": "Appendix\n\n\n\n\n\n\n‚ù§Ô∏è‚Äçü©π Status\n\n\n\n\n\n\n\nLast Updated: 02/13/2024 @ 15:18:38"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "ML Engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMostly getting supercomputers to stop yelling at each other .‚Ü©Ô∏é\n‚ö° powered by ezpz‚Ü©Ô∏é\nForked from   saforem2/opinionated‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Machine Learning Engineering Open Book",
    "section": "Table of Contents",
    "text": "Table of Contents\nMy apologies if the layout is a bit unstable while I‚Äôm writing new chapters and gradually re-organizing the content to be more intuitive.\nPart 1. Insights\n\nThe AI Battlefield Engineering - what you need to know in order to succeed\n\nPart 2. Hardware\n\nCompute - accelerators, CPUs, CPU memory.\nStorage - local, distributed and shared file systems.\nNetwork - intra- and inter-node networking.\n\nPart 3. Orchestration\n\nSLURM - the main orchestration environment\n\nPart 4. Training\n\nTraining - model training related guides\n\nPart 5. Development\n\nDebugging and Troubleshooting - how to debug easy and difficult issues\nAnd more debugging\nTesting - numerous tips and tools to make test writing enjoyable\n\nPart 6. Miscellaneous\n\nResources - LLM/VLM chronicles"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Machine Learning Engineering Open Book",
    "section": "Updates",
    "text": "Updates\nI announce any significant updates on my twitter channel https://twitter.com/StasBekman"
  },
  {
    "objectID": "index.html#pdf-version",
    "href": "index.html#pdf-version",
    "title": "Machine Learning Engineering Open Book",
    "section": "PDF version",
    "text": "PDF version\nDownload the PDF version of the book.\nI will try to rebuild it once a week or so, but if you want the latest, the instructions for building are here.\nThanks to HuggingFace for giving me permission to host my book‚Äôs PDF at the HF hub."
  },
  {
    "objectID": "index.html#shortcuts",
    "href": "index.html#shortcuts",
    "title": "Machine Learning Engineering Open Book",
    "section": "Shortcuts",
    "text": "Shortcuts\nThings that you are likely to need to find quickly and often.\n\nüõ†Ô∏è Tools:\n\nall_reduce_bench.py - a much easier way to benchmark network throughput than nccl-tests.\ntorch-distributed-gpu-test.py - a tool to quickly test your inter-node connectivity\n\nüìú Guides:\n\ndebugging pytorch applications - quick copy-n-paste solutions to resolve hanging or breaking pytorch applications\nslurm for users - a slurm cheatsheet and tricks\nmake tiny models/datasets/tokenizers\nLLM/VLM chronicles collection"
  },
  {
    "objectID": "index.html#gratitude",
    "href": "index.html#gratitude",
    "title": "Machine Learning Engineering Open Book",
    "section": "Gratitude",
    "text": "Gratitude\nNone of this would have been possible without me being entrusted with doing the specific LLM/VLM trainings I have learned this know-how from. This is a privilege that only a few enjoy due to the prohibitively expensive cost of renting huge ML compute clusters. So hopefully the rest of the ML community will vicariously learn from these notes.\nSpecial thanks go to Thom Wolf who proposed that I lead the BLOOM-176B training back when I didn‚Äôt know anything about large scale training. This was the project that catapulted me into the intense learning process. And, of course, HuggingFace for giving me the opportunity to work full time on BLOOM-176B and later on IDEFICS-80B trainings."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Machine Learning Engineering Open Book",
    "section": "Contributing",
    "text": "Contributing\nIf you found a bug, typo or would like to propose an improvement please don‚Äôt hesitate to open an Issue or contribute a PR."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Machine Learning Engineering Open Book",
    "section": "License",
    "text": "License\nThe content of this site is distributed under Attribution-ShareAlike 4.0 International."
  },
  {
    "objectID": "index.html#my-repositories-map",
    "href": "index.html#my-repositories-map",
    "title": "Machine Learning Engineering Open Book",
    "section": "My repositories map",
    "text": "My repositories map\n‚úî Machine Learning: ML Engineering Open Book | ML ways | Porting\n‚úî Guides: The Art of Debugging\n‚úî Applications: ipyexperiments\n‚úî Tools and Cheatsheets: bash | conda | git | jupyter-notebook | make | python | tensorboard | unix\n\n\n\n\n\n\n‚ù§Ô∏è‚Äçü©π Status\n\n\n\n\n\n\n\nLast Updated: 02/13/2024 @ 20:39:15"
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html",
    "href": "qmd/orchestration/slurm/users.html",
    "title": "",
    "section": "",
    "text": "üéª OrchestrationWorking in SLURM EnvironmentSLURM for users",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#quick-start",
    "href": "qmd/orchestration/slurm/users.html#quick-start",
    "title": "",
    "section": "Quick start",
    "text": "Quick start\nSimply copy this example.slurm and adapt it to your needs.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#slurm-partitions",
    "href": "qmd/orchestration/slurm/users.html#slurm-partitions",
    "title": "",
    "section": "SLURM partitions",
    "text": "SLURM partitions\nIn this doc we will use an example setup with these 2 cluster names:\n\ndev\nprod\n\nTo find out the hostname of the nodes and their availability, use:\nsinfo -p dev\nsinfo -p prod\nSlurm configuration is at /opt/slurm/etc/slurm.conf.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#wait-time-for-resource-granting",
    "href": "qmd/orchestration/slurm/users.html#wait-time-for-resource-granting",
    "title": "",
    "section": "Wait time for resource granting",
    "text": "Wait time for resource granting\nsqueue -u `whoami` --start\nwill show when any pending jobs are scheduled to start.\nThey may start sooner if others cancel their reservations before the end of the reservation.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#request-allocation-via-dependency",
    "href": "qmd/orchestration/slurm/users.html#request-allocation-via-dependency",
    "title": "",
    "section": "Request allocation via dependency",
    "text": "Request allocation via dependency\nTo schedule a new job when one more of the currently scheduled job ends (regardless of whether it still running or not started yet), use the dependency mechanism, by telling sbatch to start the new job once the currently running job succeeds, using:\nsbatch --dependency=CURRENTLY_RUNNING_JOB_ID tr1-13B-round1.slurm\nUsing --dependency may lead to shorter wait times that using --begin, since if the time passed to --begin allows even for a few minutes of delay since the stopping of the last job, the scheduler may already start some other jobs even if their priority is lower than our job. That‚Äôs because the scheduler ignores any jobs with --begin until the specified time arrives.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#make-allocations-at-a-scheduled-time",
    "href": "qmd/orchestration/slurm/users.html#make-allocations-at-a-scheduled-time",
    "title": "",
    "section": "Make allocations at a scheduled time",
    "text": "Make allocations at a scheduled time\nTo postpone making the allocation for a given time, use:\nsalloc --begin HH:MM MM/DD/YY\nSame for sbatch.\nIt will simply put the job into the queue at the requested time, as if you were to execute this command at this time. If resources are available at that time, the allocation will be given right away. Otherwise it‚Äôll be queued up.\nSometimes the relative begin time is useful. And other formats can be used. Examples:\n--begin now+2hours\n--begin=16:00\n--begin=now+1hour\n--begin=now+60  # seconds by default\n--begin=2010-01-20T12:34:00\nthe time-units can be seconds (default), minutes, hours, days, or weeks:",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#preallocated-node-without-time-60min-limit",
    "href": "qmd/orchestration/slurm/users.html#preallocated-node-without-time-60min-limit",
    "title": "",
    "section": "Preallocated node without time 60min limit",
    "text": "Preallocated node without time 60min limit\nThis is very useful for running repetitive interactive experiments - so one doesn‚Äôt need to wait for an allocation to progress. so the strategy is to allocate the resources once for an extended period of time and then running interactive srun jobs using this allocation.\nset --time to the desired window (e.g.¬†6h):\nsalloc --partition=dev --nodes=1 --ntasks-per-node=1 --cpus-per-task=96 --gres=gpu:8 --time=6:00:00 bash\nsalloc: Pending job allocation 1732778\nsalloc: job 1732778 queued and waiting for resources\nsalloc: job 1732778 has been allocated resources\nsalloc: Granted job allocation 1732778\nnow use this reserved node to run a job multiple times, by passing the job id of salloc:\nsrun --jobid $SLURM_JOBID --pty bash\nif run from inside bash started via salloc. But it can be started from another shell, but then explicitly set --jobid.\nif this srun job timed out or manually exited, you can re-start it again in this same reserved node.\nsrun can, of course, call the real training command directly and not just bash.\nImportant: when allocating a single node, the allocated shell is not on the node (it never is). You have to find out the hostname of the node (reports when giving the allocation or via squeue and ssh to it.\nWhen finished, to release the resources, either exit the shell started in salloc or scancel JOBID.\nThis reserved node will be counted towards hours usage the whole time it‚Äôs allocated, so release as soon as done with it.\nActually, if this is just one node, then it‚Äôs even easier to not use salloc but to use srun in the first place, which will both allocate and give you the shell to use:\nsrun --pty --partition=dev --nodes=1 --ntasks=1 --cpus-per-task=96 --gres=gpu:8 --time=60 bash",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#hyper-threads",
    "href": "qmd/orchestration/slurm/users.html#hyper-threads",
    "title": "",
    "section": "Hyper-Threads",
    "text": "Hyper-Threads\nBy default, if the cpu has hyper-threads (HT), SLURM will use it. If you don‚Äôt want to use HT you have to specify --hint=nomultithread.\nfootnote: HT is Intel-specific naming, the general concept is simultaneous multithreading (SMT)\nFor example for a cluster with with 2 cpus per node with 24 cores and 2 hyper-threads each, there is a total of 96 hyper-threads or 48 cpu-cores available. Therefore to utilize the node fully you‚Äôd need to configure either:\n#SBATCH --cpus-per-task=96\nor if you don‚Äôt want HT:\n#SBATCH --cpus-per-task=48\n#SBATCH --hint=nomultithread\nThis last approach will allocate one thread per core and in this mode there are only 48 cpu cores to use.\nNote that depending on your application there can be quite a performance difference between these 2 modes. Therefore try both and see which one gives you a better outcome.\nOn some setups like AWS the all-reduce throughput degrades dramatically when --hint=nomultithread is used! Whereas on some other setups the opposite is true - the throughput is worse without HT!",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#reuse-allocation",
    "href": "qmd/orchestration/slurm/users.html#reuse-allocation",
    "title": "",
    "section": "Reuse allocation",
    "text": "Reuse allocation\ne.g.¬†when wanting to run various jobs on identical node allocation.\nIn one shell:\nsalloc --partition=prod --nodes=16 --ntasks=16 --cpus-per-task=96 --gres=gpu:8 --time=3:00:00 bash\necho $SLURM_JOBID\nIn another shell:\nexport SLURM_JOBID=&lt;JOB ID FROM ABOVE&gt;\nsrun --jobid=$SLURM_JOBID ...\nYou may need to set --gres=gpu:0 to run some diagnostics job on the nodes. For example, let‚Äôs check shared memory of all the hosts:\nsrun --jobid 631078 --gres=gpu:0 bash -c 'echo $(hostname) $(df -h | grep shm)'",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#specific-nodes-selection",
    "href": "qmd/orchestration/slurm/users.html#specific-nodes-selection",
    "title": "",
    "section": "Specific nodes selection",
    "text": "Specific nodes selection\nTo exclude specific nodes (useful when you know some nodes are broken, but are still in IDLE state):\nsbatch --exclude nodeA,nodeB\nor via: #SBATCH --exclude ...\nTo use specific nodes:\nsbatch --nodelist= nodeA,nodeB\ncan also use the short -w instead of --nodelist\nThe administrator could also define a feature=example in slurm.conf and then a user could ask for that subset of nodes via --constraint=example",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#signal-the-running-jobs-to-finish",
    "href": "qmd/orchestration/slurm/users.html#signal-the-running-jobs-to-finish",
    "title": "",
    "section": "Signal the running jobs to finish",
    "text": "Signal the running jobs to finish\nSince each SLURM run has a limited time span, it can be configured to send a signal of choice to the program a desired amount of time before the end of the allocated time.\n--signal=[[R][B]:]&lt;sig_num&gt;[@&lt;sig_time&gt;]\nTODO: need to experiment with this to help training finish gracefully and not start a new cycle after saving the last checkpoint.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#detailed-job-info",
    "href": "qmd/orchestration/slurm/users.html#detailed-job-info",
    "title": "",
    "section": "Detailed job info",
    "text": "Detailed job info\nWhile most useful information is preset in various SLURM_* env vars, sometimes the info is missing. In such cases use:\nscontrol show -d job $SLURM_JOB_ID\nand then parse out what‚Äôs needed.\nFor a job that finished its run use:\nsacct -j JOBID\ne.g.¬†with more details, depending on the partition:\nsacct -u `whoami` --partition=dev  -ojobid,start,end,state,exitcode --format nodelist%300  -j JOBID\nsacct -u `whoami` --partition=prod -ojobid,start,end,state,exitcode --format nodelist%300  -j JOBID",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#show-jobs",
    "href": "qmd/orchestration/slurm/users.html#show-jobs",
    "title": "",
    "section": "show jobs",
    "text": "show jobs\nShow only my jobs:\nsqueue -u `whoami`\nShow jobs by job id:\nsqueue -j JOBID\nShow jobs of a specific partition:\nsqueue --partition=dev",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#aliases",
    "href": "qmd/orchestration/slurm/users.html#aliases",
    "title": "",
    "section": "Aliases",
    "text": "Aliases\nHandy aliases\nalias myjobs='squeue -u `whoami` -o \"%.16i %9P %26j %.8T %.10M %.8l %.6D %.20S %R\"'\nalias groupjobs='squeue -u foo,bar,tar -o \"%.16i %u %9P %26j %.8T %.10M %.8l %.6D %.20S %R\"'\nalias myjobs-pending=\"squeue -u `whoami` --start\"\nalias idle-nodes=\"sinfo -p prod -o '%A'\"",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#zombies",
    "href": "qmd/orchestration/slurm/users.html#zombies",
    "title": "",
    "section": "Zombies",
    "text": "Zombies\nIf there are any zombies left behind across nodes, send one command to kill them all.\nsrun pkill python",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#detailed-access-to-slurm-accounting",
    "href": "qmd/orchestration/slurm/users.html#detailed-access-to-slurm-accounting",
    "title": "",
    "section": "Detailed Access to SLURM Accounting",
    "text": "Detailed Access to SLURM Accounting\nsacct displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.\nSo this is a great tool for analysing past events.\nFor example, to see which nodes were used to run recent gpu jobs:\nsacct -u `whoami` --partition=dev -ojobid,start,end,state,exitcode --format nodelist%300\n%300 here tells it to use a 300 char width for the output, so that it‚Äôs not truncated.\nSee man sacct for more fields and info fields.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#queue",
    "href": "qmd/orchestration/slurm/users.html#queue",
    "title": "",
    "section": "Queue",
    "text": "Queue\n\nCancel job\nTo cancel a job:\nscancel [jobid]\nTo cancel all of your jobs:\nscancel -u &lt;userid&gt;\nTo cancel all of your jobs on a specific partition:\nscancel -u &lt;userid&gt; -p &lt;partition&gt;\n\n\nTips\n\nif you see that salloc‚Äôed interactive job is scheduled to run much later than you need, try to cancel the job and ask for shorter period - often there might be a closer window for a shorter time allocation.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#logging",
    "href": "qmd/orchestration/slurm/users.html#logging",
    "title": "",
    "section": "Logging",
    "text": "Logging\nIf we need to separate logs to different log files per node add %N (for short hostname) so that we have:\n#SBATCH --output=%x-%j-%N.out\nThat way we can tell if a specific node misbehaves - e.g.¬†has a corrupt GPU. This is because currently pytorch doesn‚Äôt log which node / gpu rank triggered an exception.\nHoping it‚Äôll be a built-in feature of pytorch https://github.com/pytorch/pytorch/issues/63174 and then one won‚Äôt need to make things complicated on the logging side.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#show-the-state-of-nodes",
    "href": "qmd/orchestration/slurm/users.html#show-the-state-of-nodes",
    "title": "",
    "section": "Show the state of nodes",
    "text": "Show the state of nodes\nsinfo -p PARTITION\nVery useful command is:\nsinfo -s\nand look for the main stat, e.g.:\nNODES(A/I/O/T) \"allocated/idle/other/total\".\n597/0/15/612\nSo here 597 out of 612 nodes are allocated. 0 idle and 15 are not available for whatever other reasons.\nsinfo -p gpu_p1 -o \"%A\"\ngives:\nNODES(A/I)\n236/24\nso you can see if any nodes are available on the 4x v100-32g partition (gpu_p1)\nTo check a specific partition:\nsinfo -p gpu_p1 -o \"%A\"\nSee the table at the top of this document for which partition is which.\n\nsinfo states\n\nidle: no jobs running\nalloc: nodes are allocated to jobs that are currently executing\nmix: the nodes have some of the CPUs allocated, while others are idle\ndrain: the node is unavailable due to an administrative reason\ndrng: the node is running a job, but will after completion not be available due to an administrative reason\n\n\n\ndrained nodes\nTo see all drained nodes and the reason for drainage (edit %50E to make the reason field longer/shorter)\n% sinfo -R -o \"%50E %12U %19H %6t %N\"\nor just -R if you want it short:\n% sinfo -R",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#job-arrays",
    "href": "qmd/orchestration/slurm/users.html#job-arrays",
    "title": "",
    "section": "Job arrays",
    "text": "Job arrays\nTo run a sequence of jobs, so that the next slurm job is scheduled as soon as the currently running one is over in 20h we use a job array.\nLet‚Äôs start with just 10 such jobs:\nsbatch --array=1-10%1 array-test.slurm\n%1 limits the number of simultaneously running tasks from this job array to 1. Without it it will try to run all the jobs at once, which we may want sometimes (in which case remove %1), but when training we need one job at a time.\nAlternatively, as always this param can be part of the script:\n#SBATCH --array=1-10%1\nHere is toy slurm script, which can be used to see how it works:\n#!/bin/bash\n#SBATCH --job-name=array-test\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=1            # number of cores per tasks\n#SBATCH --time 00:02:00              # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --error=%x-%j.out            # error file name (same to watch just one file)\n#SBATCH --partition=dev\n\necho $SLURM_JOB_ID\necho \"I am job ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\"\ndate\nsleep 10\ndate\nNote $SLURM_ARRAY_JOB_ID is the same as $SLURM_JOB_ID, and $SLURM_ARRAY_TASK_ID is the index of the job.\nTo see the jobs running:\n$ squeue -u `whoami` -o \"%.10i %9P %26j %.8T %.10M %.6D %.20S %R\"\n     JOBID PARTITION                       NAME    STATE       TIME  NODES           START_TIME NODELIST(REASON)\n591970_[2-   dev             array-test  PENDING       0:00      1  2021-07-28T20:01:06 (JobArrayTaskLimit)\nnow job 2 is running.\nTo cancel the whole array, cancel the job id as normal (the number before _):\nscancel 591970\nTo cancel a specific job:\nscancel 591970_2\nIf it‚Äôs important to have the log-file contain the array id, add %A_%a:\n#SBATCH --output=%x-%j.%A_%a.log\nMore details https://slurm.schedmd.com/job_array.html",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#job-array-trains-and-their-suspend-and-release",
    "href": "qmd/orchestration/slurm/users.html#job-array-trains-and-their-suspend-and-release",
    "title": "",
    "section": "Job Array Trains and their Suspend and Release",
    "text": "Job Array Trains and their Suspend and Release\nIn this recipe we accomplish 2 things:\n\nAllow modification to the next job‚Äôs slurm script\nAllow suspending and resuming job arrays w/o losing the place in the queue when not being ready to continue running a job\n\nSLURM is a very unforgiving environment where a small mistake can cost days of waiting time. But there are strategies to mitigate some of this harshness.\nSLURM jobs have a concept of ‚Äúage‚Äù in the queue which besides project priority governs when a job gets scheduled to run. If your have just scheduled a new job it has no ‚Äúage‚Äù and will normally be put to run last compared to jobs that have entered the queue earlier. Unless of course this new job comes from a high priority project in which case it‚Äôll progress faster.\nSo here is how one can keep the ‚Äúage‚Äù and not lose it when needing to fix something in the running script or for example to switch over to another script.\nThe idea is this:\n\nsbatch a long job array, e.g., -array=1-50%1\ninside the slurm script don‚Äôt have any code other than source another-script.slurm - so now you can modify the target script or symlink to another script before the next job starts\nif you need to stop the job array train - don‚Äôt cancel it, but suspend it without losing your place in a queue\nwhen ready to continue - unsuspend the job array - only the time while it was suspended is not counted towards its age, but all the previous age is retained.\n\nThe only limitation of this recipe is that you can‚Äôt change the number of nodes, time and hardware and partition constraints once the job array was launched.\nHere is an example:\nCreate a job script:\n$ cat train-64n.slurm\n#!/bin/bash\n#SBATCH --job-name=tr8-104B\n#SBATCH --nodes=64\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=96           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=dev\n\nsource tr8-104B-64.slurm\nStart it as:\nsbatch --array=1-50%1 train-64.slurm\nNow you can easily edit tr8-104B-64.slurm before the next job run and either let the current job finish if it‚Äôs desired or if you need to abort it, just kill the currently running job, e.g.¬†1557903_5 (not job array 1557903) and have the train pick up where it left, but with the edited script.\nThe nice thing is that this requires no changes to the original script (tr8-104B-64.slurm in this example), and the latter can still be started on its own.\nNow, what if something is wrong and you need 10min or 10h to fix something. In this case we suspend the train using:\nscontrol hold &lt;jobid&gt;\nwith  being either a ‚Äúnormal‚Äù job, the id of a job array or the id for a job array step\nand then when ready to continue release the job:\nscontrol release &lt;jobid&gt;",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#troubleshooting",
    "href": "qmd/orchestration/slurm/users.html#troubleshooting",
    "title": "",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nMismatching nodes number\nIf the pytorch launcher fails it often means that the number of SLURM nodes and the launcher nodes are mismatching, e.g.:\ngrep -ir nodes= tr123-test.slurm\n#SBATCH --nodes=40\nNNODES=64\nThis won‚Äôt work. They have to match.\nYou can add a sanity check to your script:\n#!/bin/bash\n#SBATCH --job-name=test-mismatch\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=96           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 0:05:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=prod\n\n[...]\n\nNNODES=2\n\n# sanity check for having NNODES and `#SBATCH --nodes` match, assuming you use NNODES variable\nif [ \"$NNODES\" != \"$SLURM_NNODES\" ]; then\n    echo \"Misconfigured script: NNODES=$NNODES != SLURM_NNODES=$SLURM_NNODES\"\n    exit 1\nfi\n\n[...]\nor you could just do:\n#SBATCH --nodes=2\n[...]\nNNODES=$SLURM_NNODES\nand then it will always be correct\n\n\nFind faulty nodes and exclude them\nSometimes a node is broken, which prevents one from training, especially since restarting the job often hits the same set of nodes. So one needs to be able to isolate the bad node(s) and exclude it from sbatch.\nTo find a faulty node, write a small script that reports back the status of the desired check.\nFor example to test if cuda is available on all nodes:\npython -c 'import torch, socket; print(f\"{socket.gethostname()}: {torch.cuda.is_available()}\")'\nand to only report the nodes that fail:\npython -c 'import torch, socket; torch.cuda.is_available() or print(f\"Broken node: {socket.gethostname()}\") '\nOf course, the issue could be different - e.g.¬†gpu can‚Äôt allocate memory, so change the test script to do a small allocation on cuda. Here is one way:\npython -c \"import torch; torch.ones(1000,1000).cuda()\"\nBut since we need to run the test script on all nodes and not just the first node, the slurm script needs to run it via srun. So our first diagnostics script can be written as:\nsrun --jobid $SLURM_JOBID bash -c 'python -c \"import torch, socket; print(socket.gethostname(), torch.cuda.is_available())\"'\nI slightly changed it, due to an issue with quotes.\nYou can always convert the one liner into a real script and then there is no issue with quotes.\n$ cat &lt;&lt; EOT &gt;&gt; test-nodes.py\n#!/usr/bin/env python\nimport torch, socket\nprint(socket.gethostname(), torch.cuda.is_available())\nEOT\n$ chmod a+x ./test-nodes.py\nNow let‚Äôs create a driver slurm script. Use a few minutes time for this test so that SLURM yields it faster:\n#!/bin/bash\n#SBATCH --job-name=test-nodes\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=96           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 0:05:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=prod\n\nsource $six_ALL_CCFRWORK/start-prod\nsrun --jobid $SLURM_JOBID ./test-nodes.py\nOnce it runs check the logs to see if any reported False, those are the nodes you want to exclude.\nNow once the faulty node(s) is found, feed it to sbatch:\nsbatch --exclude=hostname1,hostname2 ...\nand sbatch will exclude the bad nodes from the allocation.\nAdditionally please report the faulty nodes to #science-support so that they get replaced\nHere are a few more situations and how to find the bad nodes in those cases:\n\n\nBroken NCCL\nIf you‚Äôre testing something that requires distributed setup, it‚Äôs a bit more complex. Here is a slurm script that tests that NCCL works. It sets up NCCL and checks that barrier works:\n#!/bin/bash\n#SBATCH --job-name=test-nodes-nccl\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=96           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 0:05:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=prod\n\nsource $six_ALL_CCFRWORK/start-prod\n\nNNODES=2\n\nGPUS_PER_NODE=4\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=6000\n\nexport LAUNCHER=\"python -u -m torch.distributed.launch \\\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT \\\n    \"\n\nexport SCRIPT=test-nodes-nccl.py\n\ncat &lt;&lt; EOT &gt; $SCRIPT\n#!/usr/bin/env python\nimport torch.distributed as dist\nimport torch\nimport socket\nimport os\nimport fcntl\n\ndef printflock(*msgs):\n    \"\"\" print \"\"\"\n    with open(__file__, \"r\") as fh:\n        fcntl.flock(fh, fcntl.LOCK_EX)\n        try:\n            print(*msgs)\n        finally:\n            fcntl.flock(fh, fcntl.LOCK_UN)\n\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\ntorch.cuda.set_device(local_rank)\ndist.init_process_group(\"nccl\")\nheader = f\"{socket.gethostname()}-{local_rank}\"\ntry:\n    dist.barrier()\n    printflock(f\"{header}: NCCL {torch.cuda.nccl.version()} is OK\")\nexcept:\n    printflock(f\"{header}: NCCL {torch.cuda.nccl.version()} is broken\")\n    raise\nEOT\n\necho $LAUNCHER --node_rank $SLURM_PROCID $SCRIPT\n\nsrun --jobid $SLURM_JOBID bash -c '$LAUNCHER --node_rank $SLURM_PROCID $SCRIPT'\nThe script uses printflock to solve the interleaved print outputs issue.\n\n\nGPU Memory Check\nThis tests if each GPU on the allocated nodes can successfully allocate 77Gb (e.g.¬†to test 80GB A100s) (have to subtract a few GBs for cuda kernels).\nimport torch, os\nimport time\nimport socket\nhostname = socket.gethostname()\n\nlocal_rank = int(os.environ[\"LOCAL_RANK\"]);\n\ngbs = 77\ntry:\n    torch.ones((gbs*2**28)).cuda(local_rank).contiguous() # alloc on cpu, then move to gpu\n    print(f\"{local_rank} {hostname} is OK\")\nexcept:\n    print(f\"{local_rank} {hostname} failed to allocate {gbs}GB DRAM\")\n    pass\n\ntime.sleep(5)\n\n\nBroken Network\nYet another issue with a node is when its network is broken and other nodes fail to connect to it.\nYou‚Äôre likely to experience it with an error similar to:\nwork = default_pg.barrier(opts=opts)\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1616554793803/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8\nncclSystemError: System call (socket, malloc, munmap, etc) failed.\nHere is how to debug this issue:\n\nAdd:\nexport NCCL_DEBUG=INFO\nbefore the srun command and re-run your slurm script.\nNow study the logs. If you find:\nr11i6n2:486514:486651 [1] include/socket.h:403 NCCL WARN Connect to 10.148.3.247&lt;56821&gt; failed : Connection refused\nLet‚Äôs see which node refuses to accept connections. We get the IP address from the error above and reverse resolve it to its name:\nnslookup 10.148.3.247\n247.3.148.10.in-addr.arpa       name = r10i6n5.ib0.xa.idris.fr.\nAdd --exclude=r10i6n5 to your sbatch command and report it to JZ admins.\n\n\n\nRun py-spy or any other monitor program across all nodes\nWhen dealing with hanging, here is how to automatically log py-spy traces for each process.\nOf course, this same process can be used to run some command for all nodes of a given job. i.e.¬†it can be used to run something during the normal run - e.g.¬†dump all the memory usage in each process via nvidia-smi or whatever other program is needed to be run.\ncd ~/prod/code/tr8b-104B/bigscience/train/tr11-200B-ml/\n\nsalloc --partition=prod --nodes=40 --ntasks-per-node=1 --cpus-per-task=96 --gres=gpu:8 --time 20:00:00\n\nbash 200B-n40-bf16-mono.slurm\nIn another shell get the JOBID for the above salloc:\nsqueue -u `whoami` -o \"%.16i %9P %26j %.8T %.10M %.8l %.6D %.20S %R\"\nadjust jobid per above and the nodes count (XXX: probably can remove --nodes=40 altogether and rely on salloc config):\nsrun --jobid=2180718 --gres=gpu:0 --nodes=40 --tasks-per-node=1 --output=trace-%N.out sh -c 'ps aux | grep python | egrep -v \"grep|srun\" | grep `whoami` | awk \"{print \\$2}\" | xargs -I {} py-spy dump --native --pid {}' || echo \"failed\"\nnow all py-spy traces go into the trace-$nodename.out files under cwd.\nThe key is to use --gres=gpu:0 or otherwise the 2nd srun will block waiting for the first one to release the gpus.\nAlso the assumption is that some conda env that has py-spy installed got activated in ~/.bashrc. If yours doesn‚Äôt already do that, add the instruction to load the env to the above command, before the py-spy command - it‚Äôll fail to find it otherwise.\nDon‚Äôt forget to manually release the allocation when this process is done.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#convert-slurm_job_nodelist-into-a-hostfile",
    "href": "qmd/orchestration/slurm/users.html#convert-slurm_job_nodelist-into-a-hostfile",
    "title": "",
    "section": "Convert SLURM_JOB_NODELIST into a hostfile",
    "text": "Convert SLURM_JOB_NODELIST into a hostfile\nSome multi-node launchers require a hostfile - here is how to generate one:\n# autogenerate the hostfile for deepspeed\n# 1. deals with: SLURM_JOB_NODELIST in either of 2 formats:\n# r10i1n8,r10i2n0\n# r10i1n[7-8]\n# 2. and relies on SLURM_STEP_GPUS=0,1,2... to get how many gpu slots per node\n#\n# usage:\n# makehostfile &gt; hostfile\nfunction makehostfile() {\nperl -le '$slots=split /,/, $ENV{\"SLURM_STEP_GPUS\"}; $_=$ENV{\"SLURM_JOB_NODELIST\"}; if (/^(.*?)\\[(\\d+)-(\\d+)\\]/) { print map { \"$1$_ slots=$slots\\n\" } $2..$3} elsif (/,/) { print map { \"$1$_ slots=$slots\\n\" } split /,/ } '\n}",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#environment-variables",
    "href": "qmd/orchestration/slurm/users.html#environment-variables",
    "title": "",
    "section": "Environment variables",
    "text": "Environment variables\nYou can always do:\nexport SOMEKEY=value\nfrom the slurm script to get a desired environment variable passed to the program launched from it.\nAnd you can also add to the top of the slurm script:\n#SBATCH --export=ALL\nThe launched program will see all the environment variables visible in the shell where it was launched from.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#crontab-emulation",
    "href": "qmd/orchestration/slurm/users.html#crontab-emulation",
    "title": "",
    "section": "Crontab Emulation",
    "text": "Crontab Emulation\nOne of the most important Unix tools is the crontab, which is essential for being able to schedule various jobs. It however usually is absent from SLURM environment. Therefore one must emulate it. Here is how.\nFor this presentation we are going to use $WORK/cron/ as the base directory. And that you have an exported environment variable WORK pointing to some location on your filesystem - if you use Bash you can set it up in your ~/.bash_profile or if a different shell is used use whatever startup equivalent file is.\n\n1. A self-perpetuating scheduler job\nWe will use $WORK/cron/scheduler dir for scheduler jobs, $WORK/cron/cron.daily for daily jobs and $WORK/cron/cron.hourly for hourly jobs:\n$ mkdir -p $WORK/cron/scheduler\n$ mkdir -p $WORK/cron/cron.daily\n$ mkdir -p $WORK/cron/cron.hourly\nNow copy these two slurm script in $WORK/cron/scheduler: - cron-daily.slurm - cron-hourly.slurm\nafter editing those to fit your specific environment‚Äôs account and partition information.\nNow you can launch the crontab scheduler jobs:\n$ cd $WORK/cron/scheduler\n$ sbatch cron-hourly.slurm\n$ sbatch cron-daily.slurm\nThis is it, these jobs will now self-perpetuate and usually you don‚Äôt need to think about it again unless there is an even that makes SLURM lose all its jobs.\n\n\n2. Daily and Hourly Cronjobs\nNow whenever you want some job to run once a day, you simply create a slurm job and put it into the $WORK/cron/cron.daily dir.\nHere is an example job that runs daily to update the mlocate file index:\n$ cat $WORK/cron/cron.daily/mlocate-update.slurm\n#!/bin/bash\n#SBATCH --job-name=mlocate-update    # job name\n#SBATCH --ntasks=1                   # number of MP tasks\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread         # we get physical cores not logical\n#SBATCH --time=1:00:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=PARTITION     # edit me\n#SBATCH --account=GROUP@PARTITION # edit me\n\nset -e\ndate\necho \"updating mlocate db\"\n/usr/bin/updatedb -o $WORK/lib/mlocate/work.db -U $WORK --require-visibility 0\nThis builds an index of the files under $WORK which you can then quickly query with:\n/usr/bin/locate -d $WORK/lib/mlocate/work.db pattern\nTo stop running this job, just move it out of the $WORK/cron/cron.daily dir.\nThe same principle applies to jobs placed into the $WORK/cron/cron.hourly dir. These are useful for running something every hour.\nPlease note that this crontab implementation is approximate timing-wise, due to various delays in SLURM scheduling they will run approximately every hour and every day. You can recode these to ask SLURM to start something at a more precise time if you have to, but most of the time the just presented method works fine.\nAdditionally, you can code your own variations to meet specific needs of your project, e.g., every-30min or every-12h jobs.\n\n\n3. Cleanup\nFinally, since every cron launcher job will leave behind a log file (which is useful if for some reason things don‚Äôt work), you want to create a cronjob to clean up these logs. Otherwise you may run out of inodes - these logs files are tiny, but there could be tens of thousands of those.\nYou could use something like this in a daily job.\nfind $WORK/cron -name \"*.out\" -mtime +7 -exec rm -f {} +\nPlease note that it‚Äôs set to only delete files that are older than 7 days, in case you need the latest logs for diagnostics.\n\n\nNuances\nThe scheduler runs with Unix permissions of the person who launched the SLRUM cron scheduler job and so all other SLURM scripts launched by that cron job.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#self-perpetuating-slurm-jobs",
    "href": "qmd/orchestration/slurm/users.html#self-perpetuating-slurm-jobs",
    "title": "",
    "section": "Self-perpetuating SLURM jobs",
    "text": "Self-perpetuating SLURM jobs\nThe same approach used in building a scheduler can be used for creating stand-alone self-perpetuating jobs.\nFor example:\n#!/bin/bash\n#SBATCH --job-name=watchdog          # job name\n#SBATCH --ntasks=1                   # number of MP tasks\n#SBATCH --nodes=1\n#SBATCH --time=0:30:00               # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n#SBATCH --partition=PARTITION        # edit me\n\n# ensure to restart self first 1h from now\nRUN_FREQUENCY_IN_HOURS=1\nsbatch --begin=now+${RUN_FREQUENCY_IN_HOURS}hour watchdog.slurm\n\n... do the watchdog work here ...\nand you launch it once with:\nsbatch watchdog.slurm\nThis then will immediately schedule itself to be run 1 hour from the launch time and then the normal job work will be done. Regardless of whether the rest of the job will succeed or fail, this job will continue relaunching itself approximately once an hour. This is imprecise due to scheduler job starting overhead and node availability issues. But if there is a least one spare node available and the job itself is quick to finish the requirement to run at an approximate frequency should be sufficient.\nAs the majority of SLURM environment in addition to the expensive GPU nodes also provide much cheaper CPU-only nodes, you should choose a CPU-only SLURM partition for any jobs that don‚Äôt require GPUs to run.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#getting-information-about-the-job",
    "href": "qmd/orchestration/slurm/users.html#getting-information-about-the-job",
    "title": "",
    "section": "Getting information about the job",
    "text": "Getting information about the job\nFrom within the slurm file one can access information about the current job‚Äôs allocations.\nGetting allocated hostnames and useful derivations based on that:\nexport HOSTNAMES=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\")\nexport NUM_NODES=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | wc -l)\nexport MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#convert-compact-node-list-to-expanded-node-list",
    "href": "qmd/orchestration/slurm/users.html#convert-compact-node-list-to-expanded-node-list",
    "title": "",
    "section": "Convert compact node list to expanded node list",
    "text": "Convert compact node list to expanded node list\nSometimes you get SLURM tools give you a string like: node-[42,49-51] which will require some coding to expand it into node-42,node-49,node-50,node-51, but there is a special tool to deal with that:\n$ scontrol show hostnames node-[42,49-51]\nnode-42\nnode-49\nnode-50\nnode-51\nVoila!\ncase study: this is for example useful if you want get a list of nodes that were drained because the job was too slow to exit, but really there is no real problem with the nodes. So this one-liner will give you the list of such nodes in an expanded format which you can then script to loop over this list to undrain these nodes after perhaps checking that the processes have died by this time:\nsinfo -R | grep \"Kill task failed\" | perl -lne '/(node-.*[\\d\\]]+)/ && print $1' | xargs -n1 scontrol show hostnames",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/users.html#overcoming-the-lack-of-group-slurm-job-ownership",
    "href": "qmd/orchestration/slurm/users.html#overcoming-the-lack-of-group-slurm-job-ownership",
    "title": "",
    "section": "Overcoming the lack of group SLURM job ownership",
    "text": "Overcoming the lack of group SLURM job ownership\nSLURM runs on Unix, but surprisingly its designers haven‚Äôt adopted the concept of group ownership with regards to SLURM jobs. So if a member of your team started an array of 10 jobs 20h each, and went on vacation - unless you have sudo access you now can‚Äôt do anything to stop those jobs if something is wrong.\nI‚Äôm yet to find why this is so, but so far we have been using a kill switch workaround. You have to code it in your framework. For example, see how it was implemented in Megatron-Deepspeed (Meg-DS). The program polls for a pre-configured at start up path on the filesystem and if it finds a file there, it exits.\nSo if we start Meg-DS with --kill-switch-path $WORK/tmp/training17-kill-switch and then at any point we need to kill the SLURM job, we simply do:\ntouch $WORK/tmp/training17-kill-switch\nand the next time the program gets to check for this file it‚Äôll detect the event and will exit voluntarily. If you have a job array, well, you will have to wait until each job starts, detects the kill switch and exits.\nOf course, don‚Äôt forget to remove it when you‚Äôre done stopping the jobs.\nrm $WORK/tmp/training17-kill-switch\nNow, this doesn‚Äôt always work. If the job is hanging, it‚Äôll never come to the point of checking for kill-switch and the only solution here is to contact the sysadmins to kill the job for you. Sometimes if the hanging is a simple case pytorch‚Äôs distributed setup will typically auto-exit after 30min of preset timeout time, but it doesn‚Äôt always work.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM for users"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html",
    "href": "qmd/orchestration/slurm/admin.html",
    "title": "",
    "section": "",
    "text": "üéª OrchestrationWorking in SLURM EnvironmentSLURM Administration",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#run-a-command-on-multiple-nodes",
    "href": "qmd/orchestration/slurm/admin.html#run-a-command-on-multiple-nodes",
    "title": "",
    "section": "Run a command on multiple nodes",
    "text": "Run a command on multiple nodes\n\nto avoid being prompted with:\n\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nfor every new node you haven‚Äôt logged into yet, you can disable this check with:\necho \"Host *\" &gt;&gt; ~/.ssh/config\necho \"  StrictHostKeyChecking no\" &gt;&gt; ~/.ssh/config\nOf course, check if that‚Äôs secure enough for your needs. I‚Äôm making an assumption that you‚Äôre already on the SLURM cluster and you‚Äôre not ssh‚Äôing outside of your cluster. You can choose not to set this and then you will have to manually approve each new node.\n\nInstall pdsh\n\nYou can now run the wanted command on multiple nodes.\nFor example, let‚Äôs run date:\n$ PDSH_RCMD_TYPE=ssh pdsh -w node-[21,23-26] date\nnode-25: Sat Oct 14 02:10:01 UTC 2023\nnode-21: Sat Oct 14 02:10:02 UTC 2023\nnode-23: Sat Oct 14 02:10:02 UTC 2023\nnode-24: Sat Oct 14 02:10:02 UTC 2023\nnode-26: Sat Oct 14 02:10:02 UTC 2023\nLet‚Äôs do something more useful and complex. Let‚Äôs kill all GPU-tied processes that didn‚Äôt exit when the SLURM job was cancelled:\nFirst, this command will give us all process ids that tie up the GPUs:\nnvidia-smi --query-compute-apps=pid --format=csv,noheader | sort | uniq\nSo we can now kill all those processes in one swoop:\n PDSH_RCMD_TYPE=ssh pdsh -w node-[21,23-26]  \"nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort | uniq | xargs -n1 sudo kill -9\"",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#slurm-settings",
    "href": "qmd/orchestration/slurm/admin.html#slurm-settings",
    "title": "",
    "section": "Slurm settings",
    "text": "Slurm settings\nShow the slurm settings:\nsudo scontrol show config\nThe config file is /etc/slurm/slurm.conf on the slurm controller node.\nOnce slurm.conf was updated to reload the config run:\nsudo scontrol reconfigure\nfrom the controller node.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#auto-reboot",
    "href": "qmd/orchestration/slurm/admin.html#auto-reboot",
    "title": "",
    "section": "Auto-reboot",
    "text": "Auto-reboot\nIf the nodes need to be rebooted safely (e.g.¬†if the image has been updated), adapt the list of the node and run:\nscontrol reboot ASAP node-[1-64]\nFor each of the non-idle nodes this command will wait till the current job ends, then reboot the node and bring it back up to idle.\nNote that you need to have:\nRebootProgram = \"/sbin/reboot\"\nset in /etc/slurm/slurm.conf on the controller node for this to work (and reconfigure the SLURM daemon if you have just added this entry to the config file).",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#changing-the-state-of-the-node",
    "href": "qmd/orchestration/slurm/admin.html#changing-the-state-of-the-node",
    "title": "",
    "section": "Changing the state of the node",
    "text": "Changing the state of the node\nThe change is performed by scontrol update\nExamples:\nTo undrain a node that is ready to be used:\nscontrol update nodename=node-5 state=idle\nTo remove a node from the SLURM‚Äôs pool:\nscontrol update nodename=node-5 state=drain",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#undrain-nodes-killed-due-to-slow-process-exit",
    "href": "qmd/orchestration/slurm/admin.html#undrain-nodes-killed-due-to-slow-process-exit",
    "title": "",
    "section": "Undrain nodes killed due to slow process exit",
    "text": "Undrain nodes killed due to slow process exit\nSometimes processes are slow to exit when a job has been cancelled. If the SLURM was configured not to wait forever it‚Äôll automatically drain such nodes. But there is no reason for those nodes to not be available to the users.\nSo here is how to automate it.\nThe keys is to get the list of nodes that are drained due to \"Kill task failed\", which is retrieved with:\nsinfo -R | grep \"Kill task failed\"\nnow extract and expand the list of nodes, check that the nodes are indeed user-process free (or try to kill them first) and then undrain them.\nEarlier you learned how to run a command on multiple nodes which we will use in this script.\nHere is the script that does all that work for you: undrain-good-nodes.sh\nNow you can just run this script and any nodes that are basically ready to serve but are currently drained will be switched to idle state and become available for the users to be used.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#modify-a-jobs-timelimit",
    "href": "qmd/orchestration/slurm/admin.html#modify-a-jobs-timelimit",
    "title": "",
    "section": "Modify a job‚Äôs timelimit",
    "text": "Modify a job‚Äôs timelimit\nTo set a new timelimit on a job, e.g., 2 days:\nscontrol update JobID=$SLURM_JOB_ID TimeLimit=2-00:00:00\nTo add additional time to the previous setting, e.g.¬†3 more hours.\nscontrol update JobID=$SLURM_JOB_ID TimeLimit=+10:00:00",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/admin.html#when-something-goes-wrong-with-slurm",
    "href": "qmd/orchestration/slurm/admin.html#when-something-goes-wrong-with-slurm",
    "title": "",
    "section": "When something goes wrong with SLURM",
    "text": "When something goes wrong with SLURM\nAnalyze the events log in the SLURM‚Äôs log file:\nsudo cat /var/log/slurm/slurmctld.log\nThis, for example, can help to understand why a certain node got its jobs cancelled before time or the node got removed completely.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Administration"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/performance.html",
    "href": "qmd/orchestration/slurm/performance.html",
    "title": "",
    "section": "",
    "text": "üéª OrchestrationWorking in SLURM EnvironmentSLURM Performance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Performance"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/performance.html#sruns---cpus-per-task-may-need-to-be-explicit",
    "href": "qmd/orchestration/slurm/performance.html#sruns---cpus-per-task-may-need-to-be-explicit",
    "title": "",
    "section": "srun‚Äôs --cpus-per-task may need to be explicit",
    "text": "srun‚Äôs --cpus-per-task may need to be explicit\nYou need to make sure that the launched by srun program receives as many cpu-cores as intended. For example, in a typical case of a ML training program, each gpu needs at least one cpu-core for the process driving it plus a few more cores for the DataLoader. You need multiple cores so that each task can be performed in parallel. If you have 8 gpus and 2 DataLoader workers per gpu, you need at least 3*8=24 cpu-cores per node.\nThe number of cpus per task is defined by --cpus-per-task, which is passed to sbatch or salloc and originally srun would inherit this setting. However, recently this behavior has changed:\nA quote from the sbatch manpage:\n\nNOTE: Beginning with 22.05, srun will not inherit the ‚Äìcpus-per-task value requested by salloc or sbatch. It must be requested again with the call to srun or set with the SRUN_CPUS_PER_TASK environment variable if desired for the task(s).\n\nWhich means that if in the past your SLURM script could have been:\n#SBATCH --cpus-per-task=48\n[...]\n\nsrun myprogram\nand the program launched by srun would have received 48 cpu-cores because srun used to inherit the --cpus-per-task=48 settings from sbatch or salloc settings, according to the quoted documentation since SLURM 22.05 this behavior is no longer true.\nfootnote: I tested with SLURM@22.05.09 and the old behavior was still true, but this is definitely the case with 23.x series as reported here. So the change might have happened in the later 22.05 series.\nSo if you leave things as is, now the program will receive just 1 cpu-core (unless the srun default has been modified).\nYou can easily test if your SLURM setup is affected, using os.sched_getaffinity(0)), as it shows which cpu-cores are eligible to be used by the current process. So it should be easy to count those with len(os.sched_getaffinity(0)).\nHere is how you can test if you‚Äôre affected:\n$ cat test.slurm\n#!/bin/bash\n#SBATCH --job-name=test-cpu-cores-per-task\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=48   # adapt to your env if you have less than 48 cpu cores\n#SBATCH --time=0:10:00\n#SBATCH --partition=x        # adapt to your env to the right partition name\n#SBATCH --output=%x-%j.out\n\nsrun python -c 'import os; print(f\"visible cpu cores: {len(os.sched_getaffinity(0))}\")'\nIf you get\nvisible cpu cores: 48\nthen you don‚Äôt need to do anything, if however you get:\nvisible cpu cores: 1\nor another value smaller than 48 then you‚Äôre affected.\nTo fix that you need to change your SLURM script to either:\n#SBATCH --cpus-per-task=48\n[...]\n\nsrun --cpus-per-task=48 myprogram\nor:\n#SBATCH --cpus-per-task=48\n[...]\n\nSRUN_CPUS_PER_TASK=48\nsrun myprogram",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Performance"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/performance.html#to-enable-hyper-threads-or-not",
    "href": "qmd/orchestration/slurm/performance.html#to-enable-hyper-threads-or-not",
    "title": "",
    "section": "To enable Hyper-Threads or not",
    "text": "To enable Hyper-Threads or not\nAs explained in the Hyper-Threads section you should be able to double the number of available cpu-cores if your CPUs support hyper-threading and for some workloads this may lead to an overall faster performance.\nHowever, you should test the performance w/ and w/o HT, compare the results and choose the setting that gives the best outcome.\ncase study: on AWS p4 nodes I discovered that enabling HT made the network throughput 4x slower. Since then we were careful to have HT disabled on that particular setup.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment",
      "SLURM Performance"
    ]
  },
  {
    "objectID": "qmd/transformers/index.html",
    "href": "qmd/transformers/index.html",
    "title": "ü§ó Transformers",
    "section": "",
    "text": "HuggingFace Transformers notes\nAs this is one of the moment popular ML libraries this section will share some useful tools with HF transformers and others of their libraries.\n(Disclaimer: I worked at HF for 3 years so I‚Äôm biased - for good :)\n\nFaster debug and development with tiny models, tokenizers and datasets\nRe-train hub models from scratch using finetuning examples\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "ü§ó  Transformers"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html",
    "href": "qmd/training/performance/index.html",
    "title": "",
    "section": "",
    "text": "QmdTrainingSoftware Tune Up For The Best Performance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#macs-vs-flop-vs-flops-vs-flops",
    "href": "qmd/training/performance/index.html#macs-vs-flop-vs-flops-vs-flops",
    "title": "",
    "section": "MACs vs FLOP vs FLOPS vs FLOP/s",
    "text": "MACs vs FLOP vs FLOPS vs FLOP/s\nThis section is here to try to disambiguate the common performance metric definitions and their relationship to each other.\nMAC vs FLOP:\n\n1 FLOP (FLoating point OPeration) can be one of addition, subtraction, multiplication, or division operation.\n1 MAC (Multiply-ACCumulate) operation is a multiplication followed by an addition, that is: a * b + c\n\nThus 1 MAC = 2 FLOPs. It‚Äôs also quite common for modern hardware to perform 1 MAC in a single clock cycle.\nPlease note that to calculate the number of MACs in relationship to FLOPs the reverse logic applies, that is MACs = 0.5 FLOPs - it‚Äôs somewhat confusing since we have just said that 1 MAC = 2 FLOPs, but it checks out - observe: 100 FLOPs = 50 MACs - because there are 2 FLOPs in each MAC.\nMoreover, while 1 MAC = 2 FLOPs, the reverse isn‚Äôt necessarily true. That is 2 FLOPs isn‚Äôt necessarily equal to 1 MAC. For example, if you did .5*.6 100 times it‚Äôd be 100 FLOPs, which here would equal to 100 MACs, because here only the multiply part of the MAC is executed.\nFLOP vs FLOPS vs FLOP/s\n\n1 FLOP (FLoating point OPeration) is any floating point addition, subtraction, multiplication, or division operation.\n1 FLOPS (FLoating point OPeration per Second) is how many floating point operations were performed in 1 second - see FLOPS\n\nFurther you will find the following abbreviations: GFLOPS = Giga FLOPS, TFLOPS = Tera FLOPS, etc., since it‚Äôs much easier to quickly grasp 150TFLOPS rather than 150000000000000FLOPS.\nThere is an ambiguity when FLOPS is used in writing - sometimes people use it to indicate the total quantity of operations, at other times it refers to operations per second. The latter is the most common usage and that is the definition used in this book.\nIn scientific writing FLOP/s is often used to clearly tell the reader that it‚Äôs operations per second. Though this particular approach is hard to convert to a variable name since it still becomes flops when illegal characters are removed.\nIn some places you might also see FLOPs, which again could mean either, since it‚Äôs too easy to flip lower and upper case s.\nIf the definition is ambiguous try to search for context which should help to derive what is meant:\n\nIf it‚Äôs a math equation and there is a division by time you know it‚Äôs operations per second.\nIf speed or performance is being discussed it usually refers to operations per second.\nIf it talks about the amount of compute required to do something it refers to the total amount of operations.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#tflops-as-a-performance-metric",
    "href": "qmd/training/performance/index.html#tflops-as-a-performance-metric",
    "title": "",
    "section": "TFLOPS as a performance metric",
    "text": "TFLOPS as a performance metric\nBefore you start optimizing the performance of your training setup you need a metric that you can use to see whether the throughput is improving or not. You can measure seconds per iteration, or iterations per second, or some other such timing, but there is a more useful metric that measures TFLOPS.\nMeasuring TFLOPS is superior because without it you don‚Äôt know whether you are close to the best performance that can be achieved or not. This measurement gives you an indication of how far you‚Äôre from the peak performance reported by the hardware manufacturer.\nIn this section I will use BLOOM‚Äôs training for the exemplification. We used 80GB A100 NVIDIA GPUs and we trained in mixed bf16 regime. So let‚Äôs look at the A100 spec which tells us:\nBFLOAT16 Tensor Core    312 TFLOPS\nTherefore we now know that if we were to only run matmul on huge bf16 matrices of very specific dimensions without copying to and from the device we should get around 312 TFLOPS max.\nPractically though, due to disk IO, communications and copying data from the GPU‚Äôs memory to its computing unit overhead and because we can‚Äôt do everything in bf16 and at times we have to do math in fp32 (or tf32) we can really expect much less than that. The realistic value will vary from accelerator to accelerator, but for A100 in 2022 getting above 50% (155 TFLOPS) was an amazing sustainable throughput for a complex 384 GPUs training setup.\nfootnote: in 2023 the invention of flash attention and other techniques have pushed the bar to more than 50%.\nWhen we first started tuning things up we were at &lt;100 TFLOPS and a few weeks later when we launched the training we managed to get 150 TFLOPS.\nThe important thing to notice here is that we knew that we can‚Äôt push it further by much and we knew that there was no more point to try and optimize it even more.\nSo a general rule of thumb for when you prepare for a massive model training - ask around what‚Äôs the top TFLOPS one can expect to get with a given accelerator on a multi-node setup with the specified precision - and optimize until you get close to that. Once you did stop optimizing and start training.\nfootnote: For 80GB A100s in 2022 that was 155, in 2023 it has been pushed to about 180 TFLOPS.\nfootnote: When calculating TFLOPS it‚Äôs important to remember that the math is different if Gradient checkpointing are enabled, since when it‚Äôs activated more compute is used and it needs to be taken into an account. Usually the cost is of an additional forward path, but recently better methods have been found that saves some of that recomputation.\nFor decoder transformer models the following is an estimation formula which slightly under-reports the real TFLOPS:\nTFLOPS: model_size_in_B * 4 * 2 * seqlen * global_batch_size / (time_in_sec_per_interation * total_gpus * 1e3)\nThe factor of 4 is used with activation/gradient checkpointing, otherwise it will be 3. For 100B+ models, activation checkpointing will almost always be on.\nSo the 3*2 is often called ‚Äúmodel FLOPs‚Äù and 4*2 - ‚Äúhardware FLOPs‚Äù, correlating to MFU and HFU (model and hardware FLOPS per second divided by the accelerator‚Äôs theoretical peak FLOPS)\nperl -le '$ng=64; $ms=52; $gbs=1024; $sp=127; $seqlen=2048; print $ms*4*2*$seqlen*$gbs / ( $sp * $ng * 1e3)'\n(ng = total gpus, ms = model size in B, gbs = global batch size, sp = throughput in seconds)\nHere is the same formula using bash env vars and which breaks down GBS into MBS*DP*GAS (GAS in this case corresponded to pp_chunks which was the number of chunks in the pipeline, but normally GAS just stands for Gradient Accumulation Steps):\necho \"($MSIZE*4*2*SEQLEN*$MICRO_BATCH_SIZE*$DP_SIZE*$GAS)/($THROUGHPUT*$NNODES*4*1000)\" | bc -l\nThe exact formula is in Equation 3 of Section 5.1 of the Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM paper. You can see the code here.\nfootnote: For Inference only it‚Äôd be: 24Bsh^2 + 4Bs^2h floating point operations per layer.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#how-to-improve-speed-and-save-memory",
    "href": "qmd/training/performance/index.html#how-to-improve-speed-and-save-memory",
    "title": "",
    "section": "How To Improve Speed and Save Memory",
    "text": "How To Improve Speed and Save Memory\nThe more GPU memory you have for your batch size (BS) the more efficient the GPUs will be at performing compute, and the faster you will complete your task since you will be able to go through data faster.\nOf course, this section is crucial for when you get GPU OOM with even BS=1 and you don‚Äôt want to rent/buy more hardware.\nHere is an overview of what features can help to either improve speed or save memory\n\n\n\nMethod\nSpeed\nMemory\n\n\n\n\nGradient accumulation\nYes\nYes\n\n\nGradient checkpointing\nYes\nYes\n\n\nMixed precision training\nYes\nNo\n\n\nBatch size\nYes\nYes\n\n\nOptimizer choice\nYes\nYes\n\n\nDataLoader\nYes\nNo\n\n\nDeepSpeed Zero\nNo\nYes\n\n\nFlash Attention\nYes\nYes\n\n\n\n\nAnatomy of Model‚Äôs Operations\nTransformers architecture includes 3 main groups of operations grouped below by compute-intensity.\n\nTensor Contractions\nLinear layers and components of Multi-Head Attention all do batched matrix-matrix multiplications. These operations are the most compute-intensive part of training a transformer.\nStatistical Normalizations\nSoftmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more reduction operations, the result of which is then applied via a map.\nElement-wise Operators\nThese are the remaining operators: biases, dropout, activations, and residual connections. These are the least compute-intensive operations.\n\nThis knowledge can be helpful to know when analyzing performance bottlenecks.\nThis summary is derived from Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020\n\n\nAnatomy of Model‚Äôs Memory Usage\nWe‚Äôve seen that training the model uses much more memory than just putting the model on the GPU. This is because there are many components during training that use GPU memory. The components on GPU memory are the following:\n\nmodel weights\noptimizer states\ngradients\nforward activations saved for gradient computation\ntemporary buffers\nfunctionality-specific memory\n\nA typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory and temp memory.\nLet‚Äôs look at the details.\nModel Weights:\n\n4 bytes * number of parameters for fp32 training\n6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16/bf16 in memory)\n\nOptimizer States:\n\n8 bytes * number of parameters for normal AdamW (maintains 2 states)\n4 bytes * number of parameters for AdamW running at bf16. See this work that uses AnyPrecisionAdamW.\n4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state) or LION, or Adafactor (and others) (Adafactor uses some additional memory beside 4 bytes)\n2 bytes * number of parameters for 8-bit AdamW optimizers like bitsandbytes\n\nGradients\n\n4 bytes * number of parameters for either fp32 precision and in some frameworks with mixed half-precision precision training.\n2 bytes * number of parameters for non-mixed half-precision and in some frameworks with mixed half-precision precision training.\n\nForward Activations\n\nsize depends on many factors, the key ones being sequence length, hidden size and batch size.\n\nThere are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation.\nTemporary Memory\nAdditionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it‚Äôs crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.\nFunctionality-specific memory\nThen your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.\nFor inference, the math is very similar to training, minus optimizer states and gradients. And for model weights there is just a single multiplier of the number of parameters:\n\n6 bytes in mixed precision (4+2)\n4 bytes in fp32\n2 bytes in half precision\n1 byte in quantized int8 precision\n\nAnother excellent resource that takes you through the memory needs and other requirements is Transformer Math 101.\nThe EAI cookbook contains a set of calculation scripts that can output the theoretical memory overhead for a given training or inference calculation run based on your configuration and setup.\nThere is a very handy GPU VRAM Estimator from Alexander Smirnov, and the notes to how it works.\n\n\nAdditional GPU memory usage\nIn addition to the memory usage described in the previous section, there are other consumers of the GPU memory - so you never get the full memory for your model‚Äôs use.\n\nPreloaded CUDA kernels memory usage\nWhen PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU‚Äôs total available memory.\nThe size of allocated memory for cuda kernels varies between different GPUs, and also it can be different between pytorch versions. Let‚Äôs allocate a 4-byte tensor on cuda and check how much GPU memory is used up upfront.\nWith pytorch==1.10.2:\n$ CUDA_MODULE_LOADING=EAGER python -c \"import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \\\nused=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')\"\npt=1.10.2: used=1.78GB, free=77.43GB, total=79.21GB\nWith pytorch==1.13.1:\n$ CUDA_MODULE_LOADING=EAGER python -c \"import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \\\nused=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')\"\npt=1.13.1: used=0.90GB, free=78.31GB, total=79.21GB\nThe older pytorch ‚Äúwasted‚Äù 1.78GB of A100, the newer only 0.9GB, thus saving a whooping 0.9GB, which can be the saving grace for the OOM situations.\nCUDA_MODULE_LOADING=EAGER is needed in the recent pytorch version if we want to force cuda kernels pre-loading, which are otherwise lazy-loaded on demand. But do not use this setting in production since it‚Äôs likely to use more memory than needed. The whole point of lazy-loading is to load only the kernels that are needed.\nWith pytorch==2.1.1:\n$ CUDA_MODULE_LOADING=EAGER python -c \"import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \\\nused=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')\"\npt=2.1.1+cu121: used=0.92GB, free=78.23GB, total=79.15GB\nAs compared to the lazy mode:\n$ python -c \"import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \\\nused=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')\"\npt=2.1.1+cu121: used=0.47GB, free=78.68GB, total=79.15GB\nThere is a 450MB difference, but here we only loaded kernels to do torch.ones - the actual memory allocated at run time with other code using torch API will be somewhere between 0.47 and 0.92GB.\n\n\nMemory fragmentation\nAs the model allocates and frees tensors, the memory could fragment. That is there could be enough free memory to allocate, say, 1GB of contiguous memory, but it could be available in 100s of small segments spread out through the memory and thus even though the memory is available it can‚Äôt be used unless very small allocations are made.\nEnvironment variable PYTORCH_CUDA_ALLOC_CONF comes to help and allows you to replace the default memory allocation mechanisms with more efficient ones. For more information see Memory management.\n\n\n\nBatch sizes\nFirst, there are usually two batch sizes:\n\nmicro batch size (MBS), also known as batch size per gpu - this is how many samples a single gpu consumes during a model‚Äôs single forward call.\nglobal batch size (GBS) - this is the total amount of samples consumed between two optimizer steps across all participating GPUs.\n\nModel replica is how many gpus are needed to fit the full model.\n\nIf the model fits into a single GPU a model replica takes 1 GPU. Usually then one can use multiple GPUs to perform Data Parallelism\nIf the model doesn‚Äôt fit into a single GPU, it‚Äôd usually require some sort of sharding technique - it can be Tensor Parallelism (TP), Pipeline Parallelism (PP), or ZeRO Data Parallelism (ZeRO-DP).\n\nYou can have as many data streams as there are replicas. Which is the same as the value of DP. - So in the simple case of a model fitting into a single GPU. There are as many data streams as there are GPUs. DP=N_GPUS - when the model doesn‚Äôt fit onto a single GPU, then DP=N_GPUs/(TP*PP) in the case of 3D parallelism and DP=ZeRO-DP in the case of ZeRO parallelism.\nGoing back to our global batch size (GBS) it‚Äôs calculated as:\nGBS = MBS*DP\nSo if you have 8 gpus (N_GPUS=8) and your MBS=4 and you do DP you end up with having GBS=32 because:\nGBS = MBS*DP = 4*8 = 32\nIf you use TP with a degree of 2 (TP=2) and PP with a degree of 2 (PP=2) this means each model replica takes 4 gpus (TP*PP), and thus with N_GPUS=8\nDP = N_GPUS/(TP*PP) = 8 / (2*2) = 2\nand now GBS becomes:\nGBS = MBS*DP = 4*2 = 8\nIf your training setup requires Gradient Accumulation, one usually defines the interval of how many steps to wait before performing a gradient accumulation. The term is usually Gradient Accumulation Steps (GAS). If GAS=4 (i.e.¬†sync grads every 4 steps) and TP=1, PP=1 and DP=8:\nDP = N_GPUS/(TP*PP) = 8 / (1*1) = 8\nGBS = MBS*DP*GAS = 4*8*4 = 128\nTypically you want to make the micro batch size as large as possible so that the GPU memory is close to being full, but not too full.\nWith large models usually there is not much free GPU memory left to have a large micro batch size, therefore every additional sample you can fit is important.\nWhile it‚Äôs super important that sequence length and hidden size and various other hyper parameters are high multiples of 2 (64, 128 and higher) to achieve the highest performance, because in most models the batch dimension is flattened with the sequence length dimension during the compute the micro batch size alignment usually has little to no impact on performance.\nTherefore if you tried to fit a micro batch size of 8 and it OOM‚Äôed, but 7 fits - use the latter rather than 4. The higher the batch size the more samples you will be able to fit into a single step.\nOf course, when using hundreds of GPUs your global batch size may become very large. In that case you might use a smaller micro batch size or use less GPUs or switch to a different form of data parallelism so that the GPUs work more efficiently.\n\n\nGradient Accumulation\nThe idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model‚Äôs optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU‚Äôs memory. In turn, however, the added forward and backward passes can slow down the training a bit.\nGradient Accumulation Steps (GAS) is the definition of how many steps are done w/o updating the model weights.\nWhen using Pipeline parallelism a very large Gradient Accumulation is a must to keep the pipeline‚Äôs bubble to the minimum.\nSince the optimizer step isn‚Äôt performed as often with gradient accumulation there is an additional speed up here as well.\nThe following benchmarks demonstrate how increasing the gradient accumulation steps improves the overall throughput (20-30% speedup):\n\nRTX-3090\nA100\n\nWhen data parallelism is used gradient accumulation further improves the training throughput because it reduces the number of gradient reduction calls, which is typically done via the all_reduce collective which costs a 2x size of gradients to be reduced. So for example, if one goes from GAS=1 to GAS=8 in DistributedDataParallelism (DDP) the network overhead is reduced by 8x times, which on a slow inter-node network can lead to a noticeable improvement in the training throughput.\n\n\nGradient checkpointing\nGradient Checkpointing is also known as Activation Recompution, Activation Checkpointing and Checkpoint Activations.\nThis methodology is only relevant for training, and not during inference.\nEnabling gradient checkpointing allows one to trade training throughput for accelerator memory. When this feature is activated instead of remembering the outputs of, say, transformer blocks until the backward pass is done, these outputs are dropped. This frees up huge amounts of accelerator memory. But, of course, a backward pass is not possible without having the outputs of forward pass, and thus they have to be recalculated.\nThis, of course, can vary from model to model, but typically one pays with about 20-25% decrease in throughput, but since a huge amount of gpu memory is liberated, one can now increase the batch size per gpu and thus overall improve the effective throughput of the system. In some cases this allows you to double or quadruple the batch size if you were already able to do a small batch size w/o OOM. (Recent papers report as high as 30-40% additional overhead.)\nActivation checkpointing and gradient checkpointing are 2 terms for the same methodology.\nFor example, in HF Transformers models you do model.gradient_checkpointing_enable() to activate it in your custom Trainer or if you use the HF Trainer then you‚Äôd activate it with --gradient_checkpointing 1.\nXXX: expand on new tech from the paper: Reducing Activation Recomputation in Large Transformer Models which found a way to avoid most activation recomputations and thus save both memory and compute.\n\n\nMemory-efficient optimizers\nThe most common optimizer is Adam. It and its derivatives all use 8 bytes per param (2x fp32 tensors - one for each momentum), which account for almost half the memory allocation for the model, optimizer and gradients. So at times using other optimizers may save the day, if they successfully train that is. Not all optimizers are suitable for all training tasks.\n4-byte optimizers:\n\nThere are optimizers like Adafactor that need only 4 bytes. Same goes for the recently invented LION optimizer.\nAnyPrecisionAdamW. Some courageous souls try to do the whole training in BF16 (not mixed precision!), including the optimizer and thus need only 4 bytes per parameter for optim states. See this work. Hint: this optimizer requires Kahan summation and/or stochastic rounding, see Revisiting BFloat16 Training (2020). You need only 8 bytes per parameter for weights, optim states and gradients here! Instead of 18!\n\n2-byte optimizers:\n\nThere are quantized solutions like bnb.optim.Adam8bit which uses only 2 bytes instead of 8 (1 byte per momentum). You can get it from here. Once installed, if you‚Äôre using HF Trainer, you can enable it on with just passing --optim adamw_bnb_8bit!\n\nFor speed comparisons see this benchmark Speed-wise:apex‚Äôs apex.optimizers.FusedAdam optimizer is so far the fastest implementation of Adam. Since pytorch-2.0 torch.optim.AdamW added support for fused=True option, which brings it almost on par with apex.optimizers.FusedAdam.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#model-execution-speed",
    "href": "qmd/training/performance/index.html#model-execution-speed",
    "title": "",
    "section": "Model execution speed",
    "text": "Model execution speed\n\nforward vs backward Execution Speed\nFor convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it‚Äôs typical for an activation to have to read more data in the backward than in the forward (e.g.¬†activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput).",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#memory-profiler-tools",
    "href": "qmd/training/performance/index.html#memory-profiler-tools",
    "title": "",
    "section": "Memory profiler tools",
    "text": "Memory profiler tools\nIn this chapter we discussed the theoretical math of how much this or that feature should consume in MBs of memory. But often in reality things aren‚Äôt exactly the same. So you plan for a certain model size and batch sizes but when you come to use it suddenly there is not enough memory. So you need to work with your actual code and model and see which part takes how much memory and where things got either miscalculated or some additional missed overhead hasn‚Äôt been accounted for.\nYou‚Äôd want to use some sort of memory profiler for that purpose. There are various memory profilers out there.\nOne useful tool that I developed for quick and easy profiling of each line or block of code is IPyExperiments. You just need to load your code into a jupyter notebook and it‚Äôll automatically tell you how much CPU/GPU memory each block allocates/frees. So e.g.¬†if you want to see how much memory loading a model took, and then how much extra memory a single inference step took - including peak memory reporting.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#vector-and-matrix-size-divisibility",
    "href": "qmd/training/performance/index.html#vector-and-matrix-size-divisibility",
    "title": "",
    "section": "Vector and matrix size divisibility",
    "text": "Vector and matrix size divisibility\nThe paper, The Case for Co-Designing Model Architectures with Hardware investigates the effects of transformer sizing on the underlying hardware. The associated scripts allow you to run the benchmarks yourself if you‚Äôre running on hardware besides NVIDIA V100/A100.\nOne gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model.\nFor fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for input/output neuron counts and batch size.\nTensor Core Requirements define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it‚Äôs 64!\nFor parameters that are small, there is also Dimension Quantization Effects to consider, this is where tiling happens and the right multiplier can have a significant speedup.\nThe Case for Co-Designing Model Architectures with Hardware provides much greater detail on tile/wave quantization and the number of attention heads, but the highlights are:\n\nTile and wave quantization\nNotation:\n\na: Number of attention heads\nh: Hidden dimension size\ns: Sequence length\nb: Microbatch size\nt: Tensor-parallel size\n\nFirst, some background.\nNVIDIA GPUs divide the output matrix into regions or tiles as shown in the below figure and schedule them to one of the available streaming multiprocessors (SM) on the GPU (e.g., A100 GPUs have 108 SMs). Each tile or thread block is processed in a Tensor Core, which NVIDIA introduced for fast tensor operations. NVIDIA Tensor Cores are only available for GEMMs with appropriate dimensions. Tensor Cores can be fully utilized when GEMM dimensions m, k, and n are multiples of 16 bytes and 128 bytes for V100 and A100 GPUs, respectively. Since a FP16 element is 2 bytes, this corresponds to dimension sizes that are multiples of 8 and 64 elements, respectively. If these dimension sizes are not possible, Tensor Cores perform better with larger multiples of 2 bytes.\n\n\n\ntiling\n\n\nThere are multiple tile sizes that the kernel can choose from. If the GEMM size does not divide evenly into the tile size, there will be wasted compute, where the thread block must execute fully on the SM, but only part of the output is necessary. This is called the tile quantization effect, as the output is quantized into discrete tiles.\nAnother quantization effect is called wave quantization. As the thread blocks are scheduled to SMs, only 108 thread blocks at a time may be scheduled. If, for example, 109 thread blocks must be scheduled, two rounds, or waves, of thread blocks must be scheduled to GPU. The first wave will have 108 thread blocks, and the second wave will have 1. The second wave will have almost the same latency as the first, but with a small fraction of the useful compute. As the matrix size increases, the last or tail wave grows. The throughput will increase, until a new wave is required. Then, the throughput will drop.\nWhat this means for transformers, is that for a given ratio of h/a, one needs to ensure they‚Äôre on the crest of a wave. If you‚Äôre using NVIDIA V100/A100 GPUs, we‚Äôve already done this work for you in https://arxiv.org/pdf/2401.14489.pdf\nAn example of this for 32 attention heads:\n\n\n\nwave quantization\n\n\nMore powers of 2 in h/a helps!\n\n\nNumber and size of attention heads\nGenerally, it‚Äôs most computationally efficient to keep the ratio of h/a as large as possible without accuracy degradation. A good figure from The Case for Co-Designing Model Architectures with Hardware showing this effect is:\n\n\n\nattention heads\n\n\n\n\nFlash attention\nIf you‚Äôre using Flash Attention, good news! These MHA sizing constraints are taken care of for you. Your only constraint is to have a large enough ratio of h/a to saturate your GPU cores:\n\n\n\nflash attention\n\n\n\n\nFinal recommendations for sizing\nThe full recommendations are: 1. Vocab size divisible by 64 2. Microbatch size as large as possible 3. b*s, h/a, and h/t should be divisible by a power of 2 4. (b*a)/t should be an integer 5. t should be small as possible",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/performance/index.html#contributors",
    "href": "qmd/training/performance/index.html#contributors",
    "title": "",
    "section": "Contributors",
    "text": "Contributors\nQuentin Anthony",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Software Tune Up For The Best Performance"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/index.html",
    "href": "qmd/training/instabilities/index.html",
    "title": "",
    "section": "",
    "text": "QmdTrainingAvoiding, Recovering From and Understanding Instabilities",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Avoiding, Recovering From and Understanding Instabilities"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/index.html#learning-from-training-logbooks",
    "href": "qmd/training/instabilities/index.html#learning-from-training-logbooks",
    "title": "",
    "section": "Learning from Training Logbooks",
    "text": "Learning from Training Logbooks\nThe best learning is to read Publicly available training LLM/VLM logbooks because there you can see exactly what happened and how the problem has been overcome.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Avoiding, Recovering From and Understanding Instabilities"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/index.html#std-init",
    "href": "qmd/training/instabilities/index.html#std-init",
    "title": "",
    "section": "STD Init",
    "text": "STD Init\nCorrectly initializing the initial distribution of the tensors can have a tremendous impact on training‚Äôs stability. The std value isn‚Äôt fixed and depends on the hidden dimension size.\nThis proved to be a very crucial setting in our pre-BLOOM 104B experiments and we couldn‚Äôt break past the first few thousands iterations until we figured out that the 0.02 default --init-method-std in Megatron-LM was a way too big for our model.\nWe referred to these two sources:\n\n‚ÄúTransformers without Tears‚Äù paper https://arxiv.org/abs/1910.05895 prescribes: sqrt(2/(NHIDDEN*5))\nThe 530B training paper https://arxiv.org/abs/2201.11990 they used an even smaller init formula: sqrt(1/(NHIDDEN*3))\n\nand decided to go with the 530B one as it leads to an even smaller init value.\nTo make it easier to compare the two formulas, they can be rewritten as: 1. sqrt(0.4000/NHIDDEN) 2. sqrt(0.3333/NHIDDEN)\nThus for NHIDDEN=14336 the math was sqrt(1/(14336*3)) = 0.00482 and that‚Äôs what we used. It surely wasn‚Äôt the only reason why we had no stability issues during BLOOM-176B training, but I think it was one of the crucial ones.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Avoiding, Recovering From and Understanding Instabilities"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/index.html#numerical-instabilities",
    "href": "qmd/training/instabilities/index.html#numerical-instabilities",
    "title": "",
    "section": "Numerical instabilities",
    "text": "Numerical instabilities\nCertain mathematical operations could be unstable when dealing with low precision numbers.\nFor example, please see this very interesting PyTorch guide on numerical stability.\nNow let‚Äôs look at a specific example of this concept in action.\nDuring 104B training experiments where fp16 mixed precision was used - the following improvement was proposed by Corby Rosset to make self-attention more stable.\nSpecifically this line shows that the norm_factor may be multiplied after the Query * Key matrix multiplication. If the dim of Q and K are very large, the output may blow up and the norm_factor won‚Äôt be able to save it.\nProposal: move the norm_factor inward, so Q and K are scaled down before matrix multiply:\n        matmul_result = torch.baddbmm(\n            matmul_result,\n            1.0/math.sqrt(self.norm_factor) * query_layer.transpose(0, 1),   # [b * np, sq, hn]\n            1.0/math.sqrt(self.norm_factor) * key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n            beta=0.0 if alibi is None else 1.0, alpha=1.0)\n\n        # change view to [b, np, sq, sk]\n        attention_scores = matmul_result.view(*output_size)\nTo make the operation mathematically equivalent, moving the norm factor inward requires taking sqrt again if n is a scalar, A and B matrices:\nn * (A dot B) === (sqrt(n) * A) dot (sqrt(n) * B)\nNow A and B dimensions can be significantly larger.\nFor CUDA kernel writers CuBlas‚Äôs GemmStridedBatchedEx at the time of this writing has a similar issue. It is defined as:\nC+i*strideC=Œ±op(A+i*strideA)op(B+i*strideB)+Œ≤(C+i*strideC), for i ‚àà[0,batchCount‚àí1]\nThe issue is that alpha is multiplied after the matrix-matrix multiplication is done so it can cause instability.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Avoiding, Recovering From and Understanding Instabilities"
    ]
  },
  {
    "objectID": "qmd/training/instabilities/index.html#bad-combination-of-data-batch-and-model-parameter-state",
    "href": "qmd/training/instabilities/index.html#bad-combination-of-data-batch-and-model-parameter-state",
    "title": "",
    "section": "‚ÄúBad‚Äù combination of data batch and model parameter state",
    "text": "‚ÄúBad‚Äù combination of data batch and model parameter state\nPaLM team observed dozens of loss spikes at ‚Äúhighly irregular intervals‚Äù when training larger models. While they were not able to track down the root cause, they mitigated the issue by restarting from an earlier checkpoint and skipping potentially problematic data batches. Section 5.1 Training instability",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Avoiding, Recovering From and Understanding Instabilities"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html",
    "href": "qmd/training/fault-tolerance/index.html",
    "title": "",
    "section": "",
    "text": "QmdTrainingFault Tolerance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#always-plan-to-have-more-nodes-than-needed",
    "href": "qmd/training/fault-tolerance/index.html#always-plan-to-have-more-nodes-than-needed",
    "title": "",
    "section": "Always plan to have more nodes than needed",
    "text": "Always plan to have more nodes than needed\nThe reality of the GPU devices is that they tend to fail. Sometimes they just overheat and shut down, but can recover, at other times they just break and require a replacement.\nThe situation tends to ameliorate as you use the same nodes for some weeks/months as the bad apples get gradually replaced, but if you are lucky to get a new shipment of GPUs and especially the early GPUs when the technology has just come out, expect a sizeable proportion of those to fail.\nTherefore, if you need 64 nodes to do your training, make sure that you have a few spare nodes and study how quickly you can replace failing nodes should the spares that you have not be enough.\nIt‚Äôs hard to predict what the exact redundancy percentage should be, but 5-10% shouldn‚Äôt be unreasonable. The more you‚Äôre in a crunch to complete the training on time, the higher the safety margin should be.\nOnce you have the spare nodes available, validate that your SLURM environment will automatically remove any problematic nodes from the pool of available nodes so that it can automatically replace the bad nodes with the good ones.\nIf you use a non-SLURM scheduler validate that it too can do unmanned bad node replacements.\nYou also need at least one additional node for running various preventative watchdogs (discussed later in this chapter), possibly offloading the checkpoints and doing cleanup jobs.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#queue-up-multiple-training-jobs",
    "href": "qmd/training/fault-tolerance/index.html#queue-up-multiple-training-jobs",
    "title": "",
    "section": "Queue up multiple training jobs",
    "text": "Queue up multiple training jobs\nThe next crucial step is to ensure that if the training crashed, there is a new job lined up to take place of the previous one.\nTherefore, when a training is started, instead of using:\nsbatch train.slurm\nYou‚Äôd want to replace that with:\nsbatch --array=1-10%1 train.slurm\nThis tells SLURM to book a job array of 10 jobs, and if one of the job completes normally or it crashes, it‚Äôll immediately schedule the next one.\nfootnote: %1 in --array=1-10%1 tells SLURM to launch the job array serially - one job at a time.\nIf you have already started a training without this provision, it‚Äôs easy to fix without aborting the current job by using the --dependency argument:\nsbatch --array=1-10%1 --dependency=CURRENTLY_RUNNING_JOB_ID train.slurm\nSo if your launched job looked like this:\n$ squeue -u `whoami` -o \"%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R\"\n     JOBID PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)\n       87    prod    my-training-10b  RUNNING 2-15:52:19 1-16:00:00   64    2023-10-07T01:26:28 node-[1-63]\nYou will not that the current‚Äôs JOBID=87 and now you can use it in:\nsbatch --array=1-10%1 --dependency=87 train.slurm\nand then the new status will appear as:\n$ squeue -u `whoami` -o \"%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R\"\n     JOBID PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)\n       87    prod    my-training-10b  RUNNING 2-15:52:19 1-16:00:00   64    2023-10-07T01:26:28 node-[1-63]\n 88_[10%1]   prod    my-training-10b  PENDING       0:00 1-16:00:00   64                    N/A (Dependency)\nSo you can see that an array of 10 jobs (88_[10%1]) was appended to be started immediately after the current job (87) completes or fails.\nGranted that if the condition that lead to the crash is still there the subsequent job will fail as well. For example, if the storage device is full, no amount of restarts will allow the training to proceed. And we will discuss shortly how to avoid this situation.\nBut since the main reason for training crashes is failing GPUs, ensuring that faulty nodes are automatically removed and the new job starts with a new set of nodes makes for a smooth recovery from the crash.\nIn the SLURM lingo, the removed nodes are given a new status called drained. Here is an example of a hypothetical SLURM cluster:\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nprod*       up   infinite       4  drain node-[0-3]\nprod*       up   infinite      47  alloc node-[4-51]\nprod*       up   infinite      23   idle node-[52-73]\nHere we have 47 nodes being used (alloc), 23 available (idle) and 4 unavailable (drained).\nThe sysadmin is expected to periodically check the drained nodes, fix or replace them and then make them again available to be used by changing their state to idle.\nThe other approach is to daisy-chain jobs via --dependency as explained here. Both of these approaches could also be combined.\nHow do you know when the job array or a daisy chain should not resume - well, normally the training loop will exit immediately if it knows the job is done. But you could also add features like kill switch which are even easier to use to prevent a job array from running.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#frequent-checkpoint-saving",
    "href": "qmd/training/fault-tolerance/index.html#frequent-checkpoint-saving",
    "title": "",
    "section": "Frequent checkpoint saving",
    "text": "Frequent checkpoint saving\nWhenever the training job fails, many hours of training can be lost. This problem is mitigated by a frequent checkpoint saving. When the training is resumed it‚Äôll continue from the last checkpoint saved. If the failure occurred 12 hours after the last checkpoint has been saved, 12 hours of training is lost and needs to be re-done. This can be very expensive if the training uses hundreds of GPUs.\nIn theory one could save a checkpoint every 10 minutes and only ever lose 10 minutes of training time, but this too would dramatically delay the reaching of the finish line because large models can‚Äôt be saved quickly and if the saving time starts to create a bottleneck for the training this approach becomes counterproductive.\nDepending on your checkpointing methodology and the speed of your IO storage partition the saving of a large model can take from dozens of seconds to several minutes. Therefore, the optimal approach to saving frequency lies somewhere in the middle.\nThe math is quite simple - measure the amount of time it takes to save the checkpoint, multiply it by how many times you‚Äôd want to save it and see how much of an additional delay the checkpoint saving will contribute to the total training time.\nUse case: While training BLOOM-176B we had an incredibly fast GPFS over NVME filesystem and it took only 40 seconds to save a 2.3TB checkpoint written concurrently on 384 processes. We saved a checkpoint approximately every 3 hours. As we trained for about 3 months, that means that we saved about 720 checkpoints (90 days * 24h / 3h) - that is an additional 8 hours was spent just saving the checkpoints (720 times * 40 secs / 3600 secs) - or ~0.37% of the total training time (8h / (90 days * 24 hours). Now say if the IO were to be 5 times slower, which is not uncommon on the cloud unless one pays for premium IO, that would have become 2% of the training time, which would be quite significant.\nfootnote: If you don‚Äôt have a large local storage and you have to offload the checkpoints to the cloud, make sure that the 2 most frequent checkpoints remain local to allow for a quick resume. The reason for 2 and not 1, is that it‚Äôs possible that the very last checkpoint got corrupted or didn‚Äôt finish saving if a crash occurred during its saving.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#kill-switch",
    "href": "qmd/training/fault-tolerance/index.html#kill-switch",
    "title": "",
    "section": "Kill switch",
    "text": "Kill switch\nIn many SLURM environments users have no sudo access and when one user started a training and went to sleep, and then a problem has been discovered, the other users can‚Äôt easily stop the training and restart it again.\nThis was the situation during BLOOM-176B training and we implemented a kill-switch to handle that. The mechanism is very simple. The training loop polls for a specific file to appear before starting a new iteration and if the file is there the program saves the checkpoint and exits, allowing users other than the one who started the previous training to change things and restart it again. An additional poll was added at the very beginning of main so that if there was a long job array queued by the user who is asleep they could be ‚Äúburned through‚Äù quickly by getting each job exit quickly on start.\nThis is also discussed here.\nThis facility helps to minimize the amount of wasted training time.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#save-switch",
    "href": "qmd/training/fault-tolerance/index.html#save-switch",
    "title": "",
    "section": "Save switch",
    "text": "Save switch\nWhile mentioning the kill switch, it might be good to quickly mention its cousin, a save switch. Similarly to the kill switch the save switch is a variation of the former where instead of stopping the training, if the training loop discovers that a save-switch file appears - it will save a checkpoint, but will continue training. It‚Äôll also automatically remove the save-switch from the file-system, so that it won‚Äôt accidentally start saving a checkpoint after every iteration.\nThis feature can be very useful for those who watch the training charts. If one sees an interesting or critical situation in the training loss or some other training metric one can quickly ask the training program to save the checkpoint of interest and be able to later reproduce the current situation at will.\nThe main use of this feature is around observing training loss spikes and divergences.\n(note-to-self: better belongs to instabilities chapter)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#prevention",
    "href": "qmd/training/fault-tolerance/index.html#prevention",
    "title": "",
    "section": "Prevention",
    "text": "Prevention\nThe easiest way to avoid losing training time is to prevent certain types of problems from happening. While one can‚Äôt prevent a GPU from failing, other than ensuring that adequate cooling is provided, one can certainly ensure that there is enough of disk space remaining for the next few days of training. This is typically done by running scheduled watchdogs that monitor various resources and alert the operator of possible problems long before they occur.\n\nScheduled Watchdogs\nBefore we discuss the various watchdogs it‚Äôs critical that you have a mechanism that allows you to run scheduled jobs. In the Unix world this is implemented by the crontab facility.\nHere is an example of how ~/bin/watch-fs.sh can be launched every hour:\n0 * * * * ~/bin/watch-fs.sh\nThe link above explains how to configure a crontab job to run at various other frequencies.\nTo setup a crontab, execute crontab -e and check which jobs are scheduled crontab -l.\nThe reason I don‚Äôt go into many details is because many SLURM environments don‚Äôt provide access to the crontab facility. And therefore one needs to use other approaches to scheduling jobs.\nThe section on Crontab Emulation discusses how to implement crontab-like SLURM emulation and also Self-perpetuating SLURM jobs.\n\n\nNotification facility\nThen you need to have one or more notification facilities.\nThe simplest one is to use email to send alerts. To make this one work you need to ensure that you have a way to send an email from the SLURM job. If it isn‚Äôt already available you can request this feature from your sysadmin or alternatively you might be able to use an external SMTP server provider.\nIn addition to email you could probably also setup other notifications, such as SMS alerting and/or if you use Slack to send slack-notifications to a channel of your choice.\nOnce you understand how to schedule watchdogs and you have a notification facility working let‚Äôs next discuss the critical watchdogs.\n\n\nIs-job-running watchdog\nThe most obvious watchdog is one which checks that there is a training SLURM job running or more are scheduled to run.\nHere is an example slurm-status.py that was used during BLOOM-176B training. This watchdog was sending an email if a job was detected to be neither running nor scheduled and it was also piping its check results into the main training‚Äôs log file. As we used Crontab Emulation, we simply needed to drop slurm-status.slurm into the cron/cron.hourly/ folder and the previously launched SLURM crontab emulating scheduler would launch this check approximately once an hour.\nThe key part of the SLURM job is:\ntools/slurm-status.py --job-name $WATCH_SLURM_NAME 2&gt;&1 | tee -a $MAIN_LOG_FILE\nwhich tells the script which job name to watch for, and you can also see that it logs into a log file.\nFor example, if you launched the script with:\ntools/slurm-status.py --job-name my-training-10b\nand the current status report shows:\n$ squeue -u `whoami` -o \"%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R\"\n  JOBID    PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)\n    87     prod      my-training-10b  RUNNING 2-15:52:19 1-16:00:00  64    2023-10-07T01:26:28 node-[1-63]\nthen all is good. But if my-training-10b job doesn‚Äôt show the alert will be sent.\nYou can now adapt these scripts to your needs with minimal changes of editing the path and email addresses. And if it wasn‚Äôt you who launched the job then replace whoami with the name of the user who launched it. whoami only works if it was you who launched it.\n\n\nIs-job-hanging watchdog\nIf the application is doing torch.distributed or alike and a hanging occurs during one of the collectives, it‚Äôll eventually timeout and throw an exception, which would restart the training and one could send an alert that the job got restarted.\nHowever, if the hanging happens during another syscall which may have no timeout, e.g.¬†reading from the disk, the application could easily hang there for hours and nobody will be the wiser.\nMost applications do periodic logging, e.g., most training log the stats of the last N steps every few minutes. Then one could check if the log file has been updated during the expected time-frame - and if it didn‚Äôt - send an alert. You could write your own, or use io-watchdog for that.\n\n\nLow disk space alerts\nThe next biggest issue is running out of disk space. If your checkpoints are large and are saved frequently and aren‚Äôt offloaded elsewhere it‚Äôs easy to quickly run out of disk space. Moreover, typically multiple team members share the same cluster and it could be that your colleagues could quickly consume a lot of disk space. Ideally, you‚Äôd have a storage partition that is dedicated to your training only, but often this is difficult to accomplish. In either case you need to know when disk space is low and space making action is to be performed.\nNow what should be the threshold at which the alerts are triggered. They need to be made not too soon as users will start ignoring these alerts if you start sending those at say, 50% usage. But also the percentage isn‚Äôt always applicable, because if you have a huge disk space shared with others, 5% of that disk space could translate to many TBs of free disk space. But on a small partition even 25% might be just a few TBs. Therefore really you should know how often you write your checkpoints and how many TBs of disk space you need daily and how much disk space is available.\nUse case: During BLOOM training we wrote a 2.3TB checkpoint every 3 hours, therefore we were consuming 2.6TB a day!\nMoreover, often there will be multiple partitions - faster IO partitions dedicated to checkpoint writing, and slower partitions dedicated to code and libraries, and possibly various other partitions that could be in use and all of those need to be monitored if their availability is required for the training not crashing.\nHere is another caveat - when it comes to distributed file systems not all filesystems can reliably give you a 100% of disk space you acquired. In fact with some of those types you can only reliably use at most ~80% of the allocated storage space. The problem is that these systems use physical discs that they re-balance at the scheduled periods or triggered events, and thus any of these individual discs can reach 100% of their capacity and lead to a failed write, which would crash a training process, even though df would report only 80% space usage on the partition. We didn‚Äôt have this problem while training BLOOM-176B, but we had it when we trained IDEFICS-80B - 80% there was the new 100%. How do you know if you have this issue - well, usually you discover it while you prepare for the training.\nAnd this is not all. There is another issue of inodes availability and some storage partitions don‚Äôt have very large inode quotas. Python packages are notorious for having hundreds to thousands of small files, which combined take very little total space, but which add up to tens of thousands of files in one‚Äôs virtual environment and suddenly while one has TBs of free disk space available, but runs out of free inodes and discovering their training crashing.\nFinally, many distributed partitions don‚Äôt show you the disk usage stats in real time and could take up to 30min to update.\nfootnote: Use df -ih to see the inodes quota and the current usage.\nfootnote: Some filesystems use internal compression and so the reported disk usage can be less than reality if copied elsewhere, which can be confusing.\nSo here is fs-watchdog.py that was used during BLOOM-176B training. This watchdog was sending an email if any of the storage requirements thresholds hasn‚Äôt been met and here is the corresponding fs-watchdog.slurm that was driving it.\nIf you study the watchdog code you can see that for each partition we were monitoring both the disk usage and inodes. We used special quota tools provided by the HPC to get instant stats for some partitions, but these tools didn‚Äôt work for all partitions and there we had to fallback to using df and even a much slower du. As such it should be easy to adapt to your usecase.\n\n\nDealing with slow memory leaks\nSome programs develop tiny memory leaks which can be very difficult to debug. Do not confuse those with the usage of MMAP where the program uses the CPU memory to read quickly read data from and where the memory usage could appear to grow over time, but this is not real as this memory gets freed when needed. You can read A Deep Investigation into MMAP Not Leaking Memory to understand why.\nOf course, ideally one would analyze their software and fix the leak, but at times the leak could be coming from a 3rd party package or can be very difficult to diagnose and there isn‚Äôt often the time to do that.\nWhen it comes to GPU memory, there is the possible issue of memory fragmentation, where over time more and more tiny unused memory segments add up and make the GPU appear to have a good amount of free memory, but when the program tries to allocate a large tensor from this memory it fails with the OOM error like:\nRuntimeError: CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 8.00 GiB total capacity;\n142.76 MiB already allocated; 6.32 GiB free; 158.00 MiB reserved in total by PyTorch)\nIn this example if there are 6.32GB free, how comes that 304MB couldn‚Äôt be allocated.\nOne of the approaches my team developed during IDEFICS-80B training where there was some tiny CPU memory leak that would often take multiple days to lead to running out of CPU memory was to install a watchdog inside the training loop that would check the memory usage and if a threshold was reached it‚Äôd voluntarily exit the training loop. The next training job would then resume with all the CPU memory reclaimed.\nfootnote: The reality of machine learning trainings is that not all problems can be fixed with limited resources and often times a solid workaround provides for a quicker finish line, as compared to ‚Äústopping the presses‚Äù and potentially delaying the training for weeks, while trying to figure out where the problem is. For example we trained BLOOM-176B with CUDA_LAUNCH_BLOCKING=1 because the training would hang without it and after multiple failed attempts to diagnose that we couldn‚Äôt afford any more waiting and had to proceed as is. Luckily this environment variable that normally is used for debug purposes and which in theory should make some CUDA operations slower didn‚Äôt actually make any difference to our throughput. But we have never figured out what the problem was and today it doesn‚Äôt matter at all that we haven‚Äôt, as we moved on with other projects which aren‚Äôt impacted by that issue.\nThe idea is similar to the kill and save switches discussed earlier, but here instead of polling for a specific file appearance we simply watch how much resident memory is used. For example here is how you‚Äôd auto-exit if the OS shows only 5% of the virtual cpu memory remain:\nimport psutil\nfor batch in iterator:\n    total_used_percent = psutil.virtual_memory().percent\n    if total_used_percent &gt; 0.95:\n        print(f\"Exiting early since the cpu memory is almost full: ({total_used_percent}%)\")\n        save_checkpoint()\n        sys.exit()\n\n    train_step(batch)\nSimilar heuristics could be used for setting a threshold for GPU memory usage, except one needs to be aware of cuda tensor caching and python garbage collection scheduling, so to get the actual memory usage you‚Äôd need to do first run the garbage collector then empty the cuda cache and only then you will get real memory usage stats and then gracefully exit the training if the GPU is too close to being full.\nimport gc\nimport torch\n\nfor batch in iterator:\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # get mem usage in GBs and exit if less than 2GB of free GPU memory remain\n    free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info());\n    if free &lt; 2:\n        print(f\"Exiting early since the GPU memory is almost full: ({free}GB remain)\")\n        save_checkpoint()\n        sys.exit()\n\n    train_step(batch)\nfootnote: don‚Äôt do this unless you really have to, since caching makes things faster. Ideally figure out the fragmentation issue instead. For example, look up max_split_size_mb in the doc for PYTORCH_CUDA_ALLOC_CONF as it controls how memory is allocated. Some frameworks like Deepspeed solve this by pre-allocating tensors at start time and then reuse them again and again preventing the issue of fragmentation altogether.\nfootnote: this simplified example would work for a single node. For multiple nodes you‚Äôd need to gather the stats from all participating nodes and find the one that has the least amount of memory left and act upon that.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#dealing-with-forced-resource-preemption",
    "href": "qmd/training/fault-tolerance/index.html#dealing-with-forced-resource-preemption",
    "title": "",
    "section": "Dealing with forced resource preemption",
    "text": "Dealing with forced resource preemption\nEarlier you have seen how the training can be gracefully stopped with a kill switch solution and it‚Äôs useful when you need to stop or pause the training on demand.\nOn HPC clusters SLURM jobs have a maximum runtime. A typical one is 20 hours. This is because on HPCs resources are shared between multiple users/groups and so each is given a time slice to do compute and then the job is forcefully stopped, so that other jobs could use the shared resources.\nfootnote: this also means that you can‚Äôt plan how long the training will take unless your jobs run with the highest priority on the cluster. If your priority is not the highest it‚Äôs not uncommon to have to wait for hours and sometimes days before your job resumes.\nOne could, of course, let the job killed and hope that not many cycles were spent since the last checkpoint was saved and then let the job resume from this checkpoint, but that‚Äôs quite wasteful and best avoided.\nThe efficient solution is to gracefully exit before the hard tile limit is hit and the job is killed by SLURM.\nFirst, you need to figure out how much time your program needs to gracefully finish. This typically requires 2 durations:\n\nhow long does it take for a single iteration to finish if you have just started a new iteration\nhow long does it take to save the checkpoint\n\nIf, for example, the iteration takes 2 minutes at most and the checkpoint saving another 2 minutes, then you need at least 4 minutes of that grace time. To be safe I‚Äôd at least double it. There is no harm at exiting a bit earlier, as no resources are wasted.\nSo, for example, let‚Äôs say your HPC allows 100 hour jobs, and then your slurm script will say:\n#SBATCH --time=100:00:00\nApproach A. Tell the program at launch time when it should start the exiting process:\nsrun ... torchrun ... --exit-duration-in-mins 5990\n100h is 6000 minutes and so here we give the program 10 mins to gracefully exit.\nAnd when you start the program you create a timer and then before every new iteration starts you check if the time limit is reached. If it is you save the checkpoint and exit.\ncase study: you can see how this was set in the BLOOM training job and then acted upon here:\n        # Exiting based on duration\n        if args.exit_duration_in_mins:\n            train_time = (time.time() - _TRAIN_START_TIME) / 60.0\n            done_cuda = torch.cuda.IntTensor(\n                [train_time &gt; args.exit_duration_in_mins])\n            torch.distributed.all_reduce(\n                done_cuda, op=torch.distributed.ReduceOp.MAX)\n            done = done_cuda.item()\n            if done:\n                if not saved_checkpoint:\n                    save_checkpoint_and_time(iteration, model, optimizer,\n                                             lr_scheduler)\n                print_datetime('exiting program after {} minutes'.format(train_time))\n                sys.exit()\nAs you can see since the training is distributed we have to synchronize the exiting event across all ranks\nYou could also automate the derivation, by retrieving the EndTime for the running job:\n$ scontrol show -d job $SLURM_JOB_ID | grep Time\n   RunTime=00:00:42 TimeLimit=00:11:00 TimeMin=N/A\n   SubmitTime=2023-10-26T15:18:01 EligibleTime=2023-10-26T15:18:01\n   AccrueTime=2023-10-26T15:18:01\n   StartTime=2023-10-26T15:18:01 EndTime=2023-10-26T15:18:43 Deadline=N/A\nand then comparing with the current time in the program and instead setting the graceful exit period. There are other timestamps and durations that can be retrieved as it can be seen from the output.\nApproach B. Sending a signal X minutes before the end\nIn your sbatch script you could set\n#SBATCH --signal=USR1@600\nand then SLURM will send a SIGUSR1 signal to your program 10min before job‚Äôs end time.\nfootnote: normally SLURM schedulers send a SIGTERM signal about 30-60 seconds before the job‚Äôs time is up, and just as the time is up it will send a SIGKILL signal if the job is still running. SIGTERM can be caught and acted upon but 30 seconds is not enough time to gracefully exit a large model training program.\nLet‚Äôs demonstrate how the signal sending and trapping works. In terminal A, run:\npython -c \"\nimport time, os, signal\n\ndef sighandler(signum, frame):\n    print('Signal handler called with signal', signum)\n    exit(0)\n\nsignal.signal(signal.SIGUSR1, sighandler)\nprint(os.getpid())\ntime.sleep(1000)\n\"\nit will print the pid of the process, e.g., 4034989 and will go to sleep (emulating real work). In terminal B now send SIGUSR1 signal to the python program in terminal A with:\nkill -s USR1 4034989\nThe program will trap this signal, call the sighandler which will now print that it was called and exit.\nSignal handler called with signal 10\n10 is the numerical value of SIGUSR1.\nSo here is the same thing with the SLURM setup:\n$ cat sigusr1.slurm\n#SBATCH --job-name=sigusr1\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=0:03:00\n#SBATCH --partition=mypartition\n#SBATCH --output=%x-%j.out\n#SBATCH --signal=USR1@170\n\nsrun python -c \"\nimport time, os, signal\n\ndef sighandler(signum, frame):\n    print('Signal handler called with signal', signum)\n    exit(0)\n\nsignal.signal(signal.SIGUSR1, sighandler)\nprint(os.getpid())\ntime.sleep(1000)\n\"\nIn the SLURM script we told SLURM to send the program a signal 170 seconds before its end and the job itself was set to run for 180 secs (3 mins).\nWhen this job has been scheduled:\nsbatch sigusr1.slurm\n10 seconds (180-170) after the job started, it will exit with the log:\n58307\nSignal handler called with signal 10\nwhich means the job had a pid 58307 and it caught SIGUSR1 (10) and it exited.\nNow that you understand how this machinery works, instead of immediate exit(0) you can set exit-asap flag, finish the currently run iteration, check that the flag is up, save the checkpoint and exit. This is very similar to the code shown in Approach A above.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/training/fault-tolerance/index.html#contributors",
    "href": "qmd/training/fault-tolerance/index.html#contributors",
    "title": "",
    "section": "Contributors",
    "text": "Contributors\nAdam Moody,",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Fault Tolerance"
    ]
  },
  {
    "objectID": "qmd/testing/index.html",
    "href": "qmd/testing/index.html",
    "title": "‚úèÔ∏è Testing",
    "section": "",
    "text": "Note: a part of this document refers to functionality provided by the included testing_utils.py, the bulk of which I have developed while I worked at HuggingFace.\nThis document covers both pytest and unittest functionalities and shows how both can be used together.\n\n\n\n\npytest\nI use the following alias:\nalias pyt=\"pytest --disable-warnings --instafail -rA\"\nwhich tells pytest to:\n\ndisable warning\n--instafail shows failures as they happen, and not at the end\n-rA generates a short test summary info\n\nit requires you to install:\npip install pytest-instafail\n\n\n\nShow all tests in the test suite:\npytest --collect-only -q\nShow all tests in a given test file:\npytest tests/test_optimization.py --collect-only -q\nI use the following alias:\nalias pytc=\"pytest --disable-warnings --collect-only -q\"\n\n\n\nTo run an individual test module:\npytest tests/utils/test_logging.py\n\n\n\nIf unittest is used, to run specific subtests you need to know the name of the unittest class containing those tests. For example, it could be:\npytest tests/test_optimization.py::OptimizationTest::test_adam_w\nHere:\n\ntests/test_optimization.py - the file with tests\nOptimizationTest - the name of the test class\ntest_adam_w - the name of the specific test function\n\nIf the file contains multiple classes, you can choose to run only tests of a given class. For example:\npytest tests/test_optimization.py::OptimizationTest\nwill run all the tests inside that class.\nAs mentioned earlier you can see what tests are contained inside the OptimizationTest class by running:\npytest tests/test_optimization.py::OptimizationTest --collect-only -q\nYou can run tests by keyword expressions.\nTo run only tests whose name contains adam:\npytest -k adam tests/test_optimization.py\nLogical and and or can be used to indicate whether all keywords should match or either. not can be used to negate.\nTo run all tests except those whose name contains adam:\npytest -k \"not adam\" tests/test_optimization.py\nAnd you can combine the two patterns in one:\npytest -k \"ada and not adam\" tests/test_optimization.py\nFor example to run both test_adafactor and test_adam_w you can use:\npytest -k \"test_adam_w or test_adam_w\" tests/test_optimization.py\nNote that we use or here, since we want either of the keywords to match to include both.\nIf you want to include only tests that include both patterns, and is to be used:\npytest -k \"test and ada\" tests/test_optimization.py\n\n\n\nYou can run the tests related to the unstaged files or the current branch (according to Git) by using pytest-picked. This is a great way of quickly testing your changes didn‚Äôt break anything, since it won‚Äôt run the tests related to files you didn‚Äôt touch.\npip install pytest-picked\npytest --picked\nAll tests will be run from files and folders which are modified, but not yet committed.\n\n\n\npytest-xdist provides a very useful feature of detecting all failed tests, and then waiting for you to modify files and continuously re-rerun those failing tests until they pass while you fix them. So that you don‚Äôt need to re start pytest after you made the fix. This is repeated until all tests pass after which again a full run is performed.\npip install pytest-xdist\nTo enter the mode: pytest -f or pytest --looponfail\nFile changes are detected by looking at looponfailroots root directories and all of their contents (recursively). If the default for this value does not work for you, you can change it in your project by setting a configuration option in setup.cfg:\n[tool:pytest]\nlooponfailroots = transformers tests\nor pytest.ini/tox.ini files:\n[pytest]\nlooponfailroots = transformers tests\nThis would lead to only looking for file changes in the respective directories, specified relatively to the ini-file‚Äôs directory.\npytest-watch is an alternative implementation of this functionality.\n\n\n\nIf you want to run all test modules, except a few you can exclude them by giving an explicit list of tests to run. For example, to run all except test_modeling_*.py tests:\npytest $(ls -1 tests/*py | grep -v test_modeling)\n\n\n\nCI builds and when isolation is important (against speed), cache should be cleared:\npytest --cache-clear tests\n\n\n\nAs mentioned earlier make test runs tests in parallel via pytest-xdist plugin (-n X argument, e.g.¬†-n 2 to run 2 parallel jobs).\npytest-xdist‚Äôs --dist= option allows one to control how the tests are grouped. --dist=loadfile puts the tests located in one file onto the same process.\nSince the order of executed tests is different and unpredictable, if running the test suite with pytest-xdist produces failures (meaning we have some undetected coupled tests), use pytest-replay to replay the tests in the same order, which should help with then somehow reducing that failing sequence to a minimum.\n\n\n\nIt‚Äôs good to repeat the tests several times, in sequence, randomly, or in sets, to detect any potential inter-dependency and state-related bugs (tear down). And the straightforward multiple repetition is just good to detect some problems that get uncovered by randomness of DL.\n\n\n\npytest-flakefinder:\n\npip install pytest-flakefinder\nAnd then run every test multiple times (50 by default):\npytest --flake-finder --flake-runs=5 tests/test_failing_test.py\nfootnote: This plugin doesn‚Äôt work with -n flag from pytest-xdist.\nfootnote: There is another plugin pytest-repeat, but it doesn‚Äôt work with unittest.\n\n\n\npip install pytest-random-order\nImportant: the presence of pytest-random-order will automatically randomize tests, no configuration change or command line options is required.\nAs explained earlier this allows detection of coupled tests - where one test‚Äôs state affects the state of another. When pytest-random-order is installed it will print the random seed it used for that session, e.g:\npytest tests\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\nSo that if the given particular sequence fails, you can reproduce it by adding that exact seed, e.g.:\npytest --random-order-seed=573663\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\nIt will only reproduce the exact order if you use the exact same list of tests (or no list at all). Once you start to manually narrowing down the list you can no longer rely on the seed, but have to list them manually in the exact order they failed and tell pytest to not randomize them instead using --random-order-bucket=none, e.g.:\npytest --random-order-bucket=none tests/test_a.py tests/test_c.py tests/test_b.py\nTo disable the shuffling for all tests:\npytest --random-order-bucket=none\nBy default --random-order-bucket=module is implied, which will shuffle the files on the module levels. It can also shuffle on class, package, global and none levels. For the complete details please see its documentation.\nAnother randomization alternative is: pytest-randomly. This module has a very similar functionality/interface, but it doesn‚Äôt have the bucket modes available in pytest-random-order. It has the same problem of imposing itself once installed.\n\n\n\n\n\n\npytest-sugar is a plugin that improves the look-n-feel, adds a progressbar, and show tests that fail and the assert instantly. It gets activated automatically upon installation.\npip install pytest-sugar\nTo run tests without it, run:\npytest -p no:sugar\nor uninstall it.\n\n\n\nFor a single or a group of tests via pytest (after pip install pytest-pspec):\npytest --pspec tests/test_optimization.py\n\n\n\npytest-instafail shows failures and errors instantly instead of waiting until the end of test session.\npip install pytest-instafail\npytest --instafail\n\n\n\n\nOn a GPU-enabled setup, to test in CPU-only mode add CUDA_VISIBLE_DEVICES=\"\":\nCUDA_VISIBLE_DEVICES=\"\" pytest tests/utils/test_logging.py\nor if you have multiple gpus, you can specify which one is to be used by pytest. For example, to use only the second gpu if you have gpus 0 and 1, you can run:\nCUDA_VISIBLE_DEVICES=\"1\" pytest tests/utils/test_logging.py\nThis is handy when you want to run different tasks on different GPUs.\nSome tests must be run on CPU-only, others on either CPU or GPU or TPU, yet others on multiple-GPUs. The following skip decorators are used to set the requirements of tests CPU/GPU/TPU-wise:\n\nrequire_torch - this test will run only under torch\nrequire_torch_gpu - as require_torch plus requires at least 1 GPU\nrequire_torch_multi_gpu - as require_torch plus requires at least 2 GPUs\nrequire_torch_non_multi_gpu - as require_torch plus requires 0 or 1 GPUs\nrequire_torch_up_to_2_gpus - as require_torch plus requires 0 or 1 or 2 GPUs\nrequire_torch_tpu - as require_torch plus requires at least 1 TPU\n\nLet‚Äôs depict the GPU requirements in the following table:\n\n\n\nn gpus\ndecorator\n\n\n\n\n&gt;= 0\n@require_torch\n\n\n&gt;= 1\n@require_torch_gpu\n\n\n&gt;= 2\n@require_torch_multi_gpu\n\n\n&lt; 2\n@require_torch_non_multi_gpu\n\n\n&lt; 3\n@require_torch_up_to_2_gpus\n\n\n\nFor example, here is a test that must be run only when there are 2 or more GPUs available and pytorch is installed:\n```python no-style from testing_utils import require_torch_multi_gpu\n(require_torch_multi_gpu?) def test_example_with_multi_gpu():\n\nThese decorators can be stacked:\n\n```python no-style\nfrom testing_utils import require_torch_gpu\n\n@require_torch_gpu\n@some_other_decorator\ndef test_example_slow_on_gpu():\nSome decorators like @parametrized rewrite test names, therefore @require_* skip decorators have to be listed last for them to work correctly. Here is an example of the correct usage:\n```python no-style from testing_utils import require_torch_multi_gpu from parameterized import parameterized\n(parameterized.expand?)(‚Ä¶) (require_torch_multi_gpu?) def test_integration_foo():\n\nThis order problem doesn't exist with `@pytest.mark.parametrize`, you can put it first or last and it will still work. But it only works with non-unittests.\n\nInside tests:\n\n- How many GPUs are available:\n\n```python\nfrom testing_utils import get_gpu_count\n\nn_gpu = get_gpu_count()\n\n\n\npytest can‚Äôt deal with distributed training directly. If this is attempted - the sub-processes don‚Äôt do the right thing and end up thinking they are pytest and start running the test suite in loops. It works, however, if one spawns a normal process that then spawns off multiple workers and manages the IO pipes.\nHere are some tests that use it:\n\ntest_trainer_distributed.py\ntest_deepspeed.py\n\nTo jump right into the execution point, search for the execute_subprocess_async call in those tests, which you will find inside testing_utils.py.\nYou will need at least 2 GPUs to see these tests in action:\nCUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py\n(RUN_SLOW is a special decorator used by HF Transformers to normally skip heavy tests)\n\n\n\nDuring test execution any output sent to stdout and stderr is captured. If a test or a setup method fails, its according captured output will usually be shown along with the failure traceback.\nTo disable output capturing and to get the stdout and stderr normally, use -s or --capture=no:\npytest -s tests/utils/test_logging.py\nTo send test results to JUnit format output:\npy.test tests --junitxml=result.xml\n\n\n\nTo have no color (e.g., yellow on white background is not readable):\npytest --color=no tests/utils/test_logging.py\n\n\n\nCreating a URL for each test failure:\npytest --pastebin=failed tests/utils/test_logging.py\nThis will submit test run information to a remote Paste service and provide a URL for each failure. You may select tests as usual or add for example -x if you only want to send one particular failure.\nCreating a URL for a whole test session log:\npytest --pastebin=all tests/utils/test_logging.py\n\n\n\n\nMost of the time if combining pytest and unittest in the same test suite works just fine. You can read here which features are supported when doing that , but the important thing to remember is that most pytest fixtures don‚Äôt work. Neither parametrization, but we use the module parameterized that works in a similar way.\n\n\nOften, there is a need to run the same test multiple times, but with different arguments. It could be done from within the test, but then there is no way of running that test for just one set of arguments.\n# test_this1.py\nimport unittest\nfrom parameterized import parameterized\n\n\nclass TestMathUnitTest(unittest.TestCase):\n    @parameterized.expand(\n        [\n            (\"negative\", -1.5, -2.0),\n            (\"integer\", 1, 1.0),\n            (\"large fraction\", 1.6, 1),\n        ]\n    )\n    def test_floor(self, name, input, expected):\n        assert_equal(math.floor(input), expected)\nNow, by default this test will be run 3 times, each time with the last 3 arguments of test_floor being assigned the corresponding arguments in the parameter list.\nAnd you could run just the negative and integer sets of params with:\npytest -k \"negative and integer\" tests/test_mytest.py\nor all but negative sub-tests, with:\npytest -k \"not negative\" tests/test_mytest.py\nBesides using the -k filter that was just mentioned, you can find out the exact name of each sub-test and run any or all of them using their exact names.\npytest test_this1.py --collect-only -q\nand it will list:\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\nSo now you can run just 2 specific sub-tests:\npytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer\nThe module parameterized works for both: unittests and pytest tests.\nIf, however, the test is not a unittest, you may use pytest.mark.parametrize.\nHere is the same example, this time using pytest‚Äôs parametrize marker:\n# test_this2.py\nimport pytest\n\n\n@pytest.mark.parametrize(\n    \"name, input, expected\",\n    [\n        (\"negative\", -1.5, -2.0),\n        (\"integer\", 1, 1.0),\n        (\"large fraction\", 1.6, 1),\n    ],\n)\ndef test_floor(name, input, expected):\n    assert_equal(math.floor(input), expected)\nSame as with parameterized, with pytest.mark.parametrize you can have a fine control over which sub-tests are run, if the -k filter doesn‚Äôt do the job. Except, this parametrization function creates a slightly different set of names for the sub-tests. Here is what they look like:\npytest test_this2.py --collect-only -q\nand it will list:\ntest_this2.py::test_floor[integer-1-1.0]\ntest_this2.py::test_floor[negative--1.5--2.0]\ntest_this2.py::test_floor[large fraction-1.6-1]\nSo now you can run just the specific test:\npytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0]\nas in the previous example.\n\n\n\nIn tests often we need to know where things are relative to the current test file, and it‚Äôs not trivial since the test could be invoked from more than one directory or could reside in sub-directories with different depths. A helper class testing_utils.TestCasePlus solves this problem by sorting out all the basic paths and provides easy accessors to them:\n\npathlib objects (all fully resolved):\n\ntest_file_path - the current test file path, i.e.¬†__file__\ntest_file_dir - the directory containing the current test file\ntests_dir - the directory of the tests test suite\nexamples_dir - the directory of the examples test suite\nrepo_root_dir - the directory of the repository\nsrc_dir - the directory of src (i.e.¬†where the transformers sub-dir resides)\n\nstringified paths ‚Äì same as above but these return paths as strings, rather than pathlib objects:\n\ntest_file_path_str\ntest_file_dir_str\ntests_dir_str\nexamples_dir_str\nrepo_root_dir_str\nsrc_dir_str\n\n\nTo start using those all you need is to make sure that the test resides in a subclass of testing_utils.TestCasePlus. For example:\nfrom testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_local_locations(self):\n        data_dir = self.tests_dir / \"fixtures/tests_samples/wmt_en_ro\"\nIf you don‚Äôt need to manipulate paths via pathlib or you just need a path as a string, you can always invoked str() on the pathlib object or use the accessors ending with _str. For example:\nfrom testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_stringified_locations(self):\n        examples_dir = self.examples_dir_str\n\n\nUsing unique temporary files and directories are essential for parallel test running, so that the tests won‚Äôt overwrite each other‚Äôs data. Also we want to get the temporary files and directories removed at the end of each test that created them. Therefore, using packages like tempfile, which address these needs is essential.\nHowever, when debugging tests, you need to be able to see what goes into the temporary file or directory and you want to know it‚Äôs exact path and not having it randomized on every test re-run.\nA helper class testing_utils.TestCasePlus is best used for such purposes. It‚Äôs a sub-class of unittest.TestCase, so we can easily inherit from it in the test modules.\nHere is an example of its usage:\nfrom testing_utils import TestCasePlus\n\n\nclass ExamplesTests(TestCasePlus):\n    def test_whatever(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\nThis code creates a unique temporary directory, and sets tmp_dir to its location.\n\nCreate a unique temporary dir:\n\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir()\ntmp_dir will contain the path to the created temporary dir. It will be automatically removed at the end of the test.\n\nCreate a temporary dir of my choice, ensure it‚Äôs empty before the test starts and don‚Äôt empty it after the test.\n\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir(\"./xxx\")\nThis is useful for debug when you want to monitor a specific directory and want to make sure the previous tests didn‚Äôt leave any data in there.\n\nYou can override the default behavior by directly overriding the before and after args, leading to one of the following behaviors:\n\nbefore=True: the temporary dir will always be cleared at the beginning of the test.\nbefore=False: if the temporary dir already existed, any existing files will remain there.\nafter=True: the temporary dir will always be deleted at the end of the test.\nafter=False: the temporary dir will always be left intact at the end of the test.\n\n\nfootnote: In order to run the equivalent of rm -r safely, only subdirs of the project repository checkout are allowed if an explicit tmp_dir is used, so that by mistake no /tmp or similar important part of the filesystem will get nuked. i.e.¬†please always pass paths that start with ./.\nfootnote: Each test can register multiple temporary directories and they all will get auto-removed, unless requested otherwise.\n\n\n\nIf you need to temporary override sys.path to import from another test for example, you can use the ExtendSysPath context manager. Example:\nimport os\nfrom testing_utils import ExtendSysPath\n\nbindir = os.path.abspath(os.path.dirname(__file__))\nwith ExtendSysPath(f\"{bindir}/..\"):\n    from test_trainer import TrainerIntegrationCommon  # noqa\n\n\n\n\nThis is useful when a bug is found and a new test is written, yet the bug is not fixed yet. In order to be able to commit it to the main repository we need make sure it‚Äôs skipped during make test.\nMethods:\n\nA skip means that you expect your test to pass only if some conditions are met, otherwise pytest should skip running the test altogether. Common examples are skipping windows-only tests on non-windows platforms, or skipping tests that depend on an external resource which is not available at the moment (for example a database).\nA xfail means that you expect a test to fail for some reason. A common example is a test for a feature not yet implemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with pytest.mark.xfail), it‚Äôs an xpass and will be reported in the test summary.\n\nOne of the important differences between the two is that skip doesn‚Äôt run the test, and xfail does. So if the code that‚Äôs buggy causes some bad state that will affect other tests, do not use xfail.\n\n\n\nHere is how to skip whole test unconditionally:\n\npython no-style @unittest.skip(\"this bug needs to be fixed\") def test_feature_x():\nor via pytest:\npython no-style @pytest.mark.skip(reason=\"this bug needs to be fixed\")\nor the xfail way:\npython no-style @pytest.mark.xfail def test_feature_x():\nHere‚Äôs how to skip a test based on internal checks within the test:\ndef test_feature_x():\n    if not has_something():\n        pytest.skip(\"unsupported configuration\")\nor the whole module:\nimport pytest\n\nif not pytest.config.getoption(\"--custom-flag\"):\n    pytest.skip(\"--custom-flag is missing, skipping tests\", allow_module_level=True)\nor the xfail way:\ndef test_feature_x():\n    pytest.xfail(\"expected to fail until bug XYZ is fixed\")\n\nHere is how to skip all tests in a module if some import is missing:\n\ndocutils = pytest.importorskip(\"docutils\", minversion=\"0.3\")\n\nSkip a test based on a condition:\n\npython no-style @pytest.mark.skipif(sys.version_info &lt; (3,6), reason=\"requires python3.6 or higher\") def test_feature_x():\nor:\npython no-style @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\") def test_feature_x():\nor skip the whole module:\npython no-style @pytest.mark.skipif(sys.platform == 'win32', reason=\"does not run on windows\") class TestClass():     def test_feature_x(self):\nMore details, example and ways are here.\n\n\n\n\n\n\nIn order to test functions that write to stdout and/or stderr, the test can access those streams using the pytest‚Äôs capsys system. Here is how this is accomplished:\nimport sys\n\n\ndef print_to_stdout(s):\n    print(s)\n\n\ndef print_to_stderr(s):\n    sys.stderr.write(s)\n\n\ndef test_result_and_stdout(capsys):\n    msg = \"Hello\"\n    print_to_stdout(msg)\n    print_to_stderr(msg)\n    out, err = capsys.readouterr()  # consume the captured output streams\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    sys.stderr.write(err)\n    # test:\n    assert msg in out\n    assert msg in err\nAnd, of course, most of the time, stderr will come as a part of an exception, so try/except has to be used in such a case:\ndef raise_exception(msg):\n    raise ValueError(msg)\n\n\ndef test_something_exception():\n    msg = \"Not a good value\"\n    error = \"\"\n    try:\n        raise_exception(msg)\n    except Exception as e:\n        error = str(e)\n        assert msg in error, f\"{msg} is in the exception:\\n{error}\"\nAnother approach to capturing stdout is via contextlib.redirect_stdout:\nfrom io import StringIO\nfrom contextlib import redirect_stdout\n\n\ndef print_to_stdout(s):\n    print(s)\n\n\ndef test_result_and_stdout():\n    msg = \"Hello\"\n    buffer = StringIO()\n    with redirect_stdout(buffer):\n        print_to_stdout(msg)\n    out = buffer.getvalue()\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    # test:\n    assert msg in out\nAn important potential issue with capturing stdout is that it may contain \\r characters that in normal print reset everything that has been printed so far. There is no problem with pytest, but with pytest -s these characters get included in the buffer, so to be able to have the test run with and without -s, you have to make an extra cleanup to the captured output, using re.sub(r'~.*\\r', '', buf, 0, re.M).\nBut, then we have a helper context manager wrapper to automatically take care of it all, regardless of whether it has some \\r‚Äôs in it or not, so it‚Äôs a simple:\nfrom testing_utils import CaptureStdout\n\nwith CaptureStdout() as cs:\n    function_that_writes_to_stdout()\nprint(cs.out)\nHere is a full test example:\nfrom testing_utils import CaptureStdout\n\nmsg = \"Secret message\\r\"\nfinal = \"Hello World\"\nwith CaptureStdout() as cs:\n    print(msg + final)\nassert cs.out == final + \"\\n\", f\"captured: {cs.out}, expecting {final}\"\nIf you‚Äôd like to capture stderr use the CaptureStderr class instead:\nfrom testing_utils import CaptureStderr\n\nwith CaptureStderr() as cs:\n    function_that_writes_to_stderr()\nprint(cs.err)\nIf you need to capture both streams at once, use the parent CaptureStd class:\nfrom testing_utils import CaptureStd\n\nwith CaptureStd() as cs:\n    function_that_writes_to_stdout_and_stderr()\nprint(cs.err, cs.out)\nAlso, to aid debugging test issues, by default these context managers automatically replay the captured streams on exit from the context.\n\n\n\nIf you need to validate the output of a logger, you can use CaptureLogger:\nfrom transformers import logging\nfrom testing_utils import CaptureLogger\n\nmsg = \"Testing 1, 2, 3\"\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")\nwith CaptureLogger(logger) as cl:\n    logger.info(msg)\nassert cl.out, msg + \"\\n\"\n\n\n\n\nIf you want to test the impact of environment variables for a specific test you can use a helper decorator transformers.testing_utils.mockenv\nfrom testing_utils import mockenv\n\n\nclass HfArgumentParserTest(unittest.TestCase):\n    @mockenv(TRANSFORMERS_VERBOSITY=\"error\")\n    def test_env_override(self):\n        env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\nAt times an external program needs to be called, which requires setting PYTHONPATH in os.environ to include multiple local paths. A helper class testing_utils.TestCasePlus comes to help:\nfrom testing_utils import TestCasePlus\n\n\nclass EnvExampleTest(TestCasePlus):\n    def test_external_prog(self):\n        env = self.get_env()\n        # now call the external program, passing `env` to it\nDepending on whether the test file was under the tests test suite or examples it‚Äôll correctly set up env[PYTHONPATH] to include one of these two directories, and also the src directory to ensure the testing is done against the current repo, and finally with whatever env[PYTHONPATH] was already set to before the test was called if anything.\nThis helper method creates a copy of the os.environ object, so the original remains intact.\n\n\n\nIn some situations you may want to remove randomness for your tests. To get identical reproducible results set, you will need to fix the seed:\nseed = 42\n\n# python RNG\nimport random\n\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\n\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\n\nnp.random.seed(seed)\n\n# tf RNG\ntf.random.set_seed(seed)\n\n\n\n\nTo start a debugger at the point of the warning, do this:\npytest tests/utils/test_logging.py -W error::UserWarning --pdb\n\n\n\nHere is a massive pytest patching that I have done many years ago to aid with understanding CI reports better.\nTo activate it add to tests/conftest.py (or create it if you haven‚Äôt already):\nimport pytest\n\ndef pytest_addoption(parser):\n    from testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\nand then when you run the test suite, add --make-reports=mytests like so:\npytest --make-reports=mytests tests\nand it‚Äôll create 8 separate reports:\n$ ls -1 reports/mytests/\ndurations.txt\nerrors.txt\nfailures_line.txt\nfailures_long.txt\nfailures_short.txt\nstats.txt\nsummary_short.txt\nwarnings.txt\nso now instead of having only a single output from pytest with everything together, you can now have each type of report saved into each own file.\nThis feature is most useful on CI, which makes it much easier to both introspect problems and also view and download individual reports.\nUsing a different value to --make-reports= for different groups of tests can have each group saved separately rather than clobbering each other.\nAll this functionality was already inside pytest but there was no way to extract it easily so I added the monkey-patching overrides testing_utils.py. Well, I did ask if I can contribute this as a feature to pytest but my proposal wasn‚Äôt welcome.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "‚úèÔ∏è  Testing"
    ]
  },
  {
    "objectID": "qmd/testing/index.html#running-tests",
    "href": "qmd/testing/index.html#running-tests",
    "title": "‚úèÔ∏è Testing",
    "section": "",
    "text": "pytest\nI use the following alias:\nalias pyt=\"pytest --disable-warnings --instafail -rA\"\nwhich tells pytest to:\n\ndisable warning\n--instafail shows failures as they happen, and not at the end\n-rA generates a short test summary info\n\nit requires you to install:\npip install pytest-instafail\n\n\n\nShow all tests in the test suite:\npytest --collect-only -q\nShow all tests in a given test file:\npytest tests/test_optimization.py --collect-only -q\nI use the following alias:\nalias pytc=\"pytest --disable-warnings --collect-only -q\"\n\n\n\nTo run an individual test module:\npytest tests/utils/test_logging.py\n\n\n\nIf unittest is used, to run specific subtests you need to know the name of the unittest class containing those tests. For example, it could be:\npytest tests/test_optimization.py::OptimizationTest::test_adam_w\nHere:\n\ntests/test_optimization.py - the file with tests\nOptimizationTest - the name of the test class\ntest_adam_w - the name of the specific test function\n\nIf the file contains multiple classes, you can choose to run only tests of a given class. For example:\npytest tests/test_optimization.py::OptimizationTest\nwill run all the tests inside that class.\nAs mentioned earlier you can see what tests are contained inside the OptimizationTest class by running:\npytest tests/test_optimization.py::OptimizationTest --collect-only -q\nYou can run tests by keyword expressions.\nTo run only tests whose name contains adam:\npytest -k adam tests/test_optimization.py\nLogical and and or can be used to indicate whether all keywords should match or either. not can be used to negate.\nTo run all tests except those whose name contains adam:\npytest -k \"not adam\" tests/test_optimization.py\nAnd you can combine the two patterns in one:\npytest -k \"ada and not adam\" tests/test_optimization.py\nFor example to run both test_adafactor and test_adam_w you can use:\npytest -k \"test_adam_w or test_adam_w\" tests/test_optimization.py\nNote that we use or here, since we want either of the keywords to match to include both.\nIf you want to include only tests that include both patterns, and is to be used:\npytest -k \"test and ada\" tests/test_optimization.py\n\n\n\nYou can run the tests related to the unstaged files or the current branch (according to Git) by using pytest-picked. This is a great way of quickly testing your changes didn‚Äôt break anything, since it won‚Äôt run the tests related to files you didn‚Äôt touch.\npip install pytest-picked\npytest --picked\nAll tests will be run from files and folders which are modified, but not yet committed.\n\n\n\npytest-xdist provides a very useful feature of detecting all failed tests, and then waiting for you to modify files and continuously re-rerun those failing tests until they pass while you fix them. So that you don‚Äôt need to re start pytest after you made the fix. This is repeated until all tests pass after which again a full run is performed.\npip install pytest-xdist\nTo enter the mode: pytest -f or pytest --looponfail\nFile changes are detected by looking at looponfailroots root directories and all of their contents (recursively). If the default for this value does not work for you, you can change it in your project by setting a configuration option in setup.cfg:\n[tool:pytest]\nlooponfailroots = transformers tests\nor pytest.ini/tox.ini files:\n[pytest]\nlooponfailroots = transformers tests\nThis would lead to only looking for file changes in the respective directories, specified relatively to the ini-file‚Äôs directory.\npytest-watch is an alternative implementation of this functionality.\n\n\n\nIf you want to run all test modules, except a few you can exclude them by giving an explicit list of tests to run. For example, to run all except test_modeling_*.py tests:\npytest $(ls -1 tests/*py | grep -v test_modeling)\n\n\n\nCI builds and when isolation is important (against speed), cache should be cleared:\npytest --cache-clear tests\n\n\n\nAs mentioned earlier make test runs tests in parallel via pytest-xdist plugin (-n X argument, e.g.¬†-n 2 to run 2 parallel jobs).\npytest-xdist‚Äôs --dist= option allows one to control how the tests are grouped. --dist=loadfile puts the tests located in one file onto the same process.\nSince the order of executed tests is different and unpredictable, if running the test suite with pytest-xdist produces failures (meaning we have some undetected coupled tests), use pytest-replay to replay the tests in the same order, which should help with then somehow reducing that failing sequence to a minimum.\n\n\n\nIt‚Äôs good to repeat the tests several times, in sequence, randomly, or in sets, to detect any potential inter-dependency and state-related bugs (tear down). And the straightforward multiple repetition is just good to detect some problems that get uncovered by randomness of DL.\n\n\n\npytest-flakefinder:\n\npip install pytest-flakefinder\nAnd then run every test multiple times (50 by default):\npytest --flake-finder --flake-runs=5 tests/test_failing_test.py\nfootnote: This plugin doesn‚Äôt work with -n flag from pytest-xdist.\nfootnote: There is another plugin pytest-repeat, but it doesn‚Äôt work with unittest.\n\n\n\npip install pytest-random-order\nImportant: the presence of pytest-random-order will automatically randomize tests, no configuration change or command line options is required.\nAs explained earlier this allows detection of coupled tests - where one test‚Äôs state affects the state of another. When pytest-random-order is installed it will print the random seed it used for that session, e.g:\npytest tests\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\nSo that if the given particular sequence fails, you can reproduce it by adding that exact seed, e.g.:\npytest --random-order-seed=573663\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\nIt will only reproduce the exact order if you use the exact same list of tests (or no list at all). Once you start to manually narrowing down the list you can no longer rely on the seed, but have to list them manually in the exact order they failed and tell pytest to not randomize them instead using --random-order-bucket=none, e.g.:\npytest --random-order-bucket=none tests/test_a.py tests/test_c.py tests/test_b.py\nTo disable the shuffling for all tests:\npytest --random-order-bucket=none\nBy default --random-order-bucket=module is implied, which will shuffle the files on the module levels. It can also shuffle on class, package, global and none levels. For the complete details please see its documentation.\nAnother randomization alternative is: pytest-randomly. This module has a very similar functionality/interface, but it doesn‚Äôt have the bucket modes available in pytest-random-order. It has the same problem of imposing itself once installed.\n\n\n\n\n\n\npytest-sugar is a plugin that improves the look-n-feel, adds a progressbar, and show tests that fail and the assert instantly. It gets activated automatically upon installation.\npip install pytest-sugar\nTo run tests without it, run:\npytest -p no:sugar\nor uninstall it.\n\n\n\nFor a single or a group of tests via pytest (after pip install pytest-pspec):\npytest --pspec tests/test_optimization.py\n\n\n\npytest-instafail shows failures and errors instantly instead of waiting until the end of test session.\npip install pytest-instafail\npytest --instafail\n\n\n\n\nOn a GPU-enabled setup, to test in CPU-only mode add CUDA_VISIBLE_DEVICES=\"\":\nCUDA_VISIBLE_DEVICES=\"\" pytest tests/utils/test_logging.py\nor if you have multiple gpus, you can specify which one is to be used by pytest. For example, to use only the second gpu if you have gpus 0 and 1, you can run:\nCUDA_VISIBLE_DEVICES=\"1\" pytest tests/utils/test_logging.py\nThis is handy when you want to run different tasks on different GPUs.\nSome tests must be run on CPU-only, others on either CPU or GPU or TPU, yet others on multiple-GPUs. The following skip decorators are used to set the requirements of tests CPU/GPU/TPU-wise:\n\nrequire_torch - this test will run only under torch\nrequire_torch_gpu - as require_torch plus requires at least 1 GPU\nrequire_torch_multi_gpu - as require_torch plus requires at least 2 GPUs\nrequire_torch_non_multi_gpu - as require_torch plus requires 0 or 1 GPUs\nrequire_torch_up_to_2_gpus - as require_torch plus requires 0 or 1 or 2 GPUs\nrequire_torch_tpu - as require_torch plus requires at least 1 TPU\n\nLet‚Äôs depict the GPU requirements in the following table:\n\n\n\nn gpus\ndecorator\n\n\n\n\n&gt;= 0\n@require_torch\n\n\n&gt;= 1\n@require_torch_gpu\n\n\n&gt;= 2\n@require_torch_multi_gpu\n\n\n&lt; 2\n@require_torch_non_multi_gpu\n\n\n&lt; 3\n@require_torch_up_to_2_gpus\n\n\n\nFor example, here is a test that must be run only when there are 2 or more GPUs available and pytorch is installed:\n```python no-style from testing_utils import require_torch_multi_gpu\n(require_torch_multi_gpu?) def test_example_with_multi_gpu():\n\nThese decorators can be stacked:\n\n```python no-style\nfrom testing_utils import require_torch_gpu\n\n@require_torch_gpu\n@some_other_decorator\ndef test_example_slow_on_gpu():\nSome decorators like @parametrized rewrite test names, therefore @require_* skip decorators have to be listed last for them to work correctly. Here is an example of the correct usage:\n```python no-style from testing_utils import require_torch_multi_gpu from parameterized import parameterized\n(parameterized.expand?)(‚Ä¶) (require_torch_multi_gpu?) def test_integration_foo():\n\nThis order problem doesn't exist with `@pytest.mark.parametrize`, you can put it first or last and it will still work. But it only works with non-unittests.\n\nInside tests:\n\n- How many GPUs are available:\n\n```python\nfrom testing_utils import get_gpu_count\n\nn_gpu = get_gpu_count()\n\n\n\npytest can‚Äôt deal with distributed training directly. If this is attempted - the sub-processes don‚Äôt do the right thing and end up thinking they are pytest and start running the test suite in loops. It works, however, if one spawns a normal process that then spawns off multiple workers and manages the IO pipes.\nHere are some tests that use it:\n\ntest_trainer_distributed.py\ntest_deepspeed.py\n\nTo jump right into the execution point, search for the execute_subprocess_async call in those tests, which you will find inside testing_utils.py.\nYou will need at least 2 GPUs to see these tests in action:\nCUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py\n(RUN_SLOW is a special decorator used by HF Transformers to normally skip heavy tests)\n\n\n\nDuring test execution any output sent to stdout and stderr is captured. If a test or a setup method fails, its according captured output will usually be shown along with the failure traceback.\nTo disable output capturing and to get the stdout and stderr normally, use -s or --capture=no:\npytest -s tests/utils/test_logging.py\nTo send test results to JUnit format output:\npy.test tests --junitxml=result.xml\n\n\n\nTo have no color (e.g., yellow on white background is not readable):\npytest --color=no tests/utils/test_logging.py\n\n\n\nCreating a URL for each test failure:\npytest --pastebin=failed tests/utils/test_logging.py\nThis will submit test run information to a remote Paste service and provide a URL for each failure. You may select tests as usual or add for example -x if you only want to send one particular failure.\nCreating a URL for a whole test session log:\npytest --pastebin=all tests/utils/test_logging.py",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "‚úèÔ∏è  Testing"
    ]
  },
  {
    "objectID": "qmd/testing/index.html#writing-tests",
    "href": "qmd/testing/index.html#writing-tests",
    "title": "‚úèÔ∏è Testing",
    "section": "",
    "text": "Most of the time if combining pytest and unittest in the same test suite works just fine. You can read here which features are supported when doing that , but the important thing to remember is that most pytest fixtures don‚Äôt work. Neither parametrization, but we use the module parameterized that works in a similar way.\n\n\nOften, there is a need to run the same test multiple times, but with different arguments. It could be done from within the test, but then there is no way of running that test for just one set of arguments.\n# test_this1.py\nimport unittest\nfrom parameterized import parameterized\n\n\nclass TestMathUnitTest(unittest.TestCase):\n    @parameterized.expand(\n        [\n            (\"negative\", -1.5, -2.0),\n            (\"integer\", 1, 1.0),\n            (\"large fraction\", 1.6, 1),\n        ]\n    )\n    def test_floor(self, name, input, expected):\n        assert_equal(math.floor(input), expected)\nNow, by default this test will be run 3 times, each time with the last 3 arguments of test_floor being assigned the corresponding arguments in the parameter list.\nAnd you could run just the negative and integer sets of params with:\npytest -k \"negative and integer\" tests/test_mytest.py\nor all but negative sub-tests, with:\npytest -k \"not negative\" tests/test_mytest.py\nBesides using the -k filter that was just mentioned, you can find out the exact name of each sub-test and run any or all of them using their exact names.\npytest test_this1.py --collect-only -q\nand it will list:\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\nSo now you can run just 2 specific sub-tests:\npytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer\nThe module parameterized works for both: unittests and pytest tests.\nIf, however, the test is not a unittest, you may use pytest.mark.parametrize.\nHere is the same example, this time using pytest‚Äôs parametrize marker:\n# test_this2.py\nimport pytest\n\n\n@pytest.mark.parametrize(\n    \"name, input, expected\",\n    [\n        (\"negative\", -1.5, -2.0),\n        (\"integer\", 1, 1.0),\n        (\"large fraction\", 1.6, 1),\n    ],\n)\ndef test_floor(name, input, expected):\n    assert_equal(math.floor(input), expected)\nSame as with parameterized, with pytest.mark.parametrize you can have a fine control over which sub-tests are run, if the -k filter doesn‚Äôt do the job. Except, this parametrization function creates a slightly different set of names for the sub-tests. Here is what they look like:\npytest test_this2.py --collect-only -q\nand it will list:\ntest_this2.py::test_floor[integer-1-1.0]\ntest_this2.py::test_floor[negative--1.5--2.0]\ntest_this2.py::test_floor[large fraction-1.6-1]\nSo now you can run just the specific test:\npytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0]\nas in the previous example.\n\n\n\nIn tests often we need to know where things are relative to the current test file, and it‚Äôs not trivial since the test could be invoked from more than one directory or could reside in sub-directories with different depths. A helper class testing_utils.TestCasePlus solves this problem by sorting out all the basic paths and provides easy accessors to them:\n\npathlib objects (all fully resolved):\n\ntest_file_path - the current test file path, i.e.¬†__file__\ntest_file_dir - the directory containing the current test file\ntests_dir - the directory of the tests test suite\nexamples_dir - the directory of the examples test suite\nrepo_root_dir - the directory of the repository\nsrc_dir - the directory of src (i.e.¬†where the transformers sub-dir resides)\n\nstringified paths ‚Äì same as above but these return paths as strings, rather than pathlib objects:\n\ntest_file_path_str\ntest_file_dir_str\ntests_dir_str\nexamples_dir_str\nrepo_root_dir_str\nsrc_dir_str\n\n\nTo start using those all you need is to make sure that the test resides in a subclass of testing_utils.TestCasePlus. For example:\nfrom testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_local_locations(self):\n        data_dir = self.tests_dir / \"fixtures/tests_samples/wmt_en_ro\"\nIf you don‚Äôt need to manipulate paths via pathlib or you just need a path as a string, you can always invoked str() on the pathlib object or use the accessors ending with _str. For example:\nfrom testing_utils import TestCasePlus\n\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_stringified_locations(self):\n        examples_dir = self.examples_dir_str\n\n\nUsing unique temporary files and directories are essential for parallel test running, so that the tests won‚Äôt overwrite each other‚Äôs data. Also we want to get the temporary files and directories removed at the end of each test that created them. Therefore, using packages like tempfile, which address these needs is essential.\nHowever, when debugging tests, you need to be able to see what goes into the temporary file or directory and you want to know it‚Äôs exact path and not having it randomized on every test re-run.\nA helper class testing_utils.TestCasePlus is best used for such purposes. It‚Äôs a sub-class of unittest.TestCase, so we can easily inherit from it in the test modules.\nHere is an example of its usage:\nfrom testing_utils import TestCasePlus\n\n\nclass ExamplesTests(TestCasePlus):\n    def test_whatever(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\nThis code creates a unique temporary directory, and sets tmp_dir to its location.\n\nCreate a unique temporary dir:\n\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir()\ntmp_dir will contain the path to the created temporary dir. It will be automatically removed at the end of the test.\n\nCreate a temporary dir of my choice, ensure it‚Äôs empty before the test starts and don‚Äôt empty it after the test.\n\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir(\"./xxx\")\nThis is useful for debug when you want to monitor a specific directory and want to make sure the previous tests didn‚Äôt leave any data in there.\n\nYou can override the default behavior by directly overriding the before and after args, leading to one of the following behaviors:\n\nbefore=True: the temporary dir will always be cleared at the beginning of the test.\nbefore=False: if the temporary dir already existed, any existing files will remain there.\nafter=True: the temporary dir will always be deleted at the end of the test.\nafter=False: the temporary dir will always be left intact at the end of the test.\n\n\nfootnote: In order to run the equivalent of rm -r safely, only subdirs of the project repository checkout are allowed if an explicit tmp_dir is used, so that by mistake no /tmp or similar important part of the filesystem will get nuked. i.e.¬†please always pass paths that start with ./.\nfootnote: Each test can register multiple temporary directories and they all will get auto-removed, unless requested otherwise.\n\n\n\nIf you need to temporary override sys.path to import from another test for example, you can use the ExtendSysPath context manager. Example:\nimport os\nfrom testing_utils import ExtendSysPath\n\nbindir = os.path.abspath(os.path.dirname(__file__))\nwith ExtendSysPath(f\"{bindir}/..\"):\n    from test_trainer import TrainerIntegrationCommon  # noqa\n\n\n\n\nThis is useful when a bug is found and a new test is written, yet the bug is not fixed yet. In order to be able to commit it to the main repository we need make sure it‚Äôs skipped during make test.\nMethods:\n\nA skip means that you expect your test to pass only if some conditions are met, otherwise pytest should skip running the test altogether. Common examples are skipping windows-only tests on non-windows platforms, or skipping tests that depend on an external resource which is not available at the moment (for example a database).\nA xfail means that you expect a test to fail for some reason. A common example is a test for a feature not yet implemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with pytest.mark.xfail), it‚Äôs an xpass and will be reported in the test summary.\n\nOne of the important differences between the two is that skip doesn‚Äôt run the test, and xfail does. So if the code that‚Äôs buggy causes some bad state that will affect other tests, do not use xfail.\n\n\n\nHere is how to skip whole test unconditionally:\n\npython no-style @unittest.skip(\"this bug needs to be fixed\") def test_feature_x():\nor via pytest:\npython no-style @pytest.mark.skip(reason=\"this bug needs to be fixed\")\nor the xfail way:\npython no-style @pytest.mark.xfail def test_feature_x():\nHere‚Äôs how to skip a test based on internal checks within the test:\ndef test_feature_x():\n    if not has_something():\n        pytest.skip(\"unsupported configuration\")\nor the whole module:\nimport pytest\n\nif not pytest.config.getoption(\"--custom-flag\"):\n    pytest.skip(\"--custom-flag is missing, skipping tests\", allow_module_level=True)\nor the xfail way:\ndef test_feature_x():\n    pytest.xfail(\"expected to fail until bug XYZ is fixed\")\n\nHere is how to skip all tests in a module if some import is missing:\n\ndocutils = pytest.importorskip(\"docutils\", minversion=\"0.3\")\n\nSkip a test based on a condition:\n\npython no-style @pytest.mark.skipif(sys.version_info &lt; (3,6), reason=\"requires python3.6 or higher\") def test_feature_x():\nor:\npython no-style @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\") def test_feature_x():\nor skip the whole module:\npython no-style @pytest.mark.skipif(sys.platform == 'win32', reason=\"does not run on windows\") class TestClass():     def test_feature_x(self):\nMore details, example and ways are here.\n\n\n\n\n\n\nIn order to test functions that write to stdout and/or stderr, the test can access those streams using the pytest‚Äôs capsys system. Here is how this is accomplished:\nimport sys\n\n\ndef print_to_stdout(s):\n    print(s)\n\n\ndef print_to_stderr(s):\n    sys.stderr.write(s)\n\n\ndef test_result_and_stdout(capsys):\n    msg = \"Hello\"\n    print_to_stdout(msg)\n    print_to_stderr(msg)\n    out, err = capsys.readouterr()  # consume the captured output streams\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    sys.stderr.write(err)\n    # test:\n    assert msg in out\n    assert msg in err\nAnd, of course, most of the time, stderr will come as a part of an exception, so try/except has to be used in such a case:\ndef raise_exception(msg):\n    raise ValueError(msg)\n\n\ndef test_something_exception():\n    msg = \"Not a good value\"\n    error = \"\"\n    try:\n        raise_exception(msg)\n    except Exception as e:\n        error = str(e)\n        assert msg in error, f\"{msg} is in the exception:\\n{error}\"\nAnother approach to capturing stdout is via contextlib.redirect_stdout:\nfrom io import StringIO\nfrom contextlib import redirect_stdout\n\n\ndef print_to_stdout(s):\n    print(s)\n\n\ndef test_result_and_stdout():\n    msg = \"Hello\"\n    buffer = StringIO()\n    with redirect_stdout(buffer):\n        print_to_stdout(msg)\n    out = buffer.getvalue()\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    # test:\n    assert msg in out\nAn important potential issue with capturing stdout is that it may contain \\r characters that in normal print reset everything that has been printed so far. There is no problem with pytest, but with pytest -s these characters get included in the buffer, so to be able to have the test run with and without -s, you have to make an extra cleanup to the captured output, using re.sub(r'~.*\\r', '', buf, 0, re.M).\nBut, then we have a helper context manager wrapper to automatically take care of it all, regardless of whether it has some \\r‚Äôs in it or not, so it‚Äôs a simple:\nfrom testing_utils import CaptureStdout\n\nwith CaptureStdout() as cs:\n    function_that_writes_to_stdout()\nprint(cs.out)\nHere is a full test example:\nfrom testing_utils import CaptureStdout\n\nmsg = \"Secret message\\r\"\nfinal = \"Hello World\"\nwith CaptureStdout() as cs:\n    print(msg + final)\nassert cs.out == final + \"\\n\", f\"captured: {cs.out}, expecting {final}\"\nIf you‚Äôd like to capture stderr use the CaptureStderr class instead:\nfrom testing_utils import CaptureStderr\n\nwith CaptureStderr() as cs:\n    function_that_writes_to_stderr()\nprint(cs.err)\nIf you need to capture both streams at once, use the parent CaptureStd class:\nfrom testing_utils import CaptureStd\n\nwith CaptureStd() as cs:\n    function_that_writes_to_stdout_and_stderr()\nprint(cs.err, cs.out)\nAlso, to aid debugging test issues, by default these context managers automatically replay the captured streams on exit from the context.\n\n\n\nIf you need to validate the output of a logger, you can use CaptureLogger:\nfrom transformers import logging\nfrom testing_utils import CaptureLogger\n\nmsg = \"Testing 1, 2, 3\"\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")\nwith CaptureLogger(logger) as cl:\n    logger.info(msg)\nassert cl.out, msg + \"\\n\"\n\n\n\n\nIf you want to test the impact of environment variables for a specific test you can use a helper decorator transformers.testing_utils.mockenv\nfrom testing_utils import mockenv\n\n\nclass HfArgumentParserTest(unittest.TestCase):\n    @mockenv(TRANSFORMERS_VERBOSITY=\"error\")\n    def test_env_override(self):\n        env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\nAt times an external program needs to be called, which requires setting PYTHONPATH in os.environ to include multiple local paths. A helper class testing_utils.TestCasePlus comes to help:\nfrom testing_utils import TestCasePlus\n\n\nclass EnvExampleTest(TestCasePlus):\n    def test_external_prog(self):\n        env = self.get_env()\n        # now call the external program, passing `env` to it\nDepending on whether the test file was under the tests test suite or examples it‚Äôll correctly set up env[PYTHONPATH] to include one of these two directories, and also the src directory to ensure the testing is done against the current repo, and finally with whatever env[PYTHONPATH] was already set to before the test was called if anything.\nThis helper method creates a copy of the os.environ object, so the original remains intact.\n\n\n\nIn some situations you may want to remove randomness for your tests. To get identical reproducible results set, you will need to fix the seed:\nseed = 42\n\n# python RNG\nimport random\n\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\n\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\n\nnp.random.seed(seed)\n\n# tf RNG\ntf.random.set_seed(seed)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "‚úèÔ∏è  Testing"
    ]
  },
  {
    "objectID": "qmd/testing/index.html#debugging-tests",
    "href": "qmd/testing/index.html#debugging-tests",
    "title": "‚úèÔ∏è Testing",
    "section": "",
    "text": "To start a debugger at the point of the warning, do this:\npytest tests/utils/test_logging.py -W error::UserWarning --pdb",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "‚úèÔ∏è  Testing"
    ]
  },
  {
    "objectID": "qmd/testing/index.html#a-massive-hack-to-create-multiple-pytest-reports",
    "href": "qmd/testing/index.html#a-massive-hack-to-create-multiple-pytest-reports",
    "title": "‚úèÔ∏è Testing",
    "section": "",
    "text": "Here is a massive pytest patching that I have done many years ago to aid with understanding CI reports better.\nTo activate it add to tests/conftest.py (or create it if you haven‚Äôt already):\nimport pytest\n\ndef pytest_addoption(parser):\n    from testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\nand then when you run the test suite, add --make-reports=mytests like so:\npytest --make-reports=mytests tests\nand it‚Äôll create 8 separate reports:\n$ ls -1 reports/mytests/\ndurations.txt\nerrors.txt\nfailures_line.txt\nfailures_long.txt\nfailures_short.txt\nstats.txt\nsummary_short.txt\nwarnings.txt\nso now instead of having only a single output from pytest with everything together, you can now have each type of report saved into each own file.\nThis feature is most useful on CI, which makes it much easier to both introspect problems and also view and download individual reports.\nUsing a different value to --make-reports= for different groups of tests can have each group saved separately rather than clobbering each other.\nAll this functionality was already inside pytest but there was no way to extract it easily so I added the monkey-patching overrides testing_utils.py. Well, I did ask if I can contribute this as a feature to pytest but my proposal wasn‚Äôt welcome.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "‚úèÔ∏è  Testing"
    ]
  },
  {
    "objectID": "qmd/resources/index.html",
    "href": "qmd/resources/index.html",
    "title": "üìì Resources",
    "section": "",
    "text": "Logbooks and chronicles of training LLM/VLM are one of the best sources to learn from about dealing with training instabilities and choosing good hyper parameters.\nIf you know of a public LLM/VLM training logbook that is not on this list please kindly let me know or add it via a PR. Thank you!\nThe listing is in no particular order other than being grouped by the year.\n\n\n\nBigScience pre-BLOOM 108B training experiments (2021): chronicles | the full spec and discussions (backup: 1 | 2)\n\n\n\n\n\nBigScience BLOOM-176B (2022): chronicles-prequel | chronicles | the full spec and discussions (backup: 1 | 2 | 3)\nMeta OPT-175B (2022): logbook | Video (backup: 1)\nTHUDM GLM-130B (2022): en logbook | Mandarin version (backup: 1 | 2)\n\n\n\n\n\nHuggingFace IDEFICS-80B multimodal (Flamingo repro) (2023): Learning log | Training Chronicles (backup: 1 | 2)\nBloombergGPT 50B LLM - section C in BloombergGPT: A Large Language Model for Finance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üìì  Resources"
    ]
  },
  {
    "objectID": "qmd/resources/index.html#publicly-available-training-llmvlm-logbooks",
    "href": "qmd/resources/index.html#publicly-available-training-llmvlm-logbooks",
    "title": "üìì Resources",
    "section": "",
    "text": "Logbooks and chronicles of training LLM/VLM are one of the best sources to learn from about dealing with training instabilities and choosing good hyper parameters.\nIf you know of a public LLM/VLM training logbook that is not on this list please kindly let me know or add it via a PR. Thank you!\nThe listing is in no particular order other than being grouped by the year.\n\n\n\nBigScience pre-BLOOM 108B training experiments (2021): chronicles | the full spec and discussions (backup: 1 | 2)\n\n\n\n\n\nBigScience BLOOM-176B (2022): chronicles-prequel | chronicles | the full spec and discussions (backup: 1 | 2 | 3)\nMeta OPT-175B (2022): logbook | Video (backup: 1)\nTHUDM GLM-130B (2022): en logbook | Mandarin version (backup: 1 | 2)\n\n\n\n\n\nHuggingFace IDEFICS-80B multimodal (Flamingo repro) (2023): Learning log | Training Chronicles (backup: 1 | 2)\nBloombergGPT 50B LLM - section C in BloombergGPT: A Large Language Model for Finance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üìì  Resources"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/launchers/index.html",
    "href": "qmd/orchestration/slurm/launchers/index.html",
    "title": "",
    "section": "",
    "text": "QmdOrchestrationWorking in SLURM EnvironmentLaunchers with SLURM\n\n\n\n\n\nLaunchers with SLURM\nThe following are complete SLURM scripts that demonstrate how to integrate various launchers:\n\ntorchrun - to be used with PyTorch distributed.\naccelerate - to be used with HF Accelerate.\nlightning - to be used with Lightning (‚ÄúPyTorch Lightning‚Äù and ‚ÄúLightning Fabric‚Äù).\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam and Bekman, Stas},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam, and Stas Bekman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Orchestration",
      "Working in SLURM Environment",
      "Launchers with SLURM"
    ]
  },
  {
    "objectID": "qmd/network/index.html",
    "href": "qmd/network/index.html",
    "title": "üõú Network",
    "section": "",
    "text": "Subsections:\n\nBenchmarks\n\nThis chapter is a WIP\nIt‚Äôs not enough to buy/rent expensive accelerators to train and infer models fast. You need to ensure that your storage IO, CPU and network are fast enough to ‚Äúfeed the accelerator furnace‚Äù. If this is not ensured then the expensive accelerators will be underutilized leading to lost $$, slower training time and inference throughput. While it can be any other of the mentioned components, the network is often the bottleneck during the training (assume your DataLoader is fast).\nIf your model fits on a single accelerator, you have little to worry about. But nowadays most models require several accelerators to load and LLM/VLM models require multiple compute nodes for training and some even for inference.\nMost compute nodes contain 8 accelerators, some 4, others 16, and even more accelerators and recently there are some that have one super-accelerator per node.\nWhen the model spans several accelerators and doesn‚Äôt leave a single node all you need to worry about is fast Intra-node networking. As soon as the model requires several nodes, which is often the case for training as one can use multiple replicas to parallelize and speed up the training, then fast Inter-node networking becomes the key.\nThis article covers both types of networking hardware, reports their theoretical and effective bandwidths and explains how they inter-play with each other.\n\n\n\nDMA: Direct Memory Access\nEFA: Elastic Fabric Adapter\nHCA: Host Channel Adapter\nIB: Infiniband\nMFU: Model Flops Utilization (e.g.¬†mfu=0.5 at half-precision on A100 comes from getting 156TFLOPs, because peak half-precision spec is 312TFLOPS, and thus 156/312=0.5)\nNIC: Network Interface Card\nOPA: Omni-Path Architecture\nRoCE: RDMA over Converged Ethernet\nRoE: RDMA over Ethernet\nVPI: Virtual Protocol Interconnect\nRDMA: Remote Direct Memory Access\nxGMI: Socket to Socket Global Memory Interface\n\nSpeed-related: - Bi-directional, Duplex: a transmission from one point to another in both directions A &lt;-&gt; B, typically 2x speed of unidirectional - GBps, GB/s: Gigabytes per secs (1GBps = 8Gbps) transferred in a channel - GT/s: GigaTransfers per second - the number of operations transferring data that occur in each second. - Gbps, Gb/s: Gigabits per secs (1Gbps = 1/8GBps) transferred in a channel - Unidirectional: a transmission from one point to another in one direction A -&gt; B\n\n\n\nThis is probably one of the most important multi-segment section that you really want to understand well. While it seeks out to show how important the inter-node speed is, to build up the case it‚Äôll teach on the way many important training-related concepts.\n\n\nFirst, let‚Äôs get a bit of a feeling what all those Gbps/GBps practically mean.\nIf your model is 80B parameter large, and you need to transmit every parameter or a gradient on the network even once in float32 (fp32) format, which requires 4 bytes per parameter, so you need to send 80*4 320GB of data, or 2560Gb (*8). If your network‚Äôs bandwidth is 200Gbps it will take 12.8 seconds (2560/200) to transmit. And if you had 1600Gbps network then it‚Äôd take only 1.6 seconds. Why does it matter?\n\n\n\nLet‚Äôs start with a much smaller model of say 2B params, to train it you‚Äôd need at least 18 bytes per parameter in mixed half precision. So 18*2 36GB of memory just for model weights, optimizer states and gradients. Plus you need additional memory for activations and it‚Äôll depend on the batch size and sequence length. But with 80GB A100 GPU we can definitely train this model on a single GPU.\nWe then assume for the moment that the DataLoader is fast enough to be negligible in duration compared to the compute time. And thus we get a close to a perfect MFU (Model FLOPs Utilization):\n[DL][  compute  ][DL][  compute  ][DL][  compute  ]\n---------------------------------------------------&gt; time\n|&lt;--iteration--&gt;||&lt;--iteration--&gt;||&lt;--iteration--&gt;|\nwhich means that the GPU just needs to do many matmuls and it‚Äôd do it amazing fast. In this situation you get the highest ROI (Return on Investment).\n\n\n\nThe previous situation was fantastic due to the close to perfect MFU, but you realize that the training on a single GPU is going to take quite some time, since we are in AI race you‚Äôd probably want to finish the training sooner than later. So you‚Äôd ask - can I train the model on 8 GPUs instead, and the answer would be - yes, of course. With one caveat - at the end of each iteration you‚Äôd need to sync the gradients between the 8 processes (each process for a single GPU), so that each participating process of the training can benefit from what the other 7 have learned during the last iteration.\nfootnote: You could, of course, use less than 8 GPUs, it is just that most NVIDIA GPU-based compute nodes these days have 8 GPUs so why not get the best return on investment.\nfootnote: in the ideal world the training on 1 gpu for 8 durations of time, should cost the same as training on 8 gpus for 1 duration of time. That‚Äôs one would expect to spend the same $$ and to finish 8 times faster. But because of data synchronization requirements.\nIf the experimental model still contains 2B params like in the previous section and grads are in fp32 then the training program needs to send 8GB (2G * 4B) of data on every iteration. Moreover, since syncing the gradients requires an all_reduce collective collective - it needs to transmit the data twice - the first time sending the gradient data by each gpu, computing the sum of gradients and send this value back to each participating gpu so that each training process will benefit from the learning advancements each of its peers made in the last iteration.\nHere is the all-reduce collective visualized:\n\n\n\nall-reduce\n\n\n(source)\nSo we need to send 8GB twice, which means we need to send 16GB of data.\nfootnote: and to be exact the 2x comms volume for all-reduce is really 2*(n-1)/n where n is the number of participating gpus. So if n=2, the coefficient is just 1 since 2*(2-1)/2=1 and 1.75 for n=8 since 2*(8-1)/8=1.75 and it becomes already very close to 2 at n=64.\nfootnote: there is also the important issue of latency of the network - which is multiplied several times due to how data is gathered from all participating gpus. But, given that here we are moving a very large payload the latency contributes a very small overhead and for simplicity can be ignored.\nHow long will it take to send 16GB of data?\n\nA100 @ 300GBps: 16/300 = 0.053 secs\nH100 @ 450GBps: 16/450 = 0.035 secs\n\nwhich is incredibly fast!\nAnd here is how our timeline will look like:\n[DL][  compute ][comms][DL][  compute ][comms][DL][  compute ][comms]|\n-----------------------------------------------------------------------&gt; time\n|&lt;---- iteration ----&gt;||&lt;---- iteration ----&gt;||&lt;---- iteration -----&gt;|\noh and this whole synchronization protocol is called DDP (DistributedDataParallel) in the PyTorch lingo.\n\n\nEven with this really fast comms the network still creates a bottleneck and leads to a short idling of the gpus. To solve this issue the advanced algorithms implement an overlap of comms and compute. Until now we approached the problem as one single transmission, but in reality each model is made of many layers and each layer can transmit the gradients it has computed, while the next layer is computing its gradients. So if you look at the level of the model, what happens in the backward path is:\n[   compute   ][   compute   ][   compute   ]\n               [comms]        [comms]        [comms]\n---------------------------------------------&gt; time\n&lt;- layer -1 -&gt;|&lt;- layer -2 -&gt;|&lt;- layer -3 -&gt;|\nso once the last layer (-1) computed its gradients it all-reduces them while the 2nd to last layer performs its backward, and so on, until the first layer finished with gradients and it finally sends its gradients out.\nSo now you understand how overlapping works, So we can now update our bigger picture diagram to be:\nNow our timing diagram becomes very similar to the diagram we had for a single gpu:\n[DL][  compute  ][DL][  compute  ][DL][  compute  ]\n[  comms ]       [  comms]        [  comms]\n---------------------------------------------------&gt; time\n|&lt;--iteration--&gt;||&lt;--iteration--&gt;||&lt;--iteration--&gt;|\nand we hope that comms are faster than DL+compute, since if they aren‚Äôt faster than we have the following gpu idling gaps:\n[DL][  compute  ][idle][DL][  compute  ][idle][DL][  compute  ][idle]\n[         comms       ][         comms       ][         comms       ]\n----------------------------------------------------------------------&gt; time\n|&lt;---  iteration  ---&gt;||&lt;---  iteration  ---&gt;||&lt;---  iteration  ---&gt;|\n\n\n\nCalculating TFLOPS answers the question of how long will it take to perform a compute.\nThere is a bit of nomenclature confusion here as TFLOPS as the final s sometimes means sec and at other times just ops.\nFor example, when you read, the A100 spec the TFLOPS there means TeraFloatingPointOperations per second.\nSo let‚Äôs define these abbreviations exactly:\n\nTFLOPS - TeraFLoatingpointOPerations per Second (another way is TFLOP/s)\nTFLOP - TeraFLoatingpointOPerations (or TFLOPs - lower case s but it‚Äôs already confusing)\n\nAlso see the wiki page for more clarifications.\nFor GPT-family of decoder transformers models we can use the math described in this BLOOM-176 docs:\nHere is how many TFLOP are processed per second:\ntflops = model_size_in_B * 4 * 2 * seqlen * global_batch_size / (time_in_sec_per_interation * total_gpus * 1e3)\nThis formula assume one uses activation recomputation which saves GPU memory while introducing a smallish overhead. If one doesn‚Äôt use it then replace 4 with 3 as the model has to do only 1x compute per forward and 2x per backward (since the grads are calculated twice - once for inputs and once for weights). With activation recomputation the forward is done twice and thus you have an additional path which leads to a multiplier of 4 instead of 3\nfootnote: activation recomputation and gradient checkpointing both refer to the same technique.\nso let‚Äôs remove the time component, which will give us the total TFLOP\ntflop = model_size_in_B * 4 * 2 * seqlen * global_batch_size / (total_gpus * 1e3)\nSo let‚Äôs say we have: - seqlen=2048 (sequence length) - global_batch_size=16\nand we already defined: - total_gpus=8 - model_size_in_B=2\nThis gives us:\ntflops = 2 * 4 * 2 * 2048 * 16 / (8 * 1e3) = 65.536 TFLOP\nSo if we do a mixed half-precision training and most of the operations are done in half-precision then we can roughly say that we do 312 TFLOPS on A100 and usually a well optimized framework on a well-tuned hardware will do at least 50% MFU - that is it‚Äôll be able to compute at about 1/2 peak performance.\nfootnote: It‚Äôs a ~3x 989 TFLOPS on H100 (scroll to the end) and also it shows a misleading 2x numbers for sparsity so you have to mentally divide it by 2.\nSo continuing this train of thought it means that the setup will have about 156TFLOPS - and so it‚Äôll take 0.42 secs to process a single iteration (2x forward and 2x backward compute) if we ignore the overhead of the DataLoader (which we hope is close to instant).\nEarlier we said that a typical A100 node has an intra-node NVLink connection of 300GBps, and thus we said that to send 16GB of grads will take 16/300 = 0.053 secs.\nAnd we measured our compute to be 0.42 secs, so here we have a problem as 0.053 &gt; 0.42 so the comms will be slower than compute and the network is a bottleneck.\nYou can now do several thought experiments - for example if you halve the batch size or the sequence length you will halve the compute time.\nfootnote: this is a very rough suggestions since GPUs work the fastest when the matrices they multiple are huge. But this is good enough for a simplified thought experiment we are having here. In reality halving the dimension will not halve the compute time.\nOK, but hopefully at this point it‚Äôs quite clear that if you remain at the boundaries of a single node, you don‚Äôt need to worry about your GPUs idling.\nBut what if you want to speed up the training even more and throw say 4x 8-gpu nodes at it. (and of course you don‚Äôt have a choice but to use multiple nodes if you have a much larger model). Suddenly, the comms can become an even bigger bottleneck.\n\n\n\n\nSo here we are continuing with the idea of 2B param model and we will now use 32 gpus across 4 nodes to speed up the training even more.\nWhile each group of 8 gpus is still connected with super-fast NVLink technology, the inter-node connections are usually in an order of magnitude slower.\nLet‚Äôs say you have a 200Gbps connection. Let‚Äôs repeat the math from the previous section of how long it‚Äôll take to reduce 16GB of gradients.\n16GB is 128Gb, and so at 200Gbps this will take 0.64 seconds.\nAnd if stick to the compute taking 0.42 seconds, here we end up with comms taking longer than compute since 0.64 &gt; 0.42.\nLet‚Äôs bring both use cases together:\n\n\n\nnodes\ncomms\ncompute\ncomms is a bottleneck\n\n\n\n\n1\n0.027\n0.42\nno\n\n\n4\n0.64\n0.42\nyes\n\n\n\non this 200Gbps inter-node setup the comms are 23x slower than the same performed on an intra-node NVlink connections.\nIn this case even though we still have the much faster NVLink connection, we don‚Äôt really benefit from it, since the whole ensemble communicates at the speed of the slowest link. And that slowest link is the inter-node connection.\nSo in this particular situation if you were able to get a 400Gbps inter-node the speed would double and the comms will finish in 0.32 secs and thus will be faster than that 0.42 secs the compute would take.\nfootnote: you will never be able to get the advertised speed fully on the application level, so if it‚Äôs advertised as 400Gbps in the best case expect to get 320Gbps (about 80%). So make sure to take this into the account as well. Moreover, depending on the payload of each collective - the smaller the payload the smaller the actual network throughput will be.\nAnd remember this was all handling a pretty tiny as considered these days 2B param model.\nNow do the same math with 20B and 200B parameter model and you will see that you need to have a much much faster inter-node connectivity to efficiently scale.\n\n\n\nOf course, when we train large models we don‚Äôt use DDP, because we simply can‚Äôt fit the whole model on a single gpu so various other techniques are used. The details are discussed in a dedicated chapter on Model Parallelism, but the only important thing to understand immediately is that all scalability techniques incur a much larger comms overhead, because they all need to communicate a lot more than just gradients. and therefore the amount of traffic on the network can easily grow 3x and more as compared to the DDP protocol overhead we have been exploring so far.\nIt can be difficult to do even approximate math as we did in this chapter, because the actual compute time depends on the efficiency of the chosen framework, how well it was tuned, how fast the DataLoader can feed the batches and many other things, therefore there is no standard MFU that one can use in the math and you will discover your MFU when you configure and run the first few steps of the large model training. and then you will read the Performance chapters and improve your MFU even more.\nAs I have shown in these sections it should be possible to be able to do a back-of-envelope calculations once you understand the specific scalability technique and its networking costs, so that you could know ahead of time which Inter-node network speed you need to require from your acquisition manager. Of course, you also need to understand the particular model architecture and calculate how many TFLOP it will take to do a single iteration.\n\n\n\nMost benchmarking / bandwidth measurement tools will report a unidirectional bandwidth. So be careful when you look at unidirectional vs.¬†bidirectional (duplex) speeds. Typically the latter is ~2x faster.\nIf you measure the bandwidth on your setup and it‚Äôs about 40% of the advertised speed, carefully check if the advertised speed said duplex and if so half that and then your measured bandwidth should now be about 80% which is expected.\ncase study: for a while I couldn‚Äôt understand why when I run the nccl-tests all_reduce benchmark on an A100 node with advertised 600GBps intra-node speed I was getting only 235Gbps (40%) until Horace He kindly pointed out that I should be looking at unidirectional speed which is 300GBps, and then I get 80% of the theoretical spec which checks out.\n\n\n\n\nThere are multiple platforms/solutions out there that provide intra-node networking:\n\nGeneric: PCIe\nNVIDIA: NVLink and NVSwitch\nAMD: Infinity Fabric\nIntel: Gaudi2\n\nfootnote: In the following sections pay close attention that 1 GBps = 8 Gbps.\nfootnote: also pay close attention to when the spec says unidirectional vs bidirectional (duplex) speeds - if you read an online spec and it doesn‚Äôt explicitly declare the directionality - look for an answer. I had to research many docs to figure it out in some of the tables below as some vendors conveniently omit this crucial information. I even had to edit a few wiki pages to add the missing information. Remember that for the vendors the bigger the better so almost always they will use the duplex number, which is 2x larger than unidirectional one.\n\n\nPCIe is a high-speed serial computer expansion bus standard that can be found even on the cheapest computer desktop.\n\n\n\n\n\n\n\n\n\n\nInterconnect\nLane/Direction\nLanes\nUnidirection\nDuplex\n\n\n\n\nPCIe 4\n~2.0 GBps\n16\n31 GBps\n62 GBps\n\n\nPCIe 5\n~4.0 GBps\n16\n63 GBps\n126 GBps\n\n\nPCIe 6\n~7.5 GBps\n16\n121 GBps\n241 GBps\n\n\nPCIe 7\n~15.0 GBps\n16\n242 GBps\n484 GBps\n\n\n\nIf one compares the latest generations of different intra-node technologies (see the following sections) PCIe is usually an order of magnitude behind.\n\n\n\n\nNVLink is a wire-based serial multi-lane near-range communications link developed by Nvidia. Here is the What Is NVLink blog post with more background on it.\n\nI found the wiki pages quite difficult to follow, so I will try to help bring clarity into this.\nEffective payload rate of Intra-node GPU-to-GPU communication hardware:\n\n\n\n\n\n\n\n\n\n\n\nInterconnect\nLane/Direction\nLanes\nLinks\nUnidirection\nDuplex\n\n\n\n\nNVlink 2\n6.250 GBps\n4\n6\n150 GBps\n300 GBps\n\n\nNVlink 3\n6.250 GBps\n4\n12\n300 GBps\n600 GBps\n\n\nNVlink 4\n6.250 GBps\n4\n18\n450 GBps\n900 GBps\n\n\n\nNVlink 2, 3 and 4 use the same hardware of 4 lanes of 6.250 GBps each per link. Each has a unidirectional bandwidth of 25GB/s per link, and therefore 50GB/s per duplex link. The only difference is in the number of links:\n\nNVLink 2 has 6 links =&gt; 25* 6=&gt; 150 GBps unidirectional and 300 GBps bi-directional\nNVLink 3 has 12 links =&gt; 25*12=&gt; 300 GBps unidirectional and 600 GBps bi-directional\nNVLink 4 has 18 links =&gt; 25*18=&gt; 450 GBps unidirectional and 900 GBps bi-directional\n\nThe largest PCIe 16x slot has 16 lanes. Smaller slots have less lanes, 1x == 1 lane.\nAs of this writing NVIDIA Hopper nodes typically come equipped with PCIe 5 and NVLink 4. So there NVlink is 7x faster than PCIe.\nLet‚Äôs look at several examples of nodes and correlate the theory with reality.\nIf you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:\nnvidia-smi topo -m\nand it will tell you how the GPUs are inter-connected.\nOn a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      NV2     0-23            N/A\nGPU1    NV2      X      0-23            N/A\non a different machine w/o NVLink you may see:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      PHB     0-11            N/A\nGPU1    PHB      X      0-11            N/A\nThe report includes this legend:\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\nSo the first report NV2 tells us the GPUs are interconnected with 2 NVLinks, and the second report PHB we have a typical consumer-level PCIe+Bridge setup.\nCheck what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g.¬†NVLink), others slower (e.g.¬†PHB).\nDepending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training.\nNow, let‚Äôs look at the topology of the A100 and H100 nodes:\n\nA100 topology:\n\n$ nvidia-smi topo -m\n      GPU0  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity\nGPU0   X    NV12  NV12  NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU1  NV12   X    NV12  NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU2  NV12  NV12   X    NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU3  NV12  NV12  NV12   X    NV12  NV12  NV12  NV12   0-23         0\nGPU4  NV12  NV12  NV12  NV12   X    NV12  NV12  NV12  24-47         1\nGPU5  NV12  NV12  NV12  NV12  NV12   X    NV12  NV12  24-47         1\nGPU6  NV12  NV12  NV12  NV12  NV12  NV12   X    NV12  24-47         1\nGPU7  NV12  NV12  NV12  NV12  NV12  NV12  NV12   X    24-47         1\nYou can see there are 12 NVLinks and 2 NUMA Groups (2 CPUs w/ 24 cores each)\n\nH100 topology:\n\n$ nvidia-smi topo -m\n      GPU0  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity\nGPU0   X    NV18  NV18  NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU1  NV18   X    NV18  NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU2  NV18  NV18   X    NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU3  NV18  NV18  NV18   X    NV18  NV18  NV18  NV18   0-51         0\nGPU4  NV18  NV18  NV18  NV18   X    NV18  NV18  NV18  52-103        1\nGPU5  NV18  NV18  NV18  NV18  NV18   X    NV18  NV18  52-103        1\nGPU6  NV18  NV18  NV18  NV18  NV18  NV18   X    NV18  52-103        1\nGPU7  NV18  NV18  NV18  NV18  NV18  NV18  NV18   X    52-103        1\nYou can see there are 18 NVLinks and 2 NUMA Groups (2 CPUs w/ 52 cores each)\nOf course, other A100 and H100s node reports may vary, e.g.¬†different number of cpu cores.\n\n\n\nNVSwitch can connect more than 8 GPUs at the speed of NVLink. It‚Äôs advertised to connect up to 256 GPUs in the future generations of the switch.\nThe benefit of connecting more than 8 GPUs at the speed of NVLink is that it allows all-to-all GPU communications at a much faster speed than any intra-node hardware can provide. And with ever increasing compute speeds the network is the likely bottleneck leading to underutilized super-expensive GPUs.\nFor example, in the universe of Tensor Parallelism (Megatron), one doesn‚Äôt use TP degree of more than 8, because TP is only efficient at NVLink speed. ZeRO-DP (Depspeed/FSDP) would also run much faster if the whole cluster uses NVLink speed and involves no slow inter-node connections.\nThe NVIDIA DGX H100 has a 3.6 TBps of full-duplex NVLink Network bandwidth provided by 72 NVLinks (NVLink 4). The normal NVlink 4 has 18 NVLinks (0.9 TBps duplex). So this setup has 4 switches (18*4=72) and therefore 0.9*4=3.6 TBps. Note, that this server has 8 GPUs, so here we get a much faster intra-node communications as compared to the standard NVlink 4.0 which provides only 0.9 TBps all-to-all connectivity for 8 GPUs.\nNVIDIA DGX A100 has 6 switches of 12 NVlinks for a total of 72.\nDGX H100 SuperPOD combines 32 DGX H100 servers, for a total of 256 GPUs. It looks like here they use only half the NVLinks they used for a single DGX H100, so only 1.8 GBps per node, for a total of 57.6 GBps in total.\n\n\n\nAMD MI* Accelerators Intra-node communication is performed by AMD Infinity Fabric, which is also known as xGMI (Socket to Socket Global Memory Interface).\nThis is AMD‚Äôs answer to NVLink.\n\n\n\nInterconnect\nLink/Direction\nLinks\nUnidirection\nDuplex\n\n\n\n\nMI250x\n50 GBps\n7\n350 GBps\n700 GBps\n\n\nMI300x\n64 GBps\n7\n448 GBps\n896 GBps\n\n\n\n\n\n\n\n\n\n\n\n\n\nAMD Infinity Platform Architecture\n\n\nPlatform specs: - MI250X - MI300x\n\n\n\nAccording to Gaudi2 spec, these servers provide 8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps and each card connected with each of the other 7 cards at 262.5 GBps.\n\n\n\n\nNon-uniform memory access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. As modern servers have more than one CPU to get the best performance GPUs residing in the same block as the corresponding CPU should have the processes bound to that NUMA node.\nHere is a typical A100 8x GPUs server, as visualized by hwloc:\n\n\n\na100 server numa nodes\n\n\nAs you can see it has 2 CPUs, each defining a NUMA block, and each such block contains a group of 4 GPUs. The GPUs are the grey blocks that say CoProc with 108 compute units (SMs) and 79GB of memory.\nfootnote: was generated by lstopo a100.png\n\n\nnote-to-self: probably belongs in its own chapter?\n\n\nhttps://github.com/open-mpi/hwloc\nThe Hardware Locality (hwloc) software project aims at easing the process of discovering hardware resources in parallel architectures. It offers command-line tools and a C API for consulting these resources, their locality, attributes, and interconnection. hwloc primarily aims at helping high-performance computing (HPC) applications, but is also applicable to any project seeking to exploit code and/or data locality on modern computing platforms.\nDiagnostics: to take a snapshot of the server NUMA topology and save it as an image (supports many other formats)\nlstopo a100.png\nNUMA node binding: hwloc-bind - binding processes, threads and memory\nBind an existing process to a specific NUMA node:\nhwloc-bind --pid 1234 numa:0\nSimilar software: numactl/libnuma\nSome useful suggestions in pytorch docs\n\n\n\n\n\nAs inter-node hardware is about of an order of magnitude slower than intra-node hardware in this universe Gbps are used instead of GBps. (1 GBps = 8 Gbps)\nWhen it comes to inter-node networking hardware, there are the well established InfiniBand from NVIDIA and a few other players and there are many new comers that mainly are coming from compute cloud providers who can‚Äôt compete on the slim margin renting out someone else‚Äôs hardware so they build their own (EFA, and others not yet disclosed).\n\n\nElastic Fabric Adapter (EFA) is a recent technology created by AWS.\n\nEFA v1 0.4 Tbps (effective 340 Gbps for all_reduce tests) (P4 AWS instances)\nEFA v2 3.2 Tbps (since Q3-2023, P5 AWS instances)\n\n\n\n\nNow InfiniBand (IB) has been around for a few decades so there are many available configurations that can be found out there. So that if someone says they have InfiniBand that is insufficient information. What you need to know is the signaling rate and the number of IB links.\nHere are the most recent signaling rates which you are likely to see in the current hardware offerings:\nSignaling rate of uni-directional links in Gbps: | Links | EDR | HDR | NDR | XDR | GDR | | ‚Äî-: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | | 1 | 25 | 50 | 100 | 200 | 400 | | 4 | 100 | 200 | 400 | 800 | 1600 | | 8 | 200 | 400 | 800 | 1600 | 3200 | | 12 | 300 | 600 | 1200 | 2400 | 4800 |\nLatency in usecs: | EDR | HDR | NDR | XDR | GDR | | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | | 0.5 | 0.6 | ?? | ?? | ?? |\n?? = NDR and later didn‚Äôt publish latency data\nInfiniBand provides RDMA.\nHere are some examples of NVIDIA devices with the fastest IB:\n\nOne configuration of NVIDIA DGX H100 comes with 8x NVIDIA ConnectX-7 Ethernet/InfiniBand ports each of 200Gbps, for a total of 1.6 Gbps to connect with other DGX servers.\nFor DGX H100 SuperPOD the ConnectX-7s across all 32 DGX servers and associated InfiniBand switches provide 25.6 TBps of full duplex bandwidth for use within the pod or for scaling out the multiple SuperPODs - that is an equivalent of 0.8 TBps per node (6.4Tbps!).\n\n\n\n\nAccording to Gaudi2 spec, these servers provide 24 NICs of 100GbE RoCE v2 ROMA for a total of 2.4Tbps of inter-node connectivity with other Gaudi2 servers.\n\n\n\nHPE Slingshot interconnect seems to be used by HPCs. As of this writing it provides 200Gbps per link. Some HPCs use 4 of those links to build 800Gbps interconnects, and, of course, with more links will deliver a higher overall bandwidth.\n\n\n\nOmniPath Architecture (OPA). Originally by Intel, the technology got sold to Cornelis Networks.\ncase study: I used this technology at JeanZay HPC in France in 2022. It was only 135Gbps and while the vendor tried to fix it a year later it was still the same speed. Hopefully the issue has been resolved and the speed is much faster nowadays. Because it was so slow we had to use Megatron-Deepspeed for training BLOOM-176B instead of the much easier to use DeepSpeed ZeRO).\nAs of this writing I see that the product comes with either 100 or 200Gbps bandwidth. So it‚Äôs unlikely you will see anybody offering this solution for ML workloads, unless they manage to install many NICs perhaps?\nOmni-Path provides RDMA.\n\n\n\n\n\n\nThe network throughput in the advertised spec and the actual throughput will never be the same. In the best case you can expect about 80-90% of the advertised spec.\nThen the network throughput will depend on the size of payload being sent during each communication. The higher the payload the higher the throughput will be.\nLet‚Äôs demonstrate this using nccl-tests on a single A100 node\n$ ./build/all_reduce_perf -b 32k -e 16G -f 2 -g 8 -n 50\n[...]\n           size    time   algbw   busbw\n            (B)    (us)  (GB/s)  (GB/s)\n         32_768   43.83    0.75    1.31\n         65_536   46.80    1.40    2.45\n        131_072   51.76    2.53    4.43\n        262_144   61.38    4.27    7.47\n        524_288   80.40    6.52   11.41\n       1048_576   101.9   10.29   18.00\n       2097_152   101.4   20.68   36.18\n      4_194_304   101.5   41.33   72.33\n      8_388_608   133.5   62.82  109.93\n     16_777_216   276.6   60.66  106.16\n     33_554_432   424.0   79.14  138.49\n     67_108_864   684.6   98.02  171.54\n    134_217_728  1327.6  101.10  176.92\n    268_435_456  2420.6  110.90  194.07\n    536_870_912  4218.4  127.27  222.72\n  1_073_741_824  8203.9  130.88  229.04\n  2_147_483_648   16240  132.23  231.41\n  4_294_967_296   32136  133.65  233.88\n  8_589_934_592   64074  134.06  234.61\n 17_179_869_184  127997  134.22  234.89\nfootnote: I massaged the output to remove unwanted columns and made the size more human readable\nThis benchmark run an all_reduce collective for various payload sizes from 32KB to 16GB. The value that we care about is the busbw - this column tells us the real network throughput as explained here.\nAs you can see for payloads smaller than 8MB the throughput is very low - and it starts saturating around payload size of 536MB. It‚Äôs mostly because of latency. Reducing a single 4GB payload is much faster than 1000x 4MB payloads.\nHere is a benchmark that demonstrates that: all_reduce_latency_comp.py. Let‚Äôs run it on the same A100 node:\n$ python -u -m torch.distributed.run --nproc_per_node=8 all_reduce_latency_comp.py\n\n----------- 1x 4.0GB ----------------\n busbw: 1257.165 Gbps\n\n----------- 1000x 0.004GB ----------------\n busbw: 374.391 Gbps\nIt‚Äôs easy to see that it‚Äôs about 3x slower in this particular case to send the same payload but in 1000 smaller chunks.\nSo when you calculate how long does it take to all_reduce a given payload size, you need to use the corresponding busbw entry (after of course you have run this benchmark on your particular hardware/environment).\nFiguring out the payload can be tricky since it‚Äôd depend on the implementation of the framework. Some implementations will reduce each weight‚Äôs gradient alone which obvious would lead to a very small payload and the network will be very slow. Other implementations bucket multiple gradients together before reducing those, increasing the payload and minimizing the latency impact.\nBut let‚Äôs go back to the benchmark results table. This test was done on an A100 node that runs NVLink advertised as uni-directional 300GBs so we get about 78% of the theoretical speed with 17GB payload and more than that the benchmark crashes. It can be seen from the last few rows of the table that not much more can be squeezed.\nWe can also run p2pBandwidthLatencyTest which performs a low-level p2p benchmark:\n./p2pBandwidthLatencyTest\n[...]\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3      4      5      6      7\n     0 1581.48 274.55 275.92 272.02 275.35 275.28 273.62 273.20\n     1 274.70 1581.48 275.33 272.83 275.38 273.70 273.45 273.70\n     2 274.81 276.90 1594.39 272.66 275.39 275.79 273.97 273.94\n     3 273.25 274.87 272.12 1545.50 274.38 274.37 274.22 274.38\n     4 274.24 275.15 273.44 271.57 1584.69 275.76 275.04 273.49\n     5 274.37 275.77 273.53 270.84 274.59 1583.08 276.04 273.74\n     6 275.61 274.86 275.47 273.19 272.58 275.69 1586.29 274.76\n     7 275.26 275.46 275.49 273.61 275.50 273.28 272.24 1591.14\n[...]\nAs you can see in the Unidirectional section of the report we do get 274 GBps out of the advertised 300GBps (~91%).\nPlease note that when I re-run this same test on H100s (NVLink 4.0) I got a much worse efficiency:\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3      4      5      6      7\n     0 2494.51 364.13 375.99 378.03 376.77 376.71 374.85 375.66\n     1 375.18 2533.95 376.08 374.98 376.21 375.96 375.76 375.12\n     2 363.43 393.28 2532.67 376.35 377.14 376.47 375.76 375.48\n     3 369.90 375.92 393.63 2525.38 376.58 375.88 376.13 377.01\n     4 376.20 376.28 375.20 393.52 2526.02 375.82 375.05 376.10\n     5 376.26 376.60 375.54 375.52 376.81 2521.18 376.37 376.60\n     6 374.31 376.19 376.80 376.32 376.83 376.44 2529.85 376.39\n     7 376.17 376.49 376.53 374.95 376.30 376.82 375.71 2519.78\nSo 376GBps out of 450GBps is 83% (not very good).\nBottom line - in this particular setup: 1. if you have huge payloads you will be able to use about 80% of the advertised 300GBps 2. if the payload of each communication is smallish it could be far far lower.\nThis graph is also helpful to demonstrate how the actual bandwidth changes with the size of the message:\n (source)\nAnother tool for bandwidth measurements on NVIDIA GPUs is NVIDIA/nvbandwidth.\n\n\n\n (source)\nXXX: integrate/expand\n\n\n\nProprietary network hardware vendors like AWS (EFA) don‚Äôt disclose their secrets and therefore the public libraries like nccl cannot support those out of the box. These vendors have to supply their own versions of the network collective libraries to be used by users of their hardware.\nOriginally proprietary hardware vendors used the trick of telling the users to use LD_LIBRARY_PATH and/or LD_PRELOAD to dynamically overload libnccl.so to get their custom version loaded into PyTorch or another framework. But recently NCCL developed a NCCL Net Plugin which should be used now instead. This feature was added in NCCL v2.12.\nNow, when NCCL is initialized, it will look for a libnccl-net.so library and dynamically load it, then look for symbols inside the library. That‚Äôs where proprietary hardware vendors should now put their custom APIs. This library, of course, should still be either in LD_LIBRARY_PATH or the /etc/ld.so.conf config.\nFor more information about dynamic library loading see this section.\n\n\n\nIf you get 2 random nodes from the cloud they may not reside on the same subnet and there will be an additional latency incurred for all transmissions.\nYou want to make sure that the nodes used for a single training all reside on the same subnet/spine so they are all one hop away from each other.\nWhen you plan to eventually have a large cluster but starting small make sure that your provider can expand the cluster while keeping all the nodes close to each other.\nHere are the cloud-specific ways of accomplishing node proximity:\n\nAzure: availability set\nGCP: compact placement policies\n\nDepending on the type of package you have or what type of machines you rent - you may or may not be able to use those.\n\n\n\nIf you use a shared HPC environment, or even if you have your own cluster but sharing it with your colleagues expect the network bandwidth to be unreliable and fluctuate at different time of the day.\nThis situation unfortunately makes it extremely difficult to finetune the performance of your training setup. Since every time you run a test the TFLOPs will vary, so how do you do the optimization? Unfortunately I don‚Äôt have a magic trick here. If you have a working solution please kindly share.\ncase study: we had this issue at JeanZay HPC when we were doing preliminary experiments before we started training BLOOM-176B. As that HPC has many users it was pretty much impossible to do speed optimizations, as even running the exact same setup again and again gave different throughput results. Luckily just before we launched BLOOM-176B training we were given an exclusive access to the new at that time A100 partition so we were the only users and we were able to greatly optimize the throughput.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#glossary",
    "href": "qmd/network/index.html#glossary",
    "title": "üõú Network",
    "section": "",
    "text": "DMA: Direct Memory Access\nEFA: Elastic Fabric Adapter\nHCA: Host Channel Adapter\nIB: Infiniband\nMFU: Model Flops Utilization (e.g.¬†mfu=0.5 at half-precision on A100 comes from getting 156TFLOPs, because peak half-precision spec is 312TFLOPS, and thus 156/312=0.5)\nNIC: Network Interface Card\nOPA: Omni-Path Architecture\nRoCE: RDMA over Converged Ethernet\nRoE: RDMA over Ethernet\nVPI: Virtual Protocol Interconnect\nRDMA: Remote Direct Memory Access\nxGMI: Socket to Socket Global Memory Interface\n\nSpeed-related: - Bi-directional, Duplex: a transmission from one point to another in both directions A &lt;-&gt; B, typically 2x speed of unidirectional - GBps, GB/s: Gigabytes per secs (1GBps = 8Gbps) transferred in a channel - GT/s: GigaTransfers per second - the number of operations transferring data that occur in each second. - Gbps, Gb/s: Gigabits per secs (1Gbps = 1/8GBps) transferred in a channel - Unidirectional: a transmission from one point to another in one direction A -&gt; B",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#understanding-why-inter-node-network-speed-is-of-a-huge-importance",
    "href": "qmd/network/index.html#understanding-why-inter-node-network-speed-is-of-a-huge-importance",
    "title": "üõú Network",
    "section": "",
    "text": "This is probably one of the most important multi-segment section that you really want to understand well. While it seeks out to show how important the inter-node speed is, to build up the case it‚Äôll teach on the way many important training-related concepts.\n\n\nFirst, let‚Äôs get a bit of a feeling what all those Gbps/GBps practically mean.\nIf your model is 80B parameter large, and you need to transmit every parameter or a gradient on the network even once in float32 (fp32) format, which requires 4 bytes per parameter, so you need to send 80*4 320GB of data, or 2560Gb (*8). If your network‚Äôs bandwidth is 200Gbps it will take 12.8 seconds (2560/200) to transmit. And if you had 1600Gbps network then it‚Äôd take only 1.6 seconds. Why does it matter?\n\n\n\nLet‚Äôs start with a much smaller model of say 2B params, to train it you‚Äôd need at least 18 bytes per parameter in mixed half precision. So 18*2 36GB of memory just for model weights, optimizer states and gradients. Plus you need additional memory for activations and it‚Äôll depend on the batch size and sequence length. But with 80GB A100 GPU we can definitely train this model on a single GPU.\nWe then assume for the moment that the DataLoader is fast enough to be negligible in duration compared to the compute time. And thus we get a close to a perfect MFU (Model FLOPs Utilization):\n[DL][  compute  ][DL][  compute  ][DL][  compute  ]\n---------------------------------------------------&gt; time\n|&lt;--iteration--&gt;||&lt;--iteration--&gt;||&lt;--iteration--&gt;|\nwhich means that the GPU just needs to do many matmuls and it‚Äôd do it amazing fast. In this situation you get the highest ROI (Return on Investment).\n\n\n\nThe previous situation was fantastic due to the close to perfect MFU, but you realize that the training on a single GPU is going to take quite some time, since we are in AI race you‚Äôd probably want to finish the training sooner than later. So you‚Äôd ask - can I train the model on 8 GPUs instead, and the answer would be - yes, of course. With one caveat - at the end of each iteration you‚Äôd need to sync the gradients between the 8 processes (each process for a single GPU), so that each participating process of the training can benefit from what the other 7 have learned during the last iteration.\nfootnote: You could, of course, use less than 8 GPUs, it is just that most NVIDIA GPU-based compute nodes these days have 8 GPUs so why not get the best return on investment.\nfootnote: in the ideal world the training on 1 gpu for 8 durations of time, should cost the same as training on 8 gpus for 1 duration of time. That‚Äôs one would expect to spend the same $$ and to finish 8 times faster. But because of data synchronization requirements.\nIf the experimental model still contains 2B params like in the previous section and grads are in fp32 then the training program needs to send 8GB (2G * 4B) of data on every iteration. Moreover, since syncing the gradients requires an all_reduce collective collective - it needs to transmit the data twice - the first time sending the gradient data by each gpu, computing the sum of gradients and send this value back to each participating gpu so that each training process will benefit from the learning advancements each of its peers made in the last iteration.\nHere is the all-reduce collective visualized:\n\n\n\nall-reduce\n\n\n(source)\nSo we need to send 8GB twice, which means we need to send 16GB of data.\nfootnote: and to be exact the 2x comms volume for all-reduce is really 2*(n-1)/n where n is the number of participating gpus. So if n=2, the coefficient is just 1 since 2*(2-1)/2=1 and 1.75 for n=8 since 2*(8-1)/8=1.75 and it becomes already very close to 2 at n=64.\nfootnote: there is also the important issue of latency of the network - which is multiplied several times due to how data is gathered from all participating gpus. But, given that here we are moving a very large payload the latency contributes a very small overhead and for simplicity can be ignored.\nHow long will it take to send 16GB of data?\n\nA100 @ 300GBps: 16/300 = 0.053 secs\nH100 @ 450GBps: 16/450 = 0.035 secs\n\nwhich is incredibly fast!\nAnd here is how our timeline will look like:\n[DL][  compute ][comms][DL][  compute ][comms][DL][  compute ][comms]|\n-----------------------------------------------------------------------&gt; time\n|&lt;---- iteration ----&gt;||&lt;---- iteration ----&gt;||&lt;---- iteration -----&gt;|\noh and this whole synchronization protocol is called DDP (DistributedDataParallel) in the PyTorch lingo.\n\n\nEven with this really fast comms the network still creates a bottleneck and leads to a short idling of the gpus. To solve this issue the advanced algorithms implement an overlap of comms and compute. Until now we approached the problem as one single transmission, but in reality each model is made of many layers and each layer can transmit the gradients it has computed, while the next layer is computing its gradients. So if you look at the level of the model, what happens in the backward path is:\n[   compute   ][   compute   ][   compute   ]\n               [comms]        [comms]        [comms]\n---------------------------------------------&gt; time\n&lt;- layer -1 -&gt;|&lt;- layer -2 -&gt;|&lt;- layer -3 -&gt;|\nso once the last layer (-1) computed its gradients it all-reduces them while the 2nd to last layer performs its backward, and so on, until the first layer finished with gradients and it finally sends its gradients out.\nSo now you understand how overlapping works, So we can now update our bigger picture diagram to be:\nNow our timing diagram becomes very similar to the diagram we had for a single gpu:\n[DL][  compute  ][DL][  compute  ][DL][  compute  ]\n[  comms ]       [  comms]        [  comms]\n---------------------------------------------------&gt; time\n|&lt;--iteration--&gt;||&lt;--iteration--&gt;||&lt;--iteration--&gt;|\nand we hope that comms are faster than DL+compute, since if they aren‚Äôt faster than we have the following gpu idling gaps:\n[DL][  compute  ][idle][DL][  compute  ][idle][DL][  compute  ][idle]\n[         comms       ][         comms       ][         comms       ]\n----------------------------------------------------------------------&gt; time\n|&lt;---  iteration  ---&gt;||&lt;---  iteration  ---&gt;||&lt;---  iteration  ---&gt;|\n\n\n\nCalculating TFLOPS answers the question of how long will it take to perform a compute.\nThere is a bit of nomenclature confusion here as TFLOPS as the final s sometimes means sec and at other times just ops.\nFor example, when you read, the A100 spec the TFLOPS there means TeraFloatingPointOperations per second.\nSo let‚Äôs define these abbreviations exactly:\n\nTFLOPS - TeraFLoatingpointOPerations per Second (another way is TFLOP/s)\nTFLOP - TeraFLoatingpointOPerations (or TFLOPs - lower case s but it‚Äôs already confusing)\n\nAlso see the wiki page for more clarifications.\nFor GPT-family of decoder transformers models we can use the math described in this BLOOM-176 docs:\nHere is how many TFLOP are processed per second:\ntflops = model_size_in_B * 4 * 2 * seqlen * global_batch_size / (time_in_sec_per_interation * total_gpus * 1e3)\nThis formula assume one uses activation recomputation which saves GPU memory while introducing a smallish overhead. If one doesn‚Äôt use it then replace 4 with 3 as the model has to do only 1x compute per forward and 2x per backward (since the grads are calculated twice - once for inputs and once for weights). With activation recomputation the forward is done twice and thus you have an additional path which leads to a multiplier of 4 instead of 3\nfootnote: activation recomputation and gradient checkpointing both refer to the same technique.\nso let‚Äôs remove the time component, which will give us the total TFLOP\ntflop = model_size_in_B * 4 * 2 * seqlen * global_batch_size / (total_gpus * 1e3)\nSo let‚Äôs say we have: - seqlen=2048 (sequence length) - global_batch_size=16\nand we already defined: - total_gpus=8 - model_size_in_B=2\nThis gives us:\ntflops = 2 * 4 * 2 * 2048 * 16 / (8 * 1e3) = 65.536 TFLOP\nSo if we do a mixed half-precision training and most of the operations are done in half-precision then we can roughly say that we do 312 TFLOPS on A100 and usually a well optimized framework on a well-tuned hardware will do at least 50% MFU - that is it‚Äôll be able to compute at about 1/2 peak performance.\nfootnote: It‚Äôs a ~3x 989 TFLOPS on H100 (scroll to the end) and also it shows a misleading 2x numbers for sparsity so you have to mentally divide it by 2.\nSo continuing this train of thought it means that the setup will have about 156TFLOPS - and so it‚Äôll take 0.42 secs to process a single iteration (2x forward and 2x backward compute) if we ignore the overhead of the DataLoader (which we hope is close to instant).\nEarlier we said that a typical A100 node has an intra-node NVLink connection of 300GBps, and thus we said that to send 16GB of grads will take 16/300 = 0.053 secs.\nAnd we measured our compute to be 0.42 secs, so here we have a problem as 0.053 &gt; 0.42 so the comms will be slower than compute and the network is a bottleneck.\nYou can now do several thought experiments - for example if you halve the batch size or the sequence length you will halve the compute time.\nfootnote: this is a very rough suggestions since GPUs work the fastest when the matrices they multiple are huge. But this is good enough for a simplified thought experiment we are having here. In reality halving the dimension will not halve the compute time.\nOK, but hopefully at this point it‚Äôs quite clear that if you remain at the boundaries of a single node, you don‚Äôt need to worry about your GPUs idling.\nBut what if you want to speed up the training even more and throw say 4x 8-gpu nodes at it. (and of course you don‚Äôt have a choice but to use multiple nodes if you have a much larger model). Suddenly, the comms can become an even bigger bottleneck.\n\n\n\n\nSo here we are continuing with the idea of 2B param model and we will now use 32 gpus across 4 nodes to speed up the training even more.\nWhile each group of 8 gpus is still connected with super-fast NVLink technology, the inter-node connections are usually in an order of magnitude slower.\nLet‚Äôs say you have a 200Gbps connection. Let‚Äôs repeat the math from the previous section of how long it‚Äôll take to reduce 16GB of gradients.\n16GB is 128Gb, and so at 200Gbps this will take 0.64 seconds.\nAnd if stick to the compute taking 0.42 seconds, here we end up with comms taking longer than compute since 0.64 &gt; 0.42.\nLet‚Äôs bring both use cases together:\n\n\n\nnodes\ncomms\ncompute\ncomms is a bottleneck\n\n\n\n\n1\n0.027\n0.42\nno\n\n\n4\n0.64\n0.42\nyes\n\n\n\non this 200Gbps inter-node setup the comms are 23x slower than the same performed on an intra-node NVlink connections.\nIn this case even though we still have the much faster NVLink connection, we don‚Äôt really benefit from it, since the whole ensemble communicates at the speed of the slowest link. And that slowest link is the inter-node connection.\nSo in this particular situation if you were able to get a 400Gbps inter-node the speed would double and the comms will finish in 0.32 secs and thus will be faster than that 0.42 secs the compute would take.\nfootnote: you will never be able to get the advertised speed fully on the application level, so if it‚Äôs advertised as 400Gbps in the best case expect to get 320Gbps (about 80%). So make sure to take this into the account as well. Moreover, depending on the payload of each collective - the smaller the payload the smaller the actual network throughput will be.\nAnd remember this was all handling a pretty tiny as considered these days 2B param model.\nNow do the same math with 20B and 200B parameter model and you will see that you need to have a much much faster inter-node connectivity to efficiently scale.\n\n\n\nOf course, when we train large models we don‚Äôt use DDP, because we simply can‚Äôt fit the whole model on a single gpu so various other techniques are used. The details are discussed in a dedicated chapter on Model Parallelism, but the only important thing to understand immediately is that all scalability techniques incur a much larger comms overhead, because they all need to communicate a lot more than just gradients. and therefore the amount of traffic on the network can easily grow 3x and more as compared to the DDP protocol overhead we have been exploring so far.\nIt can be difficult to do even approximate math as we did in this chapter, because the actual compute time depends on the efficiency of the chosen framework, how well it was tuned, how fast the DataLoader can feed the batches and many other things, therefore there is no standard MFU that one can use in the math and you will discover your MFU when you configure and run the first few steps of the large model training. and then you will read the Performance chapters and improve your MFU even more.\nAs I have shown in these sections it should be possible to be able to do a back-of-envelope calculations once you understand the specific scalability technique and its networking costs, so that you could know ahead of time which Inter-node network speed you need to require from your acquisition manager. Of course, you also need to understand the particular model architecture and calculate how many TFLOP it will take to do a single iteration.\n\n\n\nMost benchmarking / bandwidth measurement tools will report a unidirectional bandwidth. So be careful when you look at unidirectional vs.¬†bidirectional (duplex) speeds. Typically the latter is ~2x faster.\nIf you measure the bandwidth on your setup and it‚Äôs about 40% of the advertised speed, carefully check if the advertised speed said duplex and if so half that and then your measured bandwidth should now be about 80% which is expected.\ncase study: for a while I couldn‚Äôt understand why when I run the nccl-tests all_reduce benchmark on an A100 node with advertised 600GBps intra-node speed I was getting only 235Gbps (40%) until Horace He kindly pointed out that I should be looking at unidirectional speed which is 300GBps, and then I get 80% of the theoretical spec which checks out.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#intra-node-networking",
    "href": "qmd/network/index.html#intra-node-networking",
    "title": "üõú Network",
    "section": "",
    "text": "There are multiple platforms/solutions out there that provide intra-node networking:\n\nGeneric: PCIe\nNVIDIA: NVLink and NVSwitch\nAMD: Infinity Fabric\nIntel: Gaudi2\n\nfootnote: In the following sections pay close attention that 1 GBps = 8 Gbps.\nfootnote: also pay close attention to when the spec says unidirectional vs bidirectional (duplex) speeds - if you read an online spec and it doesn‚Äôt explicitly declare the directionality - look for an answer. I had to research many docs to figure it out in some of the tables below as some vendors conveniently omit this crucial information. I even had to edit a few wiki pages to add the missing information. Remember that for the vendors the bigger the better so almost always they will use the duplex number, which is 2x larger than unidirectional one.\n\n\nPCIe is a high-speed serial computer expansion bus standard that can be found even on the cheapest computer desktop.\n\n\n\n\n\n\n\n\n\n\nInterconnect\nLane/Direction\nLanes\nUnidirection\nDuplex\n\n\n\n\nPCIe 4\n~2.0 GBps\n16\n31 GBps\n62 GBps\n\n\nPCIe 5\n~4.0 GBps\n16\n63 GBps\n126 GBps\n\n\nPCIe 6\n~7.5 GBps\n16\n121 GBps\n241 GBps\n\n\nPCIe 7\n~15.0 GBps\n16\n242 GBps\n484 GBps\n\n\n\nIf one compares the latest generations of different intra-node technologies (see the following sections) PCIe is usually an order of magnitude behind.\n\n\n\n\nNVLink is a wire-based serial multi-lane near-range communications link developed by Nvidia. Here is the What Is NVLink blog post with more background on it.\n\nI found the wiki pages quite difficult to follow, so I will try to help bring clarity into this.\nEffective payload rate of Intra-node GPU-to-GPU communication hardware:\n\n\n\n\n\n\n\n\n\n\n\nInterconnect\nLane/Direction\nLanes\nLinks\nUnidirection\nDuplex\n\n\n\n\nNVlink 2\n6.250 GBps\n4\n6\n150 GBps\n300 GBps\n\n\nNVlink 3\n6.250 GBps\n4\n12\n300 GBps\n600 GBps\n\n\nNVlink 4\n6.250 GBps\n4\n18\n450 GBps\n900 GBps\n\n\n\nNVlink 2, 3 and 4 use the same hardware of 4 lanes of 6.250 GBps each per link. Each has a unidirectional bandwidth of 25GB/s per link, and therefore 50GB/s per duplex link. The only difference is in the number of links:\n\nNVLink 2 has 6 links =&gt; 25* 6=&gt; 150 GBps unidirectional and 300 GBps bi-directional\nNVLink 3 has 12 links =&gt; 25*12=&gt; 300 GBps unidirectional and 600 GBps bi-directional\nNVLink 4 has 18 links =&gt; 25*18=&gt; 450 GBps unidirectional and 900 GBps bi-directional\n\nThe largest PCIe 16x slot has 16 lanes. Smaller slots have less lanes, 1x == 1 lane.\nAs of this writing NVIDIA Hopper nodes typically come equipped with PCIe 5 and NVLink 4. So there NVlink is 7x faster than PCIe.\nLet‚Äôs look at several examples of nodes and correlate the theory with reality.\nIf you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:\nnvidia-smi topo -m\nand it will tell you how the GPUs are inter-connected.\nOn a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      NV2     0-23            N/A\nGPU1    NV2      X      0-23            N/A\non a different machine w/o NVLink you may see:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      PHB     0-11            N/A\nGPU1    PHB      X      0-11            N/A\nThe report includes this legend:\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\nSo the first report NV2 tells us the GPUs are interconnected with 2 NVLinks, and the second report PHB we have a typical consumer-level PCIe+Bridge setup.\nCheck what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g.¬†NVLink), others slower (e.g.¬†PHB).\nDepending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training.\nNow, let‚Äôs look at the topology of the A100 and H100 nodes:\n\nA100 topology:\n\n$ nvidia-smi topo -m\n      GPU0  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity\nGPU0   X    NV12  NV12  NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU1  NV12   X    NV12  NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU2  NV12  NV12   X    NV12  NV12  NV12  NV12  NV12   0-23         0\nGPU3  NV12  NV12  NV12   X    NV12  NV12  NV12  NV12   0-23         0\nGPU4  NV12  NV12  NV12  NV12   X    NV12  NV12  NV12  24-47         1\nGPU5  NV12  NV12  NV12  NV12  NV12   X    NV12  NV12  24-47         1\nGPU6  NV12  NV12  NV12  NV12  NV12  NV12   X    NV12  24-47         1\nGPU7  NV12  NV12  NV12  NV12  NV12  NV12  NV12   X    24-47         1\nYou can see there are 12 NVLinks and 2 NUMA Groups (2 CPUs w/ 24 cores each)\n\nH100 topology:\n\n$ nvidia-smi topo -m\n      GPU0  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity\nGPU0   X    NV18  NV18  NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU1  NV18   X    NV18  NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU2  NV18  NV18   X    NV18  NV18  NV18  NV18  NV18   0-51         0\nGPU3  NV18  NV18  NV18   X    NV18  NV18  NV18  NV18   0-51         0\nGPU4  NV18  NV18  NV18  NV18   X    NV18  NV18  NV18  52-103        1\nGPU5  NV18  NV18  NV18  NV18  NV18   X    NV18  NV18  52-103        1\nGPU6  NV18  NV18  NV18  NV18  NV18  NV18   X    NV18  52-103        1\nGPU7  NV18  NV18  NV18  NV18  NV18  NV18  NV18   X    52-103        1\nYou can see there are 18 NVLinks and 2 NUMA Groups (2 CPUs w/ 52 cores each)\nOf course, other A100 and H100s node reports may vary, e.g.¬†different number of cpu cores.\n\n\n\nNVSwitch can connect more than 8 GPUs at the speed of NVLink. It‚Äôs advertised to connect up to 256 GPUs in the future generations of the switch.\nThe benefit of connecting more than 8 GPUs at the speed of NVLink is that it allows all-to-all GPU communications at a much faster speed than any intra-node hardware can provide. And with ever increasing compute speeds the network is the likely bottleneck leading to underutilized super-expensive GPUs.\nFor example, in the universe of Tensor Parallelism (Megatron), one doesn‚Äôt use TP degree of more than 8, because TP is only efficient at NVLink speed. ZeRO-DP (Depspeed/FSDP) would also run much faster if the whole cluster uses NVLink speed and involves no slow inter-node connections.\nThe NVIDIA DGX H100 has a 3.6 TBps of full-duplex NVLink Network bandwidth provided by 72 NVLinks (NVLink 4). The normal NVlink 4 has 18 NVLinks (0.9 TBps duplex). So this setup has 4 switches (18*4=72) and therefore 0.9*4=3.6 TBps. Note, that this server has 8 GPUs, so here we get a much faster intra-node communications as compared to the standard NVlink 4.0 which provides only 0.9 TBps all-to-all connectivity for 8 GPUs.\nNVIDIA DGX A100 has 6 switches of 12 NVlinks for a total of 72.\nDGX H100 SuperPOD combines 32 DGX H100 servers, for a total of 256 GPUs. It looks like here they use only half the NVLinks they used for a single DGX H100, so only 1.8 GBps per node, for a total of 57.6 GBps in total.\n\n\n\nAMD MI* Accelerators Intra-node communication is performed by AMD Infinity Fabric, which is also known as xGMI (Socket to Socket Global Memory Interface).\nThis is AMD‚Äôs answer to NVLink.\n\n\n\nInterconnect\nLink/Direction\nLinks\nUnidirection\nDuplex\n\n\n\n\nMI250x\n50 GBps\n7\n350 GBps\n700 GBps\n\n\nMI300x\n64 GBps\n7\n448 GBps\n896 GBps\n\n\n\n\n\n\n\n\n\n\n\n\n\nAMD Infinity Platform Architecture\n\n\nPlatform specs: - MI250X - MI300x\n\n\n\nAccording to Gaudi2 spec, these servers provide 8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps and each card connected with each of the other 7 cards at 262.5 GBps.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#numa-affinity",
    "href": "qmd/network/index.html#numa-affinity",
    "title": "üõú Network",
    "section": "",
    "text": "Non-uniform memory access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. As modern servers have more than one CPU to get the best performance GPUs residing in the same block as the corresponding CPU should have the processes bound to that NUMA node.\nHere is a typical A100 8x GPUs server, as visualized by hwloc:\n\n\n\na100 server numa nodes\n\n\nAs you can see it has 2 CPUs, each defining a NUMA block, and each such block contains a group of 4 GPUs. The GPUs are the grey blocks that say CoProc with 108 compute units (SMs) and 79GB of memory.\nfootnote: was generated by lstopo a100.png\n\n\nnote-to-self: probably belongs in its own chapter?\n\n\nhttps://github.com/open-mpi/hwloc\nThe Hardware Locality (hwloc) software project aims at easing the process of discovering hardware resources in parallel architectures. It offers command-line tools and a C API for consulting these resources, their locality, attributes, and interconnection. hwloc primarily aims at helping high-performance computing (HPC) applications, but is also applicable to any project seeking to exploit code and/or data locality on modern computing platforms.\nDiagnostics: to take a snapshot of the server NUMA topology and save it as an image (supports many other formats)\nlstopo a100.png\nNUMA node binding: hwloc-bind - binding processes, threads and memory\nBind an existing process to a specific NUMA node:\nhwloc-bind --pid 1234 numa:0\nSimilar software: numactl/libnuma\nSome useful suggestions in pytorch docs",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#inter-node-networking",
    "href": "qmd/network/index.html#inter-node-networking",
    "title": "üõú Network",
    "section": "",
    "text": "As inter-node hardware is about of an order of magnitude slower than intra-node hardware in this universe Gbps are used instead of GBps. (1 GBps = 8 Gbps)\nWhen it comes to inter-node networking hardware, there are the well established InfiniBand from NVIDIA and a few other players and there are many new comers that mainly are coming from compute cloud providers who can‚Äôt compete on the slim margin renting out someone else‚Äôs hardware so they build their own (EFA, and others not yet disclosed).\n\n\nElastic Fabric Adapter (EFA) is a recent technology created by AWS.\n\nEFA v1 0.4 Tbps (effective 340 Gbps for all_reduce tests) (P4 AWS instances)\nEFA v2 3.2 Tbps (since Q3-2023, P5 AWS instances)\n\n\n\n\nNow InfiniBand (IB) has been around for a few decades so there are many available configurations that can be found out there. So that if someone says they have InfiniBand that is insufficient information. What you need to know is the signaling rate and the number of IB links.\nHere are the most recent signaling rates which you are likely to see in the current hardware offerings:\nSignaling rate of uni-directional links in Gbps: | Links | EDR | HDR | NDR | XDR | GDR | | ‚Äî-: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | | 1 | 25 | 50 | 100 | 200 | 400 | | 4 | 100 | 200 | 400 | 800 | 1600 | | 8 | 200 | 400 | 800 | 1600 | 3200 | | 12 | 300 | 600 | 1200 | 2400 | 4800 |\nLatency in usecs: | EDR | HDR | NDR | XDR | GDR | | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | ‚Äì: | | 0.5 | 0.6 | ?? | ?? | ?? |\n?? = NDR and later didn‚Äôt publish latency data\nInfiniBand provides RDMA.\nHere are some examples of NVIDIA devices with the fastest IB:\n\nOne configuration of NVIDIA DGX H100 comes with 8x NVIDIA ConnectX-7 Ethernet/InfiniBand ports each of 200Gbps, for a total of 1.6 Gbps to connect with other DGX servers.\nFor DGX H100 SuperPOD the ConnectX-7s across all 32 DGX servers and associated InfiniBand switches provide 25.6 TBps of full duplex bandwidth for use within the pod or for scaling out the multiple SuperPODs - that is an equivalent of 0.8 TBps per node (6.4Tbps!).\n\n\n\n\nAccording to Gaudi2 spec, these servers provide 24 NICs of 100GbE RoCE v2 ROMA for a total of 2.4Tbps of inter-node connectivity with other Gaudi2 servers.\n\n\n\nHPE Slingshot interconnect seems to be used by HPCs. As of this writing it provides 200Gbps per link. Some HPCs use 4 of those links to build 800Gbps interconnects, and, of course, with more links will deliver a higher overall bandwidth.\n\n\n\nOmniPath Architecture (OPA). Originally by Intel, the technology got sold to Cornelis Networks.\ncase study: I used this technology at JeanZay HPC in France in 2022. It was only 135Gbps and while the vendor tried to fix it a year later it was still the same speed. Hopefully the issue has been resolved and the speed is much faster nowadays. Because it was so slow we had to use Megatron-Deepspeed for training BLOOM-176B instead of the much easier to use DeepSpeed ZeRO).\nAs of this writing I see that the product comes with either 100 or 200Gbps bandwidth. So it‚Äôs unlikely you will see anybody offering this solution for ML workloads, unless they manage to install many NICs perhaps?\nOmni-Path provides RDMA.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/index.html#important-nuances",
    "href": "qmd/network/index.html#important-nuances",
    "title": "üõú Network",
    "section": "",
    "text": "The network throughput in the advertised spec and the actual throughput will never be the same. In the best case you can expect about 80-90% of the advertised spec.\nThen the network throughput will depend on the size of payload being sent during each communication. The higher the payload the higher the throughput will be.\nLet‚Äôs demonstrate this using nccl-tests on a single A100 node\n$ ./build/all_reduce_perf -b 32k -e 16G -f 2 -g 8 -n 50\n[...]\n           size    time   algbw   busbw\n            (B)    (us)  (GB/s)  (GB/s)\n         32_768   43.83    0.75    1.31\n         65_536   46.80    1.40    2.45\n        131_072   51.76    2.53    4.43\n        262_144   61.38    4.27    7.47\n        524_288   80.40    6.52   11.41\n       1048_576   101.9   10.29   18.00\n       2097_152   101.4   20.68   36.18\n      4_194_304   101.5   41.33   72.33\n      8_388_608   133.5   62.82  109.93\n     16_777_216   276.6   60.66  106.16\n     33_554_432   424.0   79.14  138.49\n     67_108_864   684.6   98.02  171.54\n    134_217_728  1327.6  101.10  176.92\n    268_435_456  2420.6  110.90  194.07\n    536_870_912  4218.4  127.27  222.72\n  1_073_741_824  8203.9  130.88  229.04\n  2_147_483_648   16240  132.23  231.41\n  4_294_967_296   32136  133.65  233.88\n  8_589_934_592   64074  134.06  234.61\n 17_179_869_184  127997  134.22  234.89\nfootnote: I massaged the output to remove unwanted columns and made the size more human readable\nThis benchmark run an all_reduce collective for various payload sizes from 32KB to 16GB. The value that we care about is the busbw - this column tells us the real network throughput as explained here.\nAs you can see for payloads smaller than 8MB the throughput is very low - and it starts saturating around payload size of 536MB. It‚Äôs mostly because of latency. Reducing a single 4GB payload is much faster than 1000x 4MB payloads.\nHere is a benchmark that demonstrates that: all_reduce_latency_comp.py. Let‚Äôs run it on the same A100 node:\n$ python -u -m torch.distributed.run --nproc_per_node=8 all_reduce_latency_comp.py\n\n----------- 1x 4.0GB ----------------\n busbw: 1257.165 Gbps\n\n----------- 1000x 0.004GB ----------------\n busbw: 374.391 Gbps\nIt‚Äôs easy to see that it‚Äôs about 3x slower in this particular case to send the same payload but in 1000 smaller chunks.\nSo when you calculate how long does it take to all_reduce a given payload size, you need to use the corresponding busbw entry (after of course you have run this benchmark on your particular hardware/environment).\nFiguring out the payload can be tricky since it‚Äôd depend on the implementation of the framework. Some implementations will reduce each weight‚Äôs gradient alone which obvious would lead to a very small payload and the network will be very slow. Other implementations bucket multiple gradients together before reducing those, increasing the payload and minimizing the latency impact.\nBut let‚Äôs go back to the benchmark results table. This test was done on an A100 node that runs NVLink advertised as uni-directional 300GBs so we get about 78% of the theoretical speed with 17GB payload and more than that the benchmark crashes. It can be seen from the last few rows of the table that not much more can be squeezed.\nWe can also run p2pBandwidthLatencyTest which performs a low-level p2p benchmark:\n./p2pBandwidthLatencyTest\n[...]\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3      4      5      6      7\n     0 1581.48 274.55 275.92 272.02 275.35 275.28 273.62 273.20\n     1 274.70 1581.48 275.33 272.83 275.38 273.70 273.45 273.70\n     2 274.81 276.90 1594.39 272.66 275.39 275.79 273.97 273.94\n     3 273.25 274.87 272.12 1545.50 274.38 274.37 274.22 274.38\n     4 274.24 275.15 273.44 271.57 1584.69 275.76 275.04 273.49\n     5 274.37 275.77 273.53 270.84 274.59 1583.08 276.04 273.74\n     6 275.61 274.86 275.47 273.19 272.58 275.69 1586.29 274.76\n     7 275.26 275.46 275.49 273.61 275.50 273.28 272.24 1591.14\n[...]\nAs you can see in the Unidirectional section of the report we do get 274 GBps out of the advertised 300GBps (~91%).\nPlease note that when I re-run this same test on H100s (NVLink 4.0) I got a much worse efficiency:\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3      4      5      6      7\n     0 2494.51 364.13 375.99 378.03 376.77 376.71 374.85 375.66\n     1 375.18 2533.95 376.08 374.98 376.21 375.96 375.76 375.12\n     2 363.43 393.28 2532.67 376.35 377.14 376.47 375.76 375.48\n     3 369.90 375.92 393.63 2525.38 376.58 375.88 376.13 377.01\n     4 376.20 376.28 375.20 393.52 2526.02 375.82 375.05 376.10\n     5 376.26 376.60 375.54 375.52 376.81 2521.18 376.37 376.60\n     6 374.31 376.19 376.80 376.32 376.83 376.44 2529.85 376.39\n     7 376.17 376.49 376.53 374.95 376.30 376.82 375.71 2519.78\nSo 376GBps out of 450GBps is 83% (not very good).\nBottom line - in this particular setup: 1. if you have huge payloads you will be able to use about 80% of the advertised 300GBps 2. if the payload of each communication is smallish it could be far far lower.\nThis graph is also helpful to demonstrate how the actual bandwidth changes with the size of the message:\n (source)\nAnother tool for bandwidth measurements on NVIDIA GPUs is NVIDIA/nvbandwidth.\n\n\n\n (source)\nXXX: integrate/expand\n\n\n\nProprietary network hardware vendors like AWS (EFA) don‚Äôt disclose their secrets and therefore the public libraries like nccl cannot support those out of the box. These vendors have to supply their own versions of the network collective libraries to be used by users of their hardware.\nOriginally proprietary hardware vendors used the trick of telling the users to use LD_LIBRARY_PATH and/or LD_PRELOAD to dynamically overload libnccl.so to get their custom version loaded into PyTorch or another framework. But recently NCCL developed a NCCL Net Plugin which should be used now instead. This feature was added in NCCL v2.12.\nNow, when NCCL is initialized, it will look for a libnccl-net.so library and dynamically load it, then look for symbols inside the library. That‚Äôs where proprietary hardware vendors should now put their custom APIs. This library, of course, should still be either in LD_LIBRARY_PATH or the /etc/ld.so.conf config.\nFor more information about dynamic library loading see this section.\n\n\n\nIf you get 2 random nodes from the cloud they may not reside on the same subnet and there will be an additional latency incurred for all transmissions.\nYou want to make sure that the nodes used for a single training all reside on the same subnet/spine so they are all one hop away from each other.\nWhen you plan to eventually have a large cluster but starting small make sure that your provider can expand the cluster while keeping all the nodes close to each other.\nHere are the cloud-specific ways of accomplishing node proximity:\n\nAzure: availability set\nGCP: compact placement policies\n\nDepending on the type of package you have or what type of machines you rent - you may or may not be able to use those.\n\n\n\nIf you use a shared HPC environment, or even if you have your own cluster but sharing it with your colleagues expect the network bandwidth to be unreliable and fluctuate at different time of the day.\nThis situation unfortunately makes it extremely difficult to finetune the performance of your training setup. Since every time you run a test the TFLOPs will vary, so how do you do the optimization? Unfortunately I don‚Äôt have a magic trick here. If you have a working solution please kindly share.\ncase study: we had this issue at JeanZay HPC when we were doing preliminary experiments before we started training BLOOM-176B. As that HPC has many users it was pretty much impossible to do speed optimizations, as even running the exact same setup again and again gave different throughput results. Luckily just before we launched BLOOM-176B training we were given an exclusive access to the new at that time A100 partition so we were the only users and we were able to greatly optimize the throughput.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network"
    ]
  },
  {
    "objectID": "qmd/network/benchmarks/index.html",
    "href": "qmd/network/benchmarks/index.html",
    "title": "",
    "section": "",
    "text": "üõú NetworkNetworking Benchmarks",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network",
      "Networking Benchmarks"
    ]
  },
  {
    "objectID": "qmd/network/benchmarks/index.html#crucial-reproducibility-requirements",
    "href": "qmd/network/benchmarks/index.html#crucial-reproducibility-requirements",
    "title": "",
    "section": "Crucial reproducibility requirements",
    "text": "Crucial reproducibility requirements\nThe most important requirements for a series of successful experiments is to be able to reproduce the experiment environment again and again while changing only one or a few setup variables.\nTherefore when you try to figure out whether some change will improve performance or make it worse, you must figure out how to keep things stable.\nFor example, you need to find a way to prevent the network usage from fluctuations. When we were doing performance optimizations for 108B pre-BLOOM experiments it was close to impossible to perform, since we were on a shared internode network and the exact same setup would yield different throughput depending on how many other users used the network. It was not working. During BLOOM-176B we were given a dedicated SLURM partition with an isolated network where the only traffic was ours. Doing the performance optimization in such environment was just perfect.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network",
      "Networking Benchmarks"
    ]
  },
  {
    "objectID": "qmd/network/benchmarks/index.html#network-throughput",
    "href": "qmd/network/benchmarks/index.html#network-throughput",
    "title": "",
    "section": "Network throughput",
    "text": "Network throughput\nIt‚Äôs critical to understand your particular model size and framework requirements with regard to network bandwidth, throughput and latency. If you underpay for network you will end up having idle gpus and thus you wasted money and time. If you overpay for very fast network, but your gpus are slow, then again you wasted money and time.\nIf your network is very slow, your training is likely to be network-bound and many improvements in the training setup will not help with the improving performance.\nNote: The EAI cookbook contains a set of communication benchmarks for each collective that you can use to quickly measure the throughput of your internode or intranode network.\nHere is a simple all-reduce benchmark that you can use to quickly measure the throughput of your internode network:\nall_reduce_bench.py\nUsually benchmarking at least 4 nodes is recommended, but, of course, if you already have access to all the nodes you will be using during the training, benchmark using all of the nodes.\nTo run it on 4 nodes:\nGPUS_PER_NODE=8\nNNODES=4\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=6000\npython -u -m torch.distributed.run \\\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\\n    --rdzv_backend c10d \\\n    --max_restarts 0 \\\n    --role `hostname -s`: \\\n    --tee 3 \\\n    all_reduce_bench.py\nNotes: - adapt MASTER_ADDR to rank 0 hostname if it‚Äôs not a SLURM environment where it‚Äôs derived automatically.\nHere is how to run launch it in a SLURM env with 4 nodes:\nsalloc --partition=mypartition --nodes=4 --ntasks-per-node=1 --cpus-per-task=48 --gres=gpu:8 --time=1:00:00 bash\nsrun --gres=gpu:8 --nodes=4 --tasks-per-node=1 python -u -m torch.distributed.run --nproc_per_node=8 --nnodes 4 --rdzv_endpoint $(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1):6000 --rdzv_backend c10d all_reduce_bench.py\nNotes: - You are likely to need to adapt --cpus-per-task and --partition arguments there. - You do salloc once and then can repeat srun multiple times on the same allocation.\nYou may get results anywhere between 5Gbps and 1600Gbps (as of this writing). The minimal speed to prevent being network bound will depend on your particular training framework, but typically you‚Äôd want at least 400Gbps or higher. Though we trained BLOOM on 50Gbps.\nFrameworks that shard weights and optim stages like Deepspeed w/ ZeRO Stage-3 do a lot more traffic than frameworks like Megatron-Deepspeed which do tensor and pipeline parallelism in addition to data parallelism. The latter ones only send activations across and thus don‚Äôt need as much bandwidth. But they are much more complicated to set up and run.\nOf course, an efficient framework will overlap communications and compute, so that while one stage is fetching data, the other stage in parallel runs computations. So as long as the communication overhead is smaller than compute the network requirements are satisfied and don‚Äôt have to be super fantastic.\nTo get reasonable GPU throughput when training at scale (64+GPUs) with DeepSpeed ZeRO Stage 3 with V100s\n\n100Gbps is not enough\n200-400 Gbps is ok\n800-1000 Gbps is ideal\n\nfull details\nOf course, the requirements are higher for A100 gpu nodes and even higher for H100s (but no such benchmark information has been shared yet).",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üõú Network",
      "Networking Benchmarks"
    ]
  },
  {
    "objectID": "qmd/debug/index.html",
    "href": "qmd/debug/index.html",
    "title": "",
    "section": "",
    "text": "QmdDebugging and Troubleshooting",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Debugging and Troubleshooting"
    ]
  },
  {
    "objectID": "qmd/debug/index.html#guides",
    "href": "qmd/debug/index.html#guides",
    "title": "üêõ Debugging",
    "section": "Guides",
    "text": "Guides\n\nDebugging PyTorch programs\nDiagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs\nTroubleshooting NVIDIA GPUs\nUnderflow and Overflow Detection\nNCCL Debug and Performance - notes for debugging NCCL-based software and tuning it up for the peak performance",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging"
    ]
  },
  {
    "objectID": "qmd/debug/index.html#tools",
    "href": "qmd/debug/index.html#tools",
    "title": "üêõ Debugging",
    "section": "Tools",
    "text": "Tools\n\nDebug Tools\ntorch-distributed-gpu-test.py - this a torch.distributed diagnostics script that checks that all GPUs in the cluster (one or many nodes) can talk to each other and allocate gpu memory.\nNicerTrace - this is an improved trace python module with multiple additional flags added to the constructor and more useful output.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üêõ  Debugging"
    ]
  },
  {
    "objectID": "qmd/compute/cpu/index.html",
    "href": "qmd/compute/cpu/index.html",
    "title": "CPU",
    "section": "",
    "text": "XXX: This chapter needs a lot more work",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU"
    ]
  },
  {
    "objectID": "qmd/compute/cpu/index.html#how-many-cpu-cores-do-you-need",
    "href": "qmd/compute/cpu/index.html#how-many-cpu-cores-do-you-need",
    "title": "CPU",
    "section": "How many cpu cores do you need",
    "text": "How many cpu cores do you need\nPer 1 gpu you need:\n\n1 cpu core per process that is tied to the gpu\n1 cpu core for each DataLoader worker process - and you need 2-4 workers.\n\n2 workers is usually plenty for NLP, especially if the data is preprocessed\nIf you need to do dynamic transforms, which is often the case with computer vision models, you may need 3-4 and sometimes more workers.\nThe goal is to be able to pull from the DataLoader instantly, and not block the GPU‚Äôs compute, which means that you need to pre-process a bunch of samples for the next iteration, while the current iteration is running. In other words your next batch needs to take no longer than a single iteration GPU compute of the batch of the same size.\nBesides preprocessing if you‚Äôre pulling dynamically from the cloud instead of local storage you also need to make sure that the data is pre-fetched fast enough to feed the workers that feed the gpu furnace.\nMultiply that by the number of GPUs, add a few cores for the Operation system (let‚Äôs say 4).\nIf the node has 8 gpus, and you have n_workers, then you need 8*(num_workers+1)+4. If you‚Äôre doing NLP, it‚Äôd be usually about 2 workers per gpu, so 8*(2+1)+4 =&gt; 28 cpu cores. If you do CV training, and, say, you need 4 workers per gpu, then it‚Äôd be 8(4+1)+4 =&gt; 44 cpu cores.\nWhat happens if you have more very active processes than the total number of cpu cores? Some processes will get preempted (put in the queue for when cpu cores become available) and you absolutely want to avoid any context switching.\nBut modern cloud offerings typically have 48+ cpu-cores so usually there is no problem to have enough cores to go around.\n\nCPU offload\nSome frameworks, like Deepspeed can offload some compute work to CPU without creating an bottleneck. In which case you‚Äôd want additional cpu-cores.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU"
    ]
  },
  {
    "objectID": "qmd/compute/cpu/index.html#hyperthreads",
    "href": "qmd/compute/cpu/index.html#hyperthreads",
    "title": "CPU",
    "section": "Hyperthreads",
    "text": "Hyperthreads\nDoubles the cpu cores number\nXXX:",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html",
    "href": "qmd/compute/accelerator/index.html",
    "title": "Accelerators",
    "section": "",
    "text": "XXX: This chapter is a super-early WIP\nCompute accelerators are the workhorses of the ML training. At the beginning there were just GPUs. But now there are also TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs and more are being invented.\nThere exist two main ML workloads - training and inference. There is also the finetuning workload which is usually the same as training, unless a much lighter LORA-style finetuning is performed. The latter requires significantly fewer resources and time than normal finetuning.\nIn language models during inference the generation is performed in a sequence - one token at a time. So it has to repeat the same forward call thousands of times one smallish matmul (matrix multiplication or GEMM) at a time. And this can be done on either an accelerator, like GPU, or some of the most recent CPUs, that can handle inference quite efficiently.\nDuring training the whole sequence length is processed in one huge matmul operation. So if the sequence length is 4k long, the training of the same model will require a compute unit that can handle 4k times more operations than inference and do it fast. Accelerators excel at this task. In fact the larger the matrices they have to multiply, the more efficient the compute.\nThe other computational difference is that while both training and inference have to perform the same total amount of matmuls in the forward pass, in the backward pass, which is only done for training, an additional 2x times of matmuls is done to calculate the gradients with regards to inputs and weights. And an additional forward is performed if activations recomputation is used. Therefore the training process requires at 3-4x more matmuls than inference.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#subsections",
    "href": "qmd/compute/accelerator/index.html#subsections",
    "title": "Accelerators",
    "section": "Subsections",
    "text": "Subsections\n\nTroubleshooting NVIDIA GPUs",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#birds-eye-view-on-the-high-end-accelerator-reality",
    "href": "qmd/compute/accelerator/index.html#birds-eye-view-on-the-high-end-accelerator-reality",
    "title": "Accelerators",
    "section": "Bird‚Äôs eye view on the high end accelerator reality",
    "text": "Bird‚Äôs eye view on the high end accelerator reality\nWhile this might be changing in the future, unlike the consumer GPU market, as of this writing there aren‚Äôt that many high end accelerators, and if you rent on the cloud, most providers will have more or less the same few GPUs to offer.\nGPUs: - As of today, ML clouds/HPCs started transitioning from NVIDIA A100s to H100s and this is going to take some months due to the usual shortage of NVIDIA GPUs. - AMD‚Äôs MI250 started popping up here and there, but it‚Äôs unclear when it‚Äôll be easy to access those. From a recent discussion with an AMD representative MI300 is not planned to be in general availability until some time in 2025, though some HPCs already plan to get them some time in 2024.\nHPU: - Intel‚Äôs Gaudi2 are starting to slowly emerge on Intel‚Äôs cloud\nIPU: - And there is Graphcore with their IPU offering. You can try these out in Paperspace through their cloud notebooks.\nTPU: - Google‚Äôs TPUs are, of course, available but they aren‚Äôt the most desirable accelerators because you can only rent them, and the software isn‚Äôt quite easily convertible between GPUs and TPUs, and so many (most?) developers remain in the GPU land, since they don‚Äôt want to be locked into a hardware which is a Google monopoly.\nPods and racks: - Cerebras‚Äô WaferScale Engine (WSE) - SambaNova‚Äôs DataScale - dozens of different pod and rack configs that compose the aforementioned GPUs with super-fast interconnects.\nThat‚Äôs about it as Q4-2023.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#glossary",
    "href": "qmd/compute/accelerator/index.html#glossary",
    "title": "Accelerators",
    "section": "Glossary",
    "text": "Glossary\n\nCPU: Central Processing Unit\nFPGA: Field Programmable Gate Arrays\nGPU: Graphics Processing Unit\nHBM: High Bandwidth Memory\nHPC: High-performance Computing\nHPU: Habana Gaudi AI Processor Unit\nIPU: Intelligence Processing Unit\nMME: Matrix Multiplication Engine\nQPU: Quantum Processing Unit\nRDU: Reconfigurable Dataflow Unit\nTPU: Tensor Processing Unit",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#the-most-important-thing-to-understand",
    "href": "qmd/compute/accelerator/index.html#the-most-important-thing-to-understand",
    "title": "Accelerators",
    "section": "The most important thing to understand",
    "text": "The most important thing to understand\nI will make the following statement multiple times in this book - and that it‚Äôs not enough to buy/rent the most expensive accelerators and expect a high return on investment (ROI).\nThe two metrics for a high ROI for ML training are: 1. the speed at which the training will finish, because if the training takes 2-3x longer than planned, your model could become irrelevant before it was released - time is everything in the current super-competitive ML market. 2. the total $$ spent to train the model, because if the training takes 2-3x longer than planned, you will end up spending 2-3x times more.\nUnless the rest of the purchased/rented hardware isn‚Äôt chosen carefully to match the required workload chances are very high that the accelerators will idle a lot and both time and $$ will be lost. The most critical component is network, then storage, and the least critical ones are (CPU and CPU memory).\nIf the compute is rented one usually doesn‚Äôt have the freedom to choose - the hardware is either set in stone or some components might be replaceable but with not too many choices. Thus there are times when the chosen cloud provider doesn‚Äôt provide a sufficiently well matched hardware, in which case it‚Äôs best to seek out a different provider.\nIf you purchase your servers then I recommend to perform a very indepth due diligence before buying.\nBesides hardware, you, of course, need software that can efficiently deploy the hardware.\nWe will discuss both the hardware and the software aspects in various chapters of this book. You may want to start here and here.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#what-accelerator-characteristics-do-we-care-for",
    "href": "qmd/compute/accelerator/index.html#what-accelerator-characteristics-do-we-care-for",
    "title": "Accelerators",
    "section": "What Accelerator characteristics do we care for",
    "text": "What Accelerator characteristics do we care for\nLet‚Äôs use the NVIDIA A100 spec as a reference point in the following sections.\n\n\n\nnvidia-a100-spec\n\n\nsource\n\nTFLOPS\nAs mentioned earlier most of the work that ML training and inference do is matrix multiplication. If you remember your algebra matrix multiplication is made of many multiplications followed by summation. Each of these computations can be counted and define how many of these operations can be performed by the chip in a single seconds.\nThis is one of the key characteristics that the accelerators are judged by. The term TFLOPS defines how many trillions of FloatingPointOperations the chip can perform in a second. The more the better. There is a different definition for different data types. For example, here are a few entries for A100:\n\n\n\nData type\nTFLOPS\nw/ Sparsity\n\n\n\n\nFP32\n19.5\nn/a\n\n\nTensor Float 32 (TF32)\n156\n312\n\n\nBFLOAT16 Tensor Core\n312\n624\n\n\nFP16 Tensor Core\n312\n624\n\n\nINT8 Tensor Core\n624\n1248\n\n\n\nfootnote: INT8 is measured in TeraOperations as it‚Äôs not a floating operation.\nfootnote: the term FLOPS could mean either the total number of FloatingPointOperations, e.g.¬†when counting how many FLOPS a single Transformer iteration takes, and it could also mean FloatingPointOperations per second - so watch out for the context. When you read an accelerator spec it‚Äôs almost always a per second definition. When model architectures are discussed it‚Äôs usually just the total number of FloatingPointOperations.\nSo you can see that int8 is 2x faster than bf16 which in turn is 2x faster than tf32.\nMoreover, the TFLOPs depend on the matrices size as can be seen from this table:\n\n\n\nnvidia-a100-matmul-tflops\n\n\nsource\nAs you can see the difference in performance is non-linear due to the tile and wave quantization effects.\nLet‚Äôs look at the TFLOPS specs across the high end accelerators:\n\n\n\nAccelerator / TFLOPS\nfp32\nfp16\nfp8\nint8\n\n\n\n\nNVIDIA A100 SXM\n19.5\n312\n624\n624\n\n\nAMD MI250\n45.3\n362\nX\n362\n\n\nAMD MI250X\n47.9\n383\nX\n383\n\n\n\n\n\n\n\n\n\nNVIDIA H100 SXM\n67.0\n989\n1979\n1979\n\n\nNVIDIA H100 PCIe\n51.0\n756\n1513\n1513\n\n\nNVIDIA H100 dual NVL\n134.0\n989\n3958\n3958\n\n\nAMD MI300\n?\n?\n?\n?\n\n\n\n\n\n\n\n\n\n\n\nIntel Gaudi2 doesn‚Äôt plan to publish TFLOPS specs as of this writing\n\n\nAchievable peak TFLOPS\nThe problem with the advertised peak TFLOPS is that they are very theoretical and can‚Äôt be achieved in practice even if all the perfect conditions have been provided. Each accelerator has its own realistic TFLOPS which is not advertised and there are anecdotal community reports that do their best to find the actual best value, but I‚Äôm yet to find any official reports.\nIf you find solid reports (papers?) showing the actual TFLOPS one can expect from one or more of the high end accelerators discussed in this chapter please kindly submit a PR with this information. The key is to have a reference to a source that the reader can validate the proposed information with.\nTo provide a numerical sense to what I‚Äôm talking about is let‚Äôs take A100 with its 312 TFLOPS bf16 peak performance in the specs of this card. Until the invent of FlashAttention it was known that 150TFLOPS was close to the highest one could get for half precision mixed precision, with FlashAttention, it‚Äôs around 180TFLOPS. This is, of course, measured for training LLMs where the network and IO are involved which create additional overheads. So here the peak performance probably lays somewhere between 200 and 300 TFLOPS.\nIt should be possible to calculate the actual peak TFLOPS by doing a perfectly aligned max-size matrices matmul measured on a single accelerator.\nXXX: write a small program to do exactly dynamically figuring out the perfect shapes based on the tile and wave quantization effects and max sizes (how?) so that the benchmark isn‚Äôt hardcoded to a particular accelerator.\n\n\n\nAccelerator memory size and speed\nTypically the more on-chip memory the accelerator has the better. At any given time usually most of the model weights aren‚Äôt being used as they wait for their turn to be processed and thus large memory allows more of the model to be on the accelerator memory and immediately available for access and update. When there is not enough memory, sometimes the model has to be split across multiple accelerators, or offloaded to CPU and/or disk.\nCurrent high end accelerators (some aren‚Äôt GA yet):\n\n\n\nAccelerator\nMemory in GBs\nType\nSpeed in TB/s\n\n\n\n\nNVIDIA A100 SXM\n80\nHBM2e\n2\n\n\nNVIDIA H100 SXM\n80\nHBM3\n3.35\n\n\nNVIDIA H100 PCIe\n80\nHBM3\n2\n\n\nNVIDIA H100 dual NVL\n188\nHBM3\n7.8\n\n\nAMD MI250\n128\nHBM2e\n3.28\n\n\nAMD MI250X\n128\nHBM2e\n3.28\n\n\nAMD MI300\n192\nHBM3\n\n\n\n\n\n\n\n\n\n\n\nXXX: add other accelerators\n\nMemory speed is, of course, very important since if it‚Äôs not fast enough than the compute ends up idling waiting for the data to be moved to and from the memory.\nThe GPUs use High Bandwidth Memory (HBM) which is a 3D version of SDRAM memory. For example, A100-SXM comes with HBM2 at 1.6TB/sec, and H100-SXM comes with HBM3 at 3.35TB/s.\n\n\nHeat\nThis is of interest when you buy your own hardware, when you rent on the cloud the provider hopefully takes care of adequate cooling.\nThe only important practical understanding for heat is that if the accelerators aren‚Äôt kept cool they will throttle their compute clock and slow everything down (and could even crash sometimes, albeit throttling is supposed to prevent that).",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#high-end-accelerators-for-llmvlm-workloads",
    "href": "qmd/compute/accelerator/index.html#high-end-accelerators-for-llmvlm-workloads",
    "title": "Accelerators",
    "section": "High end accelerators for LLM/VLM workloads",
    "text": "High end accelerators for LLM/VLM workloads\n\nCloud and in-house accelerators\nMost common accelerators that can be either rented on compute clouds or purchased:\nNVIDIA: - A100 - huge availability but already getting outdated. - H100 - 2-3x faster than A100 (half precision), 6x faster for fp8, slowly emerging on all major clouds. - GH200 - 2 chips on one card - (1) H100 w/ 96GB HBM3 or 144GB HBM3e + (2) Grace CPU w/ 624GB RAM - availability is unknown.\nAMD: - MI250 ~= A100 - very few clouds have them - MI300 ~= H100 - don‚Äôt expect until late-2024 or even 2025 to be GA\nIntel: - Gaudi2 ~= H100 - Currently there is a very low availability on cloud.google.com with a long waiting list which supposedly should be reduced in Q1-2024. AWS has the older Gaudi1 via DL1 instances.\nGraphcore: - IPU - available via Paperspace\nSambaNova: - DataScale SN30\n\n\nIn-house accelerator clusters\nCerebras: - clusters - systems based on WaferScale Engine (WSE).\n\n\nCloud-only solutions\nThese can be only used via clouds:\nGoogle - TPUs - lock-in, can‚Äôt switch to another vendor like NVIDIA -&gt; AMD\nCerebras: - Cloud\n\n\nPrices\nRemember that the advertised prices are almost always open to negotiations as long as you‚Äôre willing to buy/rent in bulk and if renting then for a long time (i.e.¬†years!). When do you will discover that the actual price that you end up paying could be many times less than the original public price. Some cloud providers already include the discount as you choose a longer commitment on their website, but it‚Äôs always the best to negotiate directly with their sales team. In addition or instead of a $$-discount you could be offered some useful features/upgrades for free.\nFor the baseline prices it should be easy to find a few good sites that provide an up-to-date public price comparisons across clouds - just search for something like cloud gpu pricing comparison.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#accelerators-in-detail",
    "href": "qmd/compute/accelerator/index.html#accelerators-in-detail",
    "title": "Accelerators",
    "section": "Accelerators in detail",
    "text": "Accelerators in detail\n\nNVIDIA\nAbbreviations:\n\nCUDA: Compute Unified Device Architecture (proprietary to NVIDIA)\n\nNVIDIA-specific key GPU characteristics: - CUDA Cores - similar to CPU cores, but unlike CPUs that typically have 10-100 powerful cores, CUDA Cores are weaker and come in thousands and allow to perform massive general purpose computations (parallelization). Like CPU cores CUDA Cores perform a single operation in each clock cycle. - Tensor Cores - special compute units that are designed specifically to perform fast multiplication and addition operations like matrix multiplication. These perform multiple operations in each clock cycle. They can execute extremely fast computations on low or mixed precision data types with some loss (fp16, bf16, tf32, fp8, etc.). These cores are specifically designed for ML workloads. - Streaming Multiprocessors (SM) are clusters of CUDA Cores, Tensor Cores and other components.\nFor example, A100-80GB has:\n\n6912 CUDA Cores\n432 Tensor Cores (Gen 3)\n108 Streaming Multiprocessors (SM)\n\n\n\nAMD\nAMD-specific key GPU characteristics: - Stream Processors - are similar in functionality to CUDA Cores - that is these are the parallel computation units. But they aren‚Äôt the same, so one can‚Äôt compare 2 gpus by just comparing the number of CUDA Cores vs the number of Stream Processors. - Compute Units - are clusters of Stream Processors and other components\nfor example, AMD MI250 has: - 13,312 Stream Processors - 208 Compute Units\n\n\nIntel Gaudi2\nArchitecture\n\n24x 100 Gigabit Ethernet (RoCEv2) integrated on chip - 21 of which are used for intra-node and 3 for inter-node (so 21*8=168 cards for intra-node (262.5GBps per GPU), and 3*8=24 cards for inter-node (2.4Tbps between nodes)\n96GB HBM2E memory on board w/2.45 TBps bandwidth per chip, for a total of 768GB per node\n\nA server/node is built from 8 GPUs, which can then be expanded with racks of those servers.\nThere are no official TFLOPS information published (and from talking to an Intel representative they have no intention to publish any.) They publish the [following benchmarks](https://developer.habana.ai/resources/habana-models-performance/ but I‚Äôm not sure how these can be used to compare this compute to other providers.\nComparison: supposedly Gaudi2 competes with NVIDIA H100",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#api",
    "href": "qmd/compute/accelerator/index.html#api",
    "title": "Accelerators",
    "section": "API",
    "text": "API\n\nNVIDIA\nuses CUDA\n\n\nAMD\nuses ROCm\n\n\nIntel Gaudi\nThe API is via Habana SynapseAI¬Æ SDK which supports PyTorch and TensorFlow.\nUseful integrations: - HF Optimum Habana which also includes - DeepSpeed integration.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#apples-to-apples-comparison",
    "href": "qmd/compute/accelerator/index.html#apples-to-apples-comparison",
    "title": "Accelerators",
    "section": "Apples-to-apples Comparison",
    "text": "Apples-to-apples Comparison\nIt‚Äôs very difficult to compare specs of different offerings since marketing tricks get deployed pretty much by all competitors so that one can‚Äôt compare 2 sets of specs and know the actual difference.\n\nMLPerf via MLCommons publishes various hardware benchmarks that measure training, inference, storage and other tasks‚Äô performance. For example, here is the most recent as of this writing training v3.0 and inference v3.1 results.\nExcept I have no idea how to make use of it - it‚Äôs close to impossible to make sense of or control the view. This is a great intention lost in over-engineering and not thinking about how the user will benefit from it, IMHO. For example, I don‚Äôt care about CV data, I only want to quickly see the LLM rows, but I can‚Äôt do it. And then the comparisons are still not apples to apples so how can you possibly make sense of which hardware is better I don‚Äôt know.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/accelerator/index.html#power-and-cooling",
    "href": "qmd/compute/accelerator/index.html#power-and-cooling",
    "title": "Accelerators",
    "section": "Power and Cooling",
    "text": "Power and Cooling\nIt is most likely that you‚Äôre renting your accelerator nodes and someone else is responsible for ensuring they function properly, but if you own the accelerators you do need to know how to supply a sufficient power and adequate cooling.\n\nPower\nSome high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won‚Äôt get the full performance out of your card otherwise.\nEach PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power.\nSome other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power.\nLow end cards may use 6-Pin connectors, which supply up to 75W of power.\nAdditionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak.\nAnd of course the PSU needs to have enough unused Watts to power the card.\n\n\nCooling\nWhen a GPU gets overheated it will start throttling down and will not deliver full performance and it can even shutdown if it gets too hot.\nIt‚Äôs hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very high temperature is likely to reduce the lifespan of a GPU.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute",
      "Accelerators"
    ]
  },
  {
    "objectID": "qmd/compute/cpu-memory/index.html",
    "href": "qmd/compute/cpu-memory/index.html",
    "title": "CPU memory",
    "section": "",
    "text": "This is a tiny chapter, since usually there are very few nuances one needs to know about CPU memory - which is a good thing!\nMost of the ML workload compute happens on GPUs, but typically there should be at least as much CPU memory on each node as there is on the GPUs. So, for example, if you‚Äôre on a H100 node with 8x 80GB GPUs, you have 640GB of GPU memory. Thus you want at least as much of CPU memory. But most recent high end cloud packages usually come with 1-2TBs of CPU memory.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU memory"
    ]
  },
  {
    "objectID": "qmd/compute/cpu-memory/index.html#what-cpu-memory-is-needed-for-in-ml-workloads",
    "href": "qmd/compute/cpu-memory/index.html#what-cpu-memory-is-needed-for-in-ml-workloads",
    "title": "CPU memory",
    "section": "What CPU memory is needed for in ML workloads",
    "text": "What CPU memory is needed for in ML workloads\n\nLoading the model weights, unless they are loaded directly onto the GPUs - this is usually a transitory memory usage that goes back to zero once the model has been moved to GPUs.\nSaving the model weights. In some situations each GPU writes its own checkpoint directly to the disk, in other cases the model is recomposed on the CPU before it‚Äôs written to disk - this too is a transitory memory usage.\nPossible parameter and optimizer state offloading when using frameworks like Deepspeed. In which case quite a lot of CPU memory might be needed.\nActivations calculated in the forward pass, and which need to be available for the backward path can also be offloaded to CPU, rather than discarded and then recomputed during the backward pass to save the unnecessary overhead\nDataLoader is usually one of the main users of CPU memory and at times it may consume very large amounts of memory. Typically there are at least 2x 8 DL workers running on each node, so you need enough memory to support at least 16 processes each holding some data. For example, in the case of streaming data from the cloud, if the data shards are large, these processes could easily eat up hundreds of GBs of CPU memory.\nThe software itself and its dependent libraries uses a bit of CPU memory, but this amount is usually negligible.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU memory"
    ]
  },
  {
    "objectID": "qmd/compute/cpu-memory/index.html#things-to-know",
    "href": "qmd/compute/cpu-memory/index.html#things-to-know",
    "title": "CPU memory",
    "section": "Things to know",
    "text": "Things to know\n\nIf the DataLoader uses HF datasets in mmap mode the Resident memory usage may appear to be using a huge amount of CPU memory as it‚Äôll try to map out the whole datasets to the memory. Except this is misleading, since if the memory is needed elsewhere the OS will page out any unneeded mmap‚Äôed pages back to the system. You can read more about it here. This awareness, of course, applies to any dataset using mmap, I was using HF datasets as an example since it‚Äôs very widely used.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Compute",
      "CPU memory"
    ]
  },
  {
    "objectID": "qmd/compute/index.html",
    "href": "qmd/compute/index.html",
    "title": "üíª Compute",
    "section": "",
    "text": "Accelerator - the work horses of ML - GPUs, TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs (WIP)\nCPU - cpus, affinities (WIP)\nCPU Memory - how much CPU memory is enough - the shortest chapter ever.\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üíª Compute"
    ]
  },
  {
    "objectID": "qmd/debug/tiny-scripts/index.html",
    "href": "qmd/debug/tiny-scripts/index.html",
    "title": "",
    "section": "",
    "text": "QmdDebugging and TroubleshootingA Back up of scripts\n\n\n\n\n\nA Back up of scripts\nThis is a backup of scripts discussed in Faster debug and development with tiny models, tokenizers and datasets.\n\nc4-en-10k.py\ncm4-synthetic-testing.py\nfsmt-make-super-tiny-model.py\ngeneral-pmd-ds-unpack.py\ngeneral-pmd-synthetic-testing.py\nm4-ds-unpack.py\nmt5-make-tiny-model.py\nopenwebtext-10k.py\noscar-en-10k.py\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam and Bekman, Stas},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam, and Stas Bekman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Debugging and Troubleshooting",
      "A Back up of scripts"
    ]
  },
  {
    "objectID": "qmd/network/benchmarks/results/index.html",
    "href": "qmd/network/benchmarks/results/index.html",
    "title": "",
    "section": "",
    "text": "QmdInter-node and intra-node Networking HardwareNetworking BenchmarksNetwork Benchmarks Results\n\n\n\n\n\nNetwork Benchmarks Results\n\nDisabling NVLink\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam and Bekman, Stas},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam, and Stas Bekman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Inter-node and intra-node Networking Hardware",
      "Networking Benchmarks",
      "Network Benchmarks Results"
    ]
  },
  {
    "objectID": "qmd/orchestration/slurm/index.html",
    "href": "qmd/orchestration/slurm/index.html",
    "title": "",
    "section": "",
    "text": "üéª OrchestrationWorking in SLURM Environment\n\n\n\n\n\nWorking in SLURM Environment\nUnless you‚Äôre lucky and you have a dedicated cluster that is completely under your control chances are that you will have to use SLURM to timeshare the GPUs with others. But, often, if you train at HPC, and you‚Äôre given a dedicated partition you still will have to use SLURM.\nThe SLURM abbreviation stands for: Simple Linux Utility for Resource Management - though now it‚Äôs called The Slurm Workload Manager. It is a free and open-source job scheduler for Linux and Unix-like kernels, used by many of the world‚Äôs supercomputers and computer clusters.\nThese chapters will not try to exhaustively teach you SLURM as there are many manuals out there, but will cover some specific nuances that are useful to help in the training process.\n\nSLURM For Users - everything you need to know to do your training in the SLURM environment.\nSLURM Administration - if you‚Äôre unlucky to need to also manage the SLURM cluster besides using it, there is a growing list of recipes in this document to get things done faster for you.\nPerformance - SLURM performance nuances.\nLauncher scripts - how to launch with torchrun, accelerate, pytorch-lightning, etc. in the SLURM environment\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration",
      "Working in SLURM Environment"
    ]
  },
  {
    "objectID": "qmd/performance/index.html",
    "href": "qmd/performance/index.html",
    "title": "üèéÔ∏è Performance",
    "section": "",
    "text": "Performance and Acceleration\n\nTuning ML software for best performance - tweaking software for the best performance.\nTuning ML hardware for best performance - choosing and configuring machine learning hardware for best performance.\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèéÔ∏è  Performance"
    ]
  },
  {
    "objectID": "qmd/storage/index.html",
    "href": "qmd/storage/index.html",
    "title": "üì¶ Storage",
    "section": "",
    "text": "There are 3 distinct IO needs in the ML workload:\n\nYou need to be able to feed the DataLoader fast - (super fast read, don‚Äôt care about fast write) - requires sustainable load for hours and days\nYou need to be able to write checkpoints fast - (super fast write, fastish read as you will be resuming a few times) - requires burst writing - you want super fast to not block the training for long (unless you use some sort of cpu offloading to quickly unblock the training)\nYou need to be able to load and maintain your codebase - (medium speed for both reading and writing) - this also needs to be shared since you want all nodes to see the same codebase - as it happens only during the start or resume it‚Äôll happen infrequently\n\nAs you can see these 3 have very different requirements both on speed and sustainable load, and thus ideally you‚Äôd have 3 different filesystems, each optimized for the required use case.\nIf you have infinite funds, of course, get a single super-fast read, super-fast write, that can do that for days non-stop. But for most of us, this is not possible so getting 2 or 3 different types of partitions where you end up paying much less is a wiser choice.\nIncoming suggestions from Ross Wightman to integrate:\n\nI‚Äôd try to separate volumes by workload, so keep the ‚Äòlots of small files‚Äô, high churn like environments, code separate from bulk storage like datasets, checkpoints. Possibly even split those too since datasets are largely static and checkpoints are being rotated all the time\nWhen datasets are on network storage, just like bucket storage, they should consist of large files AND be read as large files (sequentially in large chunks, not mmapped!). Avoid seeking within datasets\nSetups like HF datasets can be deceiving, might look like one big file, but often being mmap‚Äôd and the IO read pattern is nuts, like 3-4x more iops than if you‚Äôd read them as individual files. Mmap loading can be turned off, but if that‚Äôs the case, for a lot of datasets you move a problem into the DataLoader processes, requiring reading too much data into memory at once. Better awareness of tradeoffs for different use cases, and especially using Iterable streaming when appropriate.\nNote that once your datasets are optimally friendly for a large, distributed network filesystem, they can usually just be streamed from bucket storage in cloud systems that have that option. So better to move them off the network filesystem in that case.\nIn a way, bucket storage like s3, via the interface limitations, enforces patterns that are reasonable for storage backends like this. It‚Äôs ooh, it‚Äôs mounted as a folder, I can do whatever I want (mmap files, write loads of little ones, delete them all, etc) that‚Äôs the prob.\nOne also cannot expect to treat a distributed filesystem like their local disk. If you separated volumes by workload you‚Äôd probably be able to utilize much higher % of the total storage. Don‚Äôt mix high churn, small files with low churn large files.\nAlso, note that once your datasets are optimally friendly for a large, distributed network filesystem, they can usually just be streamed from bucket storage in cloud systems that have that option. So better to move them off the network filesystem in that case.\n\n\n\n\n\nNAS: Network Attached Storage\nSAN: Storage Area Network\nDAS: Direct-Attached storage\nNSD: Network Shared Disk\nOSS: Object storage server\nMDS: Metadata server\nMGS: Management server\n\n\n\n\nDistributed Parallel File Systems are the fastest solutions\nDistributed parallel file systems dramatically improve performance where hundreds to thousands of clients can access the shared storage simultaneously. They also help a lot with reducing hotspots (where some data pockets are accessed much more often than others).\nThe 2 excellent performing parallel file systems that I had experience with are:\n\nLustre FS (Open Source) (Wiki)\nGPFS (IBM), recently renamed to IBM Storage Scale, and before that it was called IBM Spectrum Scale.\n\nBoth solutions have been around for 2+ decades. Both are POSIX-compliant. These are also not trivial to create - you have to setup a whole other cluster with multiple cpu-only VMs dedicated exclusively for those filesystems - only then you can mount those. As compared to weaker cloud-provided ‚Äúbuilt-in‚Äù solutions which take only a few screens of questions to answer in order to activate. And when creating the storage cluster there is a whole science to which VMs to choose for which functionality. For example, here is a Lustre guide on GCP.\ncase study: At JeanZay HPC (France) we were saving 2.3TB checkpoint in parallel on 384 processes in 40 secs! This is insanely fast - and it was GPFS over NVME drives.\nNASA‚Äôs cluster has a long long list of gotchas around using Lustre.\nSome very useful pros of GFPS: - If you have a lot of small files, you can easily run out of inodes (df -i to check). GFPS 5.x never runs out of inodes, it dynamically creates more as needed - GPFS doesn‚Äôt have the issue Lustre has where you can run out of disk space at 80% if one of the sub-disks got full and wasn‚Äôt re-balanced in time - you can reliably use all 100% of the allocated storage. - GPFS doesn‚Äôt use a central metadata server (or a cluster of those) which often becomes a bottleneck when dealing with small files. Just like data, metatada is handled by each node in the storage cluster. - GPFS comes with a native NSD client which is superior to the generic NFS client, but either can be used with it.\nOther parallel file systems I don‚Äôt yet have direct experience with:\n\nBeeGFS\nWekaIO\nDAOS (Distributed Asynchronous Object Storage) (Intel)\nNetApp\n\nMost clouds provide at least one implementation of these, but not all. If your cloud provider doesn‚Äôt provide at least one of these and they don‚Äôt have a fast enough alternative to meet your needs you should reconsider.\nOK‚Äôish solutions\nThere are many OK‚Äôish solutions offered by various cloud providers. Benchmark those seriously before you commit to any. Those are usually quite decent for handling large files and not so much for small files.\ncase study: As of this writing with GCP‚Äôs Zonal FileStore over NFS solution python -c \"import torch\" takes 20 secs to execute, which is extremely slow! Once the files are cached it then takes ~2 secs. Installing a conda environment with a handful of prebuilt python packages can easily take 20-30 min! This solution we started with had been very painful and counter-productive to our work. This would impact anybody who has a lot of python packages and conda environments. But, of course, GCP provides much faster solutions as well.\n\n\n\nYou will need to choose which client to use to connect the file system to your VM with.\nThe most common choice is: NFS - which has been around for 4 decades. It introduces an additional overhead and slows things down. So if there is a native client supported by your VM, you‚Äôd have an overall faster performance using it over NFS. For example, GPFS comes with an NSD client which is superior to NFS.\n\n\n\nIf the file system you use uses a block size of 16mb, but the average size of your files is 16k, you will be using 1,000 times more disk space than the actual use. For example, you will see 100TB of disk space used when the actual disk space will be just 100MB.\nfootnote: On Linux the native file systems typically use a block size of 4k.\nSo often you might have 2 very different needs and require 2 different partitions optimized for different needs.\n\nthousands to millions of tiny files - 4-8k block size\nfew large files - 2-16mb block size\n\ncase study: Python is so bad at having tens of thousand of tiny files that if you have many conda environments you are likely to run of inodes in some situations. At JeanZay HPC we had to ask for a special dedicated partition where we would install all conda environments because we kept running out of inodes on normal GPFS partitions. I think the problem is that those GPFS partitions were configured with 16MB block sizes, so this was not a suitable partition for 4KB-large files.\nThe good news is that modern solutions are starting to introduce a dynamic block size. For example, the most recent GPFS supports sub-blocks. So, for example, it‚Äôs possible to configure GPFS with a block size of 2mb, with a sub-block of 8k, and then the tiny files get packed together as sub-blocks, thus not wasting too much disk space.\n\n\n\nHere are shared file system storage solutions made available by various cloud providers:\n\nGCP\nAzure\nAWS\n\n\n\n\nWhile cloud storage is cheaper the whole idea of fetching and processing your training data stream dynamically at training time is very problematic with a huge number of issues around it.\nSame goes for dynamic offloading of checkpoints to the cloud.\nIt‚Äôs so much better to have enough disk space locally for data loading.\nFor checkpointing there should be enough local disk space for saving a checkpoint in a fast and reliable way and then having a crontab job or a slurm job to offload it to the cloud. Always keep the last few checkpoints locally for a quick resume, should your job crash, as it‚Äôd be very expensive to wait to fetch the checkpoint from the cloud for a resume.\ncase study: we didn‚Äôt have a choice and had to use cloud storage for dataloading during IDEFICS-80B training as we had barely any local storage and since it was multimodal data it was many TBs of data. We spent many weeks trying to make this solution robust and it sucked at the end. The biggest issue was that it was very difficult at the time to keep track of RNG state for the DataSampler because the solution we used, well, didn‚Äôt bother to take care of it. So a lot of data that took a lot of time to create was wasted (not used) and a lot of data was repeated, so we didn‚Äôt have a single epoch of unique data.\n\n\n\nThere is a subtle problem with distributed shared storage used on compute nodes. Since most physical disks used to build the large file systems are only 0.3-2TB large, any of these physical disks can get full before the combined storage gets full. And thus they require constant rebalancing so that there will be no situation where one disk is 99% full and others are only 50% full. Since rebalancing is a costly operation, like most programming languages‚Äô garbage collection, it happens infrequently. And so if you run df and it reports 90% full, it‚Äôs very likely that any of the programs can fail at any given time.\nFrom talking to IO engineers, the accepted reality (that for some reason is not being communicated to customers) is that only about 80% of distributed large storage is reliable.\nWhich means that if you want to have 100TB of reliable cloud storage you actually need to buy 125TB of storage, since 80% of that will be 100TB. So you need to plan to pay 25% more than what you provisioned for your actual needs. I‚Äôm not sure why the customer should pay for the technology deficiency but that‚Äôs how it is.\nFor example, GCP states that only 89% can be used reliably, albeit more than once the storage failed already at 83% for me there. Kudos to Google to even disclosing this as a known issue, albeit not at the point of where a person buys the storage. As in - we recommend you buy 12% more storage than you actually plan to use, since we can only reliably deliver 89% of it.\nI also talked to Sycomp engineers who provide managed IBM Storage Scale (GPFS) solutions, and according to them GPFS doesn‚Äôt have this issue and the whole 100% can be reliably used.\nAlso on some setups if you do backups via the cloud provider API (not directly on the filesystem), they might end up using the same partition, and, of course, consume the disk space, but when you run df it will not show the real disk usage - it may show usage not including the backups. So if your backups consume 50% of the partition.\nWhatever storage solution you pick, ask the provider how much of the storage can be reliably used, so that there will be no surprises later.\n\n\n\nThis makes no sense to me but with some providers when you make a back up of a partition using their tools, the back up will use space on that same partition. And on some of those providers you won‚Äôt even know this happened until you run out of disk space when you really used 30% of the partition you allocated. On those providers running df is pointless because it‚Äôll tell you the free disk space, but it won‚Äôt include any back ups in it. So you have no idea what‚Äôs going on.\nIf you start making a backup and suddenly everything fails because all processes fail to write but df reports 30% usage, you will now know why this happened. Snapshots too use the same partition.\nSo say you paid for a 100TB partition and you used up 95TB and now you want to back it up - well, you can‚Äôt - where would it put 95TB of data if it has 5TB of data left even if it compresses it.\nAs I discover specific solution that have this unintuitive behavior I will add pointers to how you can see the actual disk usage: - GCP FileStore (but it doesn‚Äôt work for Basic Tier)\n\n\n\nWhen you sync data to and from the cloud make sure to research whether the tool you use checks the checksums, otherwise you may end up with corrupt during transmission data. Some tools do it automatically, others you have to enable this feature (since it usually comes at additional compute cost and transmission slowdown). Better slow, but safe.\nThese are typically MD5 and SHA256 checksums. Usually MD5 is sufficient if your environment is safe, but if you want the additional security do SHA256 checksums.\n\n\n\nHere are a few key storage-related concepts that you likely need to be familiar with:\n\n\nQueue depth (or IO depth) is the number of IO requests that can be queued at one time on a storage device controller. If more IO requests than the controller can queue are being sent the OS will usually put those into its own queue.\nOn Linux the local block devices‚Äô queue depth is usually pre-configured by the kernel. For example, if you want to check the max queue depth set for /dev/sda you can cat /sys/block/sda/queue/nr_requests. To see the current queue depth of a local device run iostat -x and watch for aqu-sz column. (apt install sysstat to get iostat.)\nTypically the more IO requests get buffered the bigger the latency will be, and the better the throughput will be. This is because if a request can‚Äôt be acted upon immediately it‚Äôll prolong the response time as it has to wait before being served. But having multiple requests awaiting to be served in a device‚Äôs queue would typically speed up the total throughput as there is less waiting time between issuing individual requests.\n\n\n\nDirect IO refers to IO that bypasses the operating system‚Äôs caching buffers. This corresponds to O_DIRECT flag in open(2) system call.\nThe opposite is the buffered IO, which is usually the default way most applications do IO since caching typically makes things faster.\nWhen we run an IO benchmark it‚Äôs critical to turn the caching/buffering off, because otherwise the benchmark‚Äôs results will most likely be invalid. You normally won‚Äôt be reading or writing the same file hundreds of times in a row. Hence most likely you‚Äôd want to turn the direct mode on in the benchmark‚Äôs flags if it provides such.\nIn certain situation opening files with O_DIRECT may actually help to overcome delays. For example, if the training program logs to a log file (especially on a slow shared file system), you might not be able to see the logs for many seconds if both the application and the file system buffering are in the way. Opening the log file with O_DIRECT by the writer typically helps to get the reader see the logged lines much sooner.\n\n\n\nIn synchronous IO the client submits an IO request and wait for it to be finished before submitting the next IO request to the same target device.\nIn asynchronous IO the client may submit multiple IO requests one after another without waiting for any to finish first. This requires that the target device can queue up multiple IO requests.\n\n\n\nSequential access IO is when you read blocks of data one by one sequentially (think a movie). Here are some examples: - reading or writing a model‚Äôs checkpoint file all at once - loading a python program - installing a package\nRandom access IO is when you‚Äôre accessing part of a file at random. Here are some examples: - database querying - reading samples from a pre-processed dataset in a random fashion - moving around a file using seek\n\n\n\n\nTime is money both in terms of a developer‚Äôs time and model‚Äôs training time, so it‚Äôs crucial that storage IO isn‚Äôt a bottleneck in your human and compute workflows.\nIn the following sections we will discuss various approaches to figuring out whether the proposed storage solution satisfies your work needs.\n\n\nThe three main storage IO metrics one typically cares for are:\n\nThroughput or Bandwidth (bytes per second - can be MBps, GBps, etc.)\nIOPS (Input/output operations per second that a system can perform\nLatency (msecs or usecs)\n\n\nIOPS measures how many input and/or output operations a given storage device or a cluster can perform per second. Typically read and write IOPS won‚Äôt be the same. And for many systems it‚Äôll also depend on whether the operation is sequential or random. So a storage system will have 4 different IOPS rates:\n\n\nIOPS of random reads\nIOPS of random writes\nIOPS of sequential reads\nIOPS of sequential writes\n\n\nThroughput refers to how much data can be processed per second.\n\nIOPS vs.¬†Throughput\n\nwhen you deal with small files high IOPS is important.\nwhen you deal with large files high throughput is important.\n\nIOPS correlates to Throughput via block size: Throughput = IOPS * block_size\nThus given a fixed IOPS - the larger the block size that the system can read or write the bigger the throughput will be.\nAnd since there are 4 IOPS categories, correspondingly there are 4 throughput values to match.\nLatency: is the delay between the moment the instruction to transfer data is issued and when the response to that instruction arrives.\nTypically the more distance (switches, relays, actual distance) the packet has to travel the bigger the latency will be.\nSo if you have a local NVME drive your read or write latency will be much shorter as compared to reading or writing to a storage device that is located on another continent.\n\n\n\nfio - Flexible I/O tester is a commonly used IO benchmarking tool, which is relatively easy to operate. It has many options which allow you to emulate pretty much any type of a load and it provides a very detailed performance report.\nFirst install fio with apt install fio or however your package manager does it.\nHere is an example of a read benchmark:\nbase_path=/path/to/partition/\nfio --ioengine=libaio --filesize=16k --ramp_time=2s --time_based --runtime=3m --numjobs=16 \\\n--direct=1 --verify=0 --randrepeat=0 --group_reporting --unlink=1 --directory=$base_path  \\\n--name=read-test --blocksize=4k --iodepth=64 --readwrite=read\nHere 16 concurrent read threads will run for 3 minutes. The benchmark uses a block size of 4k (typical for most OSes) with the file size of 16k (a common size of most Python files) in a sequential reading style using non-buffered IO. So this particular set of flags will create a good benchmark to show how fast you can import Python modules on 16 concurrent processes.\ncase study: on one NFS setup we had python -c \"import torch\" taking 20 seconds the first time it was run, which is about 20x slower than the same test on a normal NVME drive. Granted once the files were cached the loading was much faster but it made for a very painful development process since everything was slow.\ngood read: Fio Output Explained - it‚Äôs an oldie but is still a goodie - if you have a more up-to-date write up please send me a link or a PR.\nImportant: if you don‚Äôt use the --unlink=1 flag make sure to delete fio‚Äôs work files between different benchmarks - not doing so can lead to seriously wrong reports as fio will reuse files it prepared for a different benchmark which must not be re-used if the benchmark parameters have changed. Apparently this reuse is an fio feature, but to me it‚Äôs a bug since I didn‚Äôt know this nuance and got a whole lot of invalid reports because of it and it took awhile to realize they were wrong.\nGoing back to the benchmark - the parameters will need to change to fit the type of the IO operation you care to be fast - is it doing a lot of pip installs or writing a checkpoint on 512 processes, or doing a random read from a parquet file - each benchmark will have to be adapted to measure the right thing.\nAt the beginning I was manually fishing out the bits I was after, so I automated it resulting in fio-scan benchmark that will run a pair of read/write benchmarks on 16KB, 1MB and 1GB file sizes each using a fixed 4k block size (6 benchmarks in total). It uses a helper fio-json-extract.py to parse the log files and pull out the average latency, bandwidth and iops and report them in a nicely formatted markdown table.\nHere is how to run it:\ngit clone https://github.com/stas00/ml-engineering/\ncd ml-engineering\ncd storage\n\npath_to_test=/path/to/partition/to/test\n./fio-scan $path_to_test\nAdapt path_to_test to point to the partition path you want to benchmark.\nnote: the log parser uses python3. if fio-scan fails it‚Äôs most likely because you run it on a system with python2 installed by default. It expects python --version to be some python 3.x version. You can edit fio-scan to point to the right python.\nHere is an example of this IO scan on my Samsung SSD 980 PRO 2TB NVME drive (summary):\n\nfilesize=16k read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n4.0\n1006.3\n257614\n16\n\n\n\n\nfilesize=16k write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n3.2\n1239.1\n317200\n16\n\n\n\n\nfilesize=1m read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.7\n2400.1\n614419\n16\n\n\n\n\nfilesize=1m write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1940.5\n496765\n16\n\n\n\n\nfilesize=1g read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.4\n2762.0\n707062\n16\n\n\n\n\nfilesize=1g write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1943.9\n497638\n16\n\n\n\nAs you can see as of this writing this is a pretty fast NVMe drive if you want to use it as a base-line against, say, a network shared file system.\n\n\n\nBesides properly designed performance benchmarks which give you some numbers that you may or may not be able to appreciate there is a perception benchmark, and that is how does a certain functionality or a service feel. For example, when going to a website, does it feel like it‚Äôs taking too long to load a webpage? or when going to a video service, does it take too long for the video to start playing and does it stop every few seconds to buffer the stream?\nSo with file system the questions are very simple - does it feel that it takes too long to install or launch a program? Since a lot of us live in the Python world, python is known to have thousands of tiny files which are usually installed into a virtual environment, with conda being the choice of many as of this writing.\nIn one of the environments we have noticed that our developers‚Äô productivity was really bad on a shared filesystem because it was taking up to 30min to install a conda environment with various packages needed for using a certain ML-training framework, and we also noticed that python -c \"import torch' could take more than 20 seconds. This is about 5-10x slower than a fast local NVME-based filesystem would deliver. Obviously, this is bad. So I devised a perception test using time to measure the common activities. That way we could quickly tell if the proposed shared file system solution that we contemplated to switch to were significantly better. We didn‚Äôt want a solution that was 2x faster, we wanted a solution that was 10x better, because having an expensive developer wait for proverbial paint to dry is not a good thing for a business.\nSo here is the poor man‚Äôs benchmark that we used, so this is just an example. Surely if you think about the workflow of your developers you would quickly identify where things are slow and devise yours best fitting your needs.\nnote: To have a baseline to compare to do these timing tests on a recently manufactured local NVME. This way you know what the ceiling is, but with beware that many shared file systems won‚Äôt be able to match that.\nStep 1. Install conda onto the shared file system you want to test if it‚Äôs not there already.\nexport target_partition_path=/mnt/weka  # edit me!!!\nmkdir -p $target_partition_path/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O $target_partition_path/miniconda3/miniconda.sh\nbash $target_partition_path/miniconda3/miniconda.sh -b -u -p $target_partition_path/miniconda3\nrm -rf $target_partition_path/miniconda3/miniconda.sh\n$target_partition_path/miniconda3/bin/conda init bash\nbash\nnotes: - adapt target_partition_path and the miniconda download link if you aren‚Äôt on the x86 platform. - at the end we launch a new bash shell for conda setup to take an effect, you might need to tweak things further if you‚Äôre not a bash user - I trust you will know what to do.\nStep 2. Measure conda install time (write test)\nTime the creation of a new conda environment:\ntime conda create -y -n install-test python=3.9\nreal    0m29.657s\nuser    0m9.141s\nsys     0m2.861s\nTime the installation of some heavy pip packages:\nconda deactivate\nconda activate install-test\ntime pip install torch torchvision torchaudio\nreal    2m10.355s\nuser    0m50.547s\nsys     0m12.144s\nPlease note that this test is somewhat skewed since it also includes the packages download in it and depending on your incoming network speed it could be super fast or super slow and could impact the outcome. But once the downloaded packages are cached, in the case of conda they are also untarred, so if you try to install the packages the 2nd time the benchmark will no longer be fair as on a slow shared file system the untarring could be very slow and we want to catch that.\nI don‚Äôt worry about it because usually when the file system is very slow usually you can tell it‚Äôs very slow even if the downloads are slow, you just watch the progress and you can just tell.\nIf you do want to make this benchmark precise, you probably could keep the pre-downloaded conda packages and just deleting their untar‚Äôed dirs:\nfind $target_partition_path/miniconda3/pkgs -mindepth 1 -type d -exec rm -rf {} +\nin the case of pip it doesn‚Äôt untar anything, but just caches the wheels it downloaded, so the time pip install benchmark can definitely be more precise if you run it the 2nd time (the first time it‚Äôs downloaded, cached and installed, the second time it‚Äôs installed from cache. So you could do:\nconda create -y -n install-test python=3.9\nconda activate install-test\npip install torch torchvision torchaudio\nconda create -y -n install-test2 python=3.9\nconda activate install-test2\ntime pip install torch torchvision torchaudio\nAs you can see here we time only the 2nd time we install the pip packages.\nStep 3. Measure loading time after flushing the memory and file system caches (read test)\nsudo sync\necho 3 | sudo tee /proc/sys/vm/drop_caches\ntime python -c \"import torch\"\nAs you can see before we do the measurement we have to tell the OS to flush its memory and file system caches.\nIf you don‚Äôt have sudo access you can skip the command involving sudo, also sometimes the system is setup to work w/o sudo. If you can‚Äôt run the syncing and flushing of the file system caches you will just get incorrect results as the benchmark will be measuring the time to load already cached file system objects. To overcome this either ask your sysadmin to do it for you or simply come back in the morning while hopefully your file system caches other things and evicts the python packages, and then repeat the python one liner then with the hope those files are no longer in the cache.\nHere is how to see the caching effect:\n$ time python -c \"import torch\"\n\nreal    0m5.404s\nuser    0m1.761s\nsys     0m0.751s\n\n$ time python -c \"import torch\"\n\nreal    0m1.977s\nuser    0m1.623s\nsys     0m0.519s\n\n$ sudo sync\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n$ time python -c \"import torch\"\n\nreal    0m5.698s\nuser    0m1.712s\nsys     0m0.734s\nYou can see that the first time it wasn‚Äôt cached and took ~3x longer, then when I run it the second time. And then I told the system to flush memory and file system caches and you can see it was 3x longer again.\nI think it might be a good idea to do the memory and file system caching in the write tests again, since even there caching will make the benchmark appear faster than what it would be like in the real world where a new package is installed for the first time.\n\n\n\n\n\nHPC IO Benchmark Repository (mdtest has been merged into ior in 2017)\nDLIO\n\nXXX: expand on how these are used when I get a chance to try those\n\n\n\nHere are some published IO benchmarks:\n\nMLPerf via MLCommons publishes various hardware benchmarks that measure training, inference, storage and other tasks‚Äô performance. For example, here is the most recent as of this writing storage v0.5 results. Though I find the results are very difficult to make sense of - too many columns and no control whatsoever by the user, and each test uses different parameters - so how do you compare things.\n\nThen various benchmarks that you can run yourself:\n\n\n\n\nTalking to a few storage providers I understood that many companies don‚Äôt bother cleaning up and just keep on buying more and more storage. If you‚Äôre not that company and want to keep things tidy in the following sections I will share how to easily prune various caches that many of us in the Python/Pytorch ecosphere use (and a lot of those will apply to other ecospheres).\n\n\nThe very popular HuggingFace Hub makes it super easy to download models and datasets and cache them locally. What you might not be aware of is that whenever a new revision of the model or a dataset is released, the old revisions remain on your disk - so over time you are likely to have a lot of dead weight.\nThe cached files are usually found at ~/.cache/huggingface but it‚Äôs possible to override those with HF_HOME environment variable and place them elsewhere if your /home/ doesn‚Äôt have space for huge files. (and in the past those were HUGGINGFACE_HUB_CACHE and TRANSFORMERS_CACHE and some others).\nThe other solution that requires no mucking with environment variables, which requires you to remember to set them, is to symlink your cache to another partition. You could do it for all of your caches:\nmkdir -p ~/.cache\nmv ~/.cache /some/path/\nln -s /some/path/.cache ~/.cache\nor just for HF hub caches:\nmkdir -p ~/.cache/huggingface\nmv ~/.cache/huggingface /some/path/\nln -s /some/path/cache/huggingface ~/.cache/cache/huggingface\nThe mkdir calls are there in case you have haven‚Äôt used the caches yet, so they weren‚Äôt there and they ensure the above code won‚Äôt fail.\nNow that you know where the caches are, you could, of course, nuke the whole cache every so often, but if these are huge models and datasets, and especially if there was some preprocessing done for the latter - you really won‚Äôt want to repeat those time consuming tasks again and again. So I will teach you how to use special tools provided by HuggingFace to do the cleanup.\nThe way revisions work on the HF hub is by pointing main to the latest revision of the files while keeping the old revisions around should anyone want to use the older revision for some reason. Chance are very high you always want the latest revision, and so here is how to delete all old revisions and only keeping main in a few quick steps without tedious manual editing.\nIn terminal A:\n$ pip install huggingface_hub[\"cli\"] -U\n$ huggingface-cli delete-cache --disable-tui\nFile to edit: /tmp/tmpundr7lky.txt\n0 revisions selected counting for 0.0. Continue ? (y/N)\nDo not answer the prompt and proceed with my instructions.\n(note your tmp file will have a different path, so adjust it below)\nIn terminal B:\n$ cp /tmp/tmpedbz00ox.txt cache.txt\n$ perl -pi -e 's|^#(.*\\(detached\\).*)|$1|' cache.txt\n$ cat cache.txt &gt;&gt;  /tmp/tmpundr7lky.txt\nThe perl one-liner uncommented out all lines that had (detached) in it - so can be wiped out. And then we pasted it back into the tmp file huggingface-cli expects to be edited.\nNow go back to terminal A and hit: N, Y, Y, so it looks like:\n0 revisions selected counting for 0.0. Continue ? (y/N) n\n89 revisions selected counting for 211.7G. Continue ? (y/N) y\n89 revisions selected counting for 211.7G. Confirm deletion ? (Y/n) y\nDone.\nIf you messed up with the prompt answering you still have cache.txt file which you can feed again to the new tmp file it‚Äôll create when you run huggingface-cli delete-cache --disable-tui again.\nattached as a snapshot as well as it‚Äôs easier to read on twitter, but use the message to copy-n-paste from.\nPlease note that you can also use this tool to choose which models or datasets to delete completely. You just need to open cache.txt in your editor and remove the # in front of lines that contain main in it for models/datasets you want to be deleted for you. and then repeat the process explained above minus the perl one liner which you‚Äôd replace with manual editing.\nAdditionally you will find that HF datasets have a ~/.cache/huggingface/datasets/downloads dir which often will contain a ton of leftovers from datasets downloads and their preprocessing, including various lock files. On one setup I found literally a few millions of files there. So here is how I clean those up:\nsudo find ~/.cache/huggingface/datasets/downloads -type f -mtime +3 -exec rm {} \\+\nsudo find ~/.cache/huggingface/datasets/downloads -type d -empty -delete\nThe first command leaves files that are younger than 3 days in place, in case someone is in the process of download/processing things and we don‚Äôt want to swipe the carpet from under their feet.\nAs usual you may need to adjust the paths if you placed your caches elsewhere.\n\n\n\nconda and pip will pile up more and more files on your system over time. conda is the worst because it keeps the untarred files which consume an insane amount of inodes and make backups and scans slow. pip at least caches just the wheels (tarred files).\nSo you can safely nuke these dirs:\nrm -rf ~/.cache/pip\nrm -rf ~/anaconda3/pkgs/\nMake sure edit the last command if your conda is installed elsewhere.\n\n\n\nIf you have more than 2 people working on the same system, you really want to avoid each person having their own cache of pip, conda, HF models, datasets and possibly other things. It is very easy to get each user‚Äôs setup to point to a shared cache.\nFor example, let‚Äôs say you make pip and conda caches under /data/cache like so:\nmkdir /data/cache/conda\nmkdir /data/cache/pip\nchmod a+rwx /data/cache/conda\nchmod a+rwx /data/cache/pip\nnow you just need to symlink from each user‚Äôs local cache to this shared cache:\nmkdir -p ~/.cache\n\nrm -rf ~/.cache/pip\nln -s /data/cache/pip ~/.cache/pip\n\nrm -rf ~/.conda/pkgs\nln -s /data/cache/conda/pkgs ~/.conda/pkgs\nnote that we wiped out the existing caches, but you could also move them to the shared cache instead - whatever works, you will want to periodically nuke those anyway.\nSo now when pip or conda will try to reach the user caches they will get redirected to the shared cache. If you have 20 people in the group that‚Äôs 20x less files - and this is very important because conda pkg files are untarred and take up a huge amount of inodes on the disk.\nSo the only issue with this approach is file permissions. If user A installs some packages, user B might not be able to read or write them.\nIf this is an isolated cluster where there are no malicious users you can simply ask everybody to use umask 000 in their ~/.bashrc or even configuring this setting system-wide via /etc/profile or /etc/bash.bashrc and different other shell config files if bash isn‚Äôt your shell of choice.\nOnce umask 000 is run, most files will be created with read/write perms so that all users can read/write each others files.\nOf course, if you are using a sort of HPC, where many unrelated groups use the same cluster this won‚Äôt work and then you would either use groups instead of making files read/write by all, with possibly setgid bit preset or using ACL . In any such environments there are always sysadmins so you can ask them how to setup a shared cache for your team and they will know what to do.\nAdditionally, recently some of these applications added tools to do the cleanup, e.g.¬†for conda and pip:\nconda clean --all -f -y\npip cache purge\n\n\n\nOf course, sooner or later, your partition will get bigger and bigger, and you will probably want to understand where data is leaking. Typically you will need to find the users who contribute to the most of data consumption and ask them to do some cleanups.\nSo for example to find which users consume the most disk run:\nsudo du -ahd1 /home/* | sort -rh\nit will sort the data by the worst offenders. If you want to help them out you could go into their dirs and analyse the data a level deeper:\nsudo du -ahd1 /home/*/* | sort -rh\nor for a specific user foo:\nsudo du -ahd1 /home/foo/* | sort -rh\nYou could also set disk usage quotas but usually this doesn‚Äôt work too well, because depending on the workflows of your company some users need to generate a lot more data then others, so they shouldn‚Äôt be punished for that with inability to do their work and have their job crash - which could have been run for many hours and all that work will be lost - so at the end of the day the company will be paying for the lost time.\nGetting users to be aware of them using too much disk space can be a very difficult task.\n\n\n\nAlso beware of inode usage, on some shared partitions on HPCs I have seen more than once cases where a job crashed not because there was no disk space left, but because the job used up the last inodes and the whole thing crashed.\nTo see inode usage, use df -i:\n$ /bin/df -hi\nFilesystem     Inodes IUsed IFree IUse% Mounted on\ntmpfs             16M  1.9K   16M    1% /run\n/dev/sda1         59M  4.1M   55M    7% /\n-h formats huge numbers into human-readable strings.\nSo here you can see the the / partition is using 7% of the total possible inodes.\nDepending on the type of filesystem in some cases it‚Äôs possible to add more inodes whereas in other cases it‚Äôs not possible.\nSo as part of your monitoring of disk space you also need to monitor inode usage as a critical resource.\n\n\n\nNormally compute nodes will use /tmp/ for temp files. The problem is on most set ups /tmp resides on the tiny / filesystem of each node (often &lt;100GB) and since /tmp/ only gets reset on reboot, this doesn‚Äôt get cleaned up between SLURM jobs and this leads to /tmp running out of space and so when you try to run something that let‚Äôs say untars a file you‚Äôre likely to run into:\nOSError: [Errno 28] No space left on device\nThe solution is to set in your SLURM launcher script.\nexport TMPDIR=/scratch\nNow, the slurm job will use a much larger /scratch instead of /tmp, so plenty of temp space to write too.\nfootnote: while /scratch is quite common - the mounted local SSD disk mount point could be named anything, e.g.¬†/localssd - it should be easy to see the right path by running df on one of the compute nodes.\nYou can also arrange for the SLURM setup to automatically clean up such folders on job‚Äôs termination.\n\n\n\nDo you have a problem when your team trains models and you constantly have to buy more storage because huge model checkpoints aren‚Äôt being offloaded to bucket storage fast enough?\nHere is a one-liner that will recursively analyze a path of your choice, find all the checkpoints, sum up their sizes and print the totals sorted by the biggest user, so that you could tell them to clean up their act :) Just edit /mypath to the actual path\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" | \\\nperl -nle 'chomp; ($uid,$size)=(stat($_))[4,7]; $x{$uid}+=$size;\nEND { map { printf qq[%-10s: %7.1fTB\\n], (getpwuid($_))[0], $x{$_}/2**40 }\nsort { $x{$b} &lt;=&gt; $x{$a} } keys %x }'\ngives:\nuser_a    :     2.5TB\nuser_c    :     1.6TB\nuser_b   :      1.2TB\nOf course, you can change the regex to match other patterns or you can remove it altogether to measure all files:\nfind /mypath/ | \\\nperl -nle 'chomp; ($uid,$size)=(stat($_))[4,7]; $x{$uid}+=$size;\nEND { map { printf qq[%-10s: %7.1fTB\\n], (getpwuid($_))[0], $x{$_}/2**40 }\nsort { $x{$b} &lt;=&gt; $x{$a} } keys %x }'\n\n\n\nContinuing the item from above, if you want to automatically delete old checkpoints instead (e.g.¬†those older than 30 days).\nFirst try to ensure the candidates are indeed good to delete:\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" -mtime +30\nand when you feel it‚Äôs safe to delete, only then add rm\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" -mtime +30 -exec rm {} +\n\n\n\n\nRoss Wightman",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#machine-learning-io-needs",
    "href": "qmd/storage/index.html#machine-learning-io-needs",
    "title": "üì¶ Storage",
    "section": "",
    "text": "There are 3 distinct IO needs in the ML workload:\n\nYou need to be able to feed the DataLoader fast - (super fast read, don‚Äôt care about fast write) - requires sustainable load for hours and days\nYou need to be able to write checkpoints fast - (super fast write, fastish read as you will be resuming a few times) - requires burst writing - you want super fast to not block the training for long (unless you use some sort of cpu offloading to quickly unblock the training)\nYou need to be able to load and maintain your codebase - (medium speed for both reading and writing) - this also needs to be shared since you want all nodes to see the same codebase - as it happens only during the start or resume it‚Äôll happen infrequently\n\nAs you can see these 3 have very different requirements both on speed and sustainable load, and thus ideally you‚Äôd have 3 different filesystems, each optimized for the required use case.\nIf you have infinite funds, of course, get a single super-fast read, super-fast write, that can do that for days non-stop. But for most of us, this is not possible so getting 2 or 3 different types of partitions where you end up paying much less is a wiser choice.\nIncoming suggestions from Ross Wightman to integrate:\n\nI‚Äôd try to separate volumes by workload, so keep the ‚Äòlots of small files‚Äô, high churn like environments, code separate from bulk storage like datasets, checkpoints. Possibly even split those too since datasets are largely static and checkpoints are being rotated all the time\nWhen datasets are on network storage, just like bucket storage, they should consist of large files AND be read as large files (sequentially in large chunks, not mmapped!). Avoid seeking within datasets\nSetups like HF datasets can be deceiving, might look like one big file, but often being mmap‚Äôd and the IO read pattern is nuts, like 3-4x more iops than if you‚Äôd read them as individual files. Mmap loading can be turned off, but if that‚Äôs the case, for a lot of datasets you move a problem into the DataLoader processes, requiring reading too much data into memory at once. Better awareness of tradeoffs for different use cases, and especially using Iterable streaming when appropriate.\nNote that once your datasets are optimally friendly for a large, distributed network filesystem, they can usually just be streamed from bucket storage in cloud systems that have that option. So better to move them off the network filesystem in that case.\nIn a way, bucket storage like s3, via the interface limitations, enforces patterns that are reasonable for storage backends like this. It‚Äôs ooh, it‚Äôs mounted as a folder, I can do whatever I want (mmap files, write loads of little ones, delete them all, etc) that‚Äôs the prob.\nOne also cannot expect to treat a distributed filesystem like their local disk. If you separated volumes by workload you‚Äôd probably be able to utilize much higher % of the total storage. Don‚Äôt mix high churn, small files with low churn large files.\nAlso, note that once your datasets are optimally friendly for a large, distributed network filesystem, they can usually just be streamed from bucket storage in cloud systems that have that option. So better to move them off the network filesystem in that case.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#glossary",
    "href": "qmd/storage/index.html#glossary",
    "title": "üì¶ Storage",
    "section": "",
    "text": "NAS: Network Attached Storage\nSAN: Storage Area Network\nDAS: Direct-Attached storage\nNSD: Network Shared Disk\nOSS: Object storage server\nMDS: Metadata server\nMGS: Management server",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#which-file-system-to-choose",
    "href": "qmd/storage/index.html#which-file-system-to-choose",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Distributed Parallel File Systems are the fastest solutions\nDistributed parallel file systems dramatically improve performance where hundreds to thousands of clients can access the shared storage simultaneously. They also help a lot with reducing hotspots (where some data pockets are accessed much more often than others).\nThe 2 excellent performing parallel file systems that I had experience with are:\n\nLustre FS (Open Source) (Wiki)\nGPFS (IBM), recently renamed to IBM Storage Scale, and before that it was called IBM Spectrum Scale.\n\nBoth solutions have been around for 2+ decades. Both are POSIX-compliant. These are also not trivial to create - you have to setup a whole other cluster with multiple cpu-only VMs dedicated exclusively for those filesystems - only then you can mount those. As compared to weaker cloud-provided ‚Äúbuilt-in‚Äù solutions which take only a few screens of questions to answer in order to activate. And when creating the storage cluster there is a whole science to which VMs to choose for which functionality. For example, here is a Lustre guide on GCP.\ncase study: At JeanZay HPC (France) we were saving 2.3TB checkpoint in parallel on 384 processes in 40 secs! This is insanely fast - and it was GPFS over NVME drives.\nNASA‚Äôs cluster has a long long list of gotchas around using Lustre.\nSome very useful pros of GFPS: - If you have a lot of small files, you can easily run out of inodes (df -i to check). GFPS 5.x never runs out of inodes, it dynamically creates more as needed - GPFS doesn‚Äôt have the issue Lustre has where you can run out of disk space at 80% if one of the sub-disks got full and wasn‚Äôt re-balanced in time - you can reliably use all 100% of the allocated storage. - GPFS doesn‚Äôt use a central metadata server (or a cluster of those) which often becomes a bottleneck when dealing with small files. Just like data, metatada is handled by each node in the storage cluster. - GPFS comes with a native NSD client which is superior to the generic NFS client, but either can be used with it.\nOther parallel file systems I don‚Äôt yet have direct experience with:\n\nBeeGFS\nWekaIO\nDAOS (Distributed Asynchronous Object Storage) (Intel)\nNetApp\n\nMost clouds provide at least one implementation of these, but not all. If your cloud provider doesn‚Äôt provide at least one of these and they don‚Äôt have a fast enough alternative to meet your needs you should reconsider.\nOK‚Äôish solutions\nThere are many OK‚Äôish solutions offered by various cloud providers. Benchmark those seriously before you commit to any. Those are usually quite decent for handling large files and not so much for small files.\ncase study: As of this writing with GCP‚Äôs Zonal FileStore over NFS solution python -c \"import torch\" takes 20 secs to execute, which is extremely slow! Once the files are cached it then takes ~2 secs. Installing a conda environment with a handful of prebuilt python packages can easily take 20-30 min! This solution we started with had been very painful and counter-productive to our work. This would impact anybody who has a lot of python packages and conda environments. But, of course, GCP provides much faster solutions as well.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#remote-file-system-clients",
    "href": "qmd/storage/index.html#remote-file-system-clients",
    "title": "üì¶ Storage",
    "section": "",
    "text": "You will need to choose which client to use to connect the file system to your VM with.\nThe most common choice is: NFS - which has been around for 4 decades. It introduces an additional overhead and slows things down. So if there is a native client supported by your VM, you‚Äôd have an overall faster performance using it over NFS. For example, GPFS comes with an NSD client which is superior to NFS.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#file-block-size",
    "href": "qmd/storage/index.html#file-block-size",
    "title": "üì¶ Storage",
    "section": "",
    "text": "If the file system you use uses a block size of 16mb, but the average size of your files is 16k, you will be using 1,000 times more disk space than the actual use. For example, you will see 100TB of disk space used when the actual disk space will be just 100MB.\nfootnote: On Linux the native file systems typically use a block size of 4k.\nSo often you might have 2 very different needs and require 2 different partitions optimized for different needs.\n\nthousands to millions of tiny files - 4-8k block size\nfew large files - 2-16mb block size\n\ncase study: Python is so bad at having tens of thousand of tiny files that if you have many conda environments you are likely to run of inodes in some situations. At JeanZay HPC we had to ask for a special dedicated partition where we would install all conda environments because we kept running out of inodes on normal GPFS partitions. I think the problem is that those GPFS partitions were configured with 16MB block sizes, so this was not a suitable partition for 4KB-large files.\nThe good news is that modern solutions are starting to introduce a dynamic block size. For example, the most recent GPFS supports sub-blocks. So, for example, it‚Äôs possible to configure GPFS with a block size of 2mb, with a sub-block of 8k, and then the tiny files get packed together as sub-blocks, thus not wasting too much disk space.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#cloud-shared-storage-solutions",
    "href": "qmd/storage/index.html#cloud-shared-storage-solutions",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Here are shared file system storage solutions made available by various cloud providers:\n\nGCP\nAzure\nAWS",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#local-storage-beats-cloud-storage",
    "href": "qmd/storage/index.html#local-storage-beats-cloud-storage",
    "title": "üì¶ Storage",
    "section": "",
    "text": "While cloud storage is cheaper the whole idea of fetching and processing your training data stream dynamically at training time is very problematic with a huge number of issues around it.\nSame goes for dynamic offloading of checkpoints to the cloud.\nIt‚Äôs so much better to have enough disk space locally for data loading.\nFor checkpointing there should be enough local disk space for saving a checkpoint in a fast and reliable way and then having a crontab job or a slurm job to offload it to the cloud. Always keep the last few checkpoints locally for a quick resume, should your job crash, as it‚Äôd be very expensive to wait to fetch the checkpoint from the cloud for a resume.\ncase study: we didn‚Äôt have a choice and had to use cloud storage for dataloading during IDEFICS-80B training as we had barely any local storage and since it was multimodal data it was many TBs of data. We spent many weeks trying to make this solution robust and it sucked at the end. The biggest issue was that it was very difficult at the time to keep track of RNG state for the DataSampler because the solution we used, well, didn‚Äôt bother to take care of it. So a lot of data that took a lot of time to create was wasted (not used) and a lot of data was repeated, so we didn‚Äôt have a single epoch of unique data.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#beware-that-youre-often-being-sold-only-80-of-the-storage-you-pay-for",
    "href": "qmd/storage/index.html#beware-that-youre-often-being-sold-only-80-of-the-storage-you-pay-for",
    "title": "üì¶ Storage",
    "section": "",
    "text": "There is a subtle problem with distributed shared storage used on compute nodes. Since most physical disks used to build the large file systems are only 0.3-2TB large, any of these physical disks can get full before the combined storage gets full. And thus they require constant rebalancing so that there will be no situation where one disk is 99% full and others are only 50% full. Since rebalancing is a costly operation, like most programming languages‚Äô garbage collection, it happens infrequently. And so if you run df and it reports 90% full, it‚Äôs very likely that any of the programs can fail at any given time.\nFrom talking to IO engineers, the accepted reality (that for some reason is not being communicated to customers) is that only about 80% of distributed large storage is reliable.\nWhich means that if you want to have 100TB of reliable cloud storage you actually need to buy 125TB of storage, since 80% of that will be 100TB. So you need to plan to pay 25% more than what you provisioned for your actual needs. I‚Äôm not sure why the customer should pay for the technology deficiency but that‚Äôs how it is.\nFor example, GCP states that only 89% can be used reliably, albeit more than once the storage failed already at 83% for me there. Kudos to Google to even disclosing this as a known issue, albeit not at the point of where a person buys the storage. As in - we recommend you buy 12% more storage than you actually plan to use, since we can only reliably deliver 89% of it.\nI also talked to Sycomp engineers who provide managed IBM Storage Scale (GPFS) solutions, and according to them GPFS doesn‚Äôt have this issue and the whole 100% can be reliably used.\nAlso on some setups if you do backups via the cloud provider API (not directly on the filesystem), they might end up using the same partition, and, of course, consume the disk space, but when you run df it will not show the real disk usage - it may show usage not including the backups. So if your backups consume 50% of the partition.\nWhatever storage solution you pick, ask the provider how much of the storage can be reliably used, so that there will be no surprises later.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#beware-that-on-some-cloud-providers-backups-use-the-same-partition-they-backup",
    "href": "qmd/storage/index.html#beware-that-on-some-cloud-providers-backups-use-the-same-partition-they-backup",
    "title": "üì¶ Storage",
    "section": "",
    "text": "This makes no sense to me but with some providers when you make a back up of a partition using their tools, the back up will use space on that same partition. And on some of those providers you won‚Äôt even know this happened until you run out of disk space when you really used 30% of the partition you allocated. On those providers running df is pointless because it‚Äôll tell you the free disk space, but it won‚Äôt include any back ups in it. So you have no idea what‚Äôs going on.\nIf you start making a backup and suddenly everything fails because all processes fail to write but df reports 30% usage, you will now know why this happened. Snapshots too use the same partition.\nSo say you paid for a 100TB partition and you used up 95TB and now you want to back it up - well, you can‚Äôt - where would it put 95TB of data if it has 5TB of data left even if it compresses it.\nAs I discover specific solution that have this unintuitive behavior I will add pointers to how you can see the actual disk usage: - GCP FileStore (but it doesn‚Äôt work for Basic Tier)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#dont-forget-the-checksums",
    "href": "qmd/storage/index.html#dont-forget-the-checksums",
    "title": "üì¶ Storage",
    "section": "",
    "text": "When you sync data to and from the cloud make sure to research whether the tool you use checks the checksums, otherwise you may end up with corrupt during transmission data. Some tools do it automatically, others you have to enable this feature (since it usually comes at additional compute cost and transmission slowdown). Better slow, but safe.\nThese are typically MD5 and SHA256 checksums. Usually MD5 is sufficient if your environment is safe, but if you want the additional security do SHA256 checksums.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#concepts",
    "href": "qmd/storage/index.html#concepts",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Here are a few key storage-related concepts that you likely need to be familiar with:\n\n\nQueue depth (or IO depth) is the number of IO requests that can be queued at one time on a storage device controller. If more IO requests than the controller can queue are being sent the OS will usually put those into its own queue.\nOn Linux the local block devices‚Äô queue depth is usually pre-configured by the kernel. For example, if you want to check the max queue depth set for /dev/sda you can cat /sys/block/sda/queue/nr_requests. To see the current queue depth of a local device run iostat -x and watch for aqu-sz column. (apt install sysstat to get iostat.)\nTypically the more IO requests get buffered the bigger the latency will be, and the better the throughput will be. This is because if a request can‚Äôt be acted upon immediately it‚Äôll prolong the response time as it has to wait before being served. But having multiple requests awaiting to be served in a device‚Äôs queue would typically speed up the total throughput as there is less waiting time between issuing individual requests.\n\n\n\nDirect IO refers to IO that bypasses the operating system‚Äôs caching buffers. This corresponds to O_DIRECT flag in open(2) system call.\nThe opposite is the buffered IO, which is usually the default way most applications do IO since caching typically makes things faster.\nWhen we run an IO benchmark it‚Äôs critical to turn the caching/buffering off, because otherwise the benchmark‚Äôs results will most likely be invalid. You normally won‚Äôt be reading or writing the same file hundreds of times in a row. Hence most likely you‚Äôd want to turn the direct mode on in the benchmark‚Äôs flags if it provides such.\nIn certain situation opening files with O_DIRECT may actually help to overcome delays. For example, if the training program logs to a log file (especially on a slow shared file system), you might not be able to see the logs for many seconds if both the application and the file system buffering are in the way. Opening the log file with O_DIRECT by the writer typically helps to get the reader see the logged lines much sooner.\n\n\n\nIn synchronous IO the client submits an IO request and wait for it to be finished before submitting the next IO request to the same target device.\nIn asynchronous IO the client may submit multiple IO requests one after another without waiting for any to finish first. This requires that the target device can queue up multiple IO requests.\n\n\n\nSequential access IO is when you read blocks of data one by one sequentially (think a movie). Here are some examples: - reading or writing a model‚Äôs checkpoint file all at once - loading a python program - installing a package\nRandom access IO is when you‚Äôre accessing part of a file at random. Here are some examples: - database querying - reading samples from a pre-processed dataset in a random fashion - moving around a file using seek",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#benchmarks",
    "href": "qmd/storage/index.html#benchmarks",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Time is money both in terms of a developer‚Äôs time and model‚Äôs training time, so it‚Äôs crucial that storage IO isn‚Äôt a bottleneck in your human and compute workflows.\nIn the following sections we will discuss various approaches to figuring out whether the proposed storage solution satisfies your work needs.\n\n\nThe three main storage IO metrics one typically cares for are:\n\nThroughput or Bandwidth (bytes per second - can be MBps, GBps, etc.)\nIOPS (Input/output operations per second that a system can perform\nLatency (msecs or usecs)\n\n\nIOPS measures how many input and/or output operations a given storage device or a cluster can perform per second. Typically read and write IOPS won‚Äôt be the same. And for many systems it‚Äôll also depend on whether the operation is sequential or random. So a storage system will have 4 different IOPS rates:\n\n\nIOPS of random reads\nIOPS of random writes\nIOPS of sequential reads\nIOPS of sequential writes\n\n\nThroughput refers to how much data can be processed per second.\n\nIOPS vs.¬†Throughput\n\nwhen you deal with small files high IOPS is important.\nwhen you deal with large files high throughput is important.\n\nIOPS correlates to Throughput via block size: Throughput = IOPS * block_size\nThus given a fixed IOPS - the larger the block size that the system can read or write the bigger the throughput will be.\nAnd since there are 4 IOPS categories, correspondingly there are 4 throughput values to match.\nLatency: is the delay between the moment the instruction to transfer data is issued and when the response to that instruction arrives.\nTypically the more distance (switches, relays, actual distance) the packet has to travel the bigger the latency will be.\nSo if you have a local NVME drive your read or write latency will be much shorter as compared to reading or writing to a storage device that is located on another continent.\n\n\n\nfio - Flexible I/O tester is a commonly used IO benchmarking tool, which is relatively easy to operate. It has many options which allow you to emulate pretty much any type of a load and it provides a very detailed performance report.\nFirst install fio with apt install fio or however your package manager does it.\nHere is an example of a read benchmark:\nbase_path=/path/to/partition/\nfio --ioengine=libaio --filesize=16k --ramp_time=2s --time_based --runtime=3m --numjobs=16 \\\n--direct=1 --verify=0 --randrepeat=0 --group_reporting --unlink=1 --directory=$base_path  \\\n--name=read-test --blocksize=4k --iodepth=64 --readwrite=read\nHere 16 concurrent read threads will run for 3 minutes. The benchmark uses a block size of 4k (typical for most OSes) with the file size of 16k (a common size of most Python files) in a sequential reading style using non-buffered IO. So this particular set of flags will create a good benchmark to show how fast you can import Python modules on 16 concurrent processes.\ncase study: on one NFS setup we had python -c \"import torch\" taking 20 seconds the first time it was run, which is about 20x slower than the same test on a normal NVME drive. Granted once the files were cached the loading was much faster but it made for a very painful development process since everything was slow.\ngood read: Fio Output Explained - it‚Äôs an oldie but is still a goodie - if you have a more up-to-date write up please send me a link or a PR.\nImportant: if you don‚Äôt use the --unlink=1 flag make sure to delete fio‚Äôs work files between different benchmarks - not doing so can lead to seriously wrong reports as fio will reuse files it prepared for a different benchmark which must not be re-used if the benchmark parameters have changed. Apparently this reuse is an fio feature, but to me it‚Äôs a bug since I didn‚Äôt know this nuance and got a whole lot of invalid reports because of it and it took awhile to realize they were wrong.\nGoing back to the benchmark - the parameters will need to change to fit the type of the IO operation you care to be fast - is it doing a lot of pip installs or writing a checkpoint on 512 processes, or doing a random read from a parquet file - each benchmark will have to be adapted to measure the right thing.\nAt the beginning I was manually fishing out the bits I was after, so I automated it resulting in fio-scan benchmark that will run a pair of read/write benchmarks on 16KB, 1MB and 1GB file sizes each using a fixed 4k block size (6 benchmarks in total). It uses a helper fio-json-extract.py to parse the log files and pull out the average latency, bandwidth and iops and report them in a nicely formatted markdown table.\nHere is how to run it:\ngit clone https://github.com/stas00/ml-engineering/\ncd ml-engineering\ncd storage\n\npath_to_test=/path/to/partition/to/test\n./fio-scan $path_to_test\nAdapt path_to_test to point to the partition path you want to benchmark.\nnote: the log parser uses python3. if fio-scan fails it‚Äôs most likely because you run it on a system with python2 installed by default. It expects python --version to be some python 3.x version. You can edit fio-scan to point to the right python.\nHere is an example of this IO scan on my Samsung SSD 980 PRO 2TB NVME drive (summary):\n\nfilesize=16k read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n4.0\n1006.3\n257614\n16\n\n\n\n\nfilesize=16k write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n3.2\n1239.1\n317200\n16\n\n\n\n\nfilesize=1m read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.7\n2400.1\n614419\n16\n\n\n\n\nfilesize=1m write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1940.5\n496765\n16\n\n\n\n\nfilesize=1g read\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n1.4\n2762.0\n707062\n16\n\n\n\n\nfilesize=1g write\n\n\n\n\nlat msec\nbw MBps\nIOPS\njobs\n\n\n\n\n2.1\n1943.9\n497638\n16\n\n\n\nAs you can see as of this writing this is a pretty fast NVMe drive if you want to use it as a base-line against, say, a network shared file system.\n\n\n\nBesides properly designed performance benchmarks which give you some numbers that you may or may not be able to appreciate there is a perception benchmark, and that is how does a certain functionality or a service feel. For example, when going to a website, does it feel like it‚Äôs taking too long to load a webpage? or when going to a video service, does it take too long for the video to start playing and does it stop every few seconds to buffer the stream?\nSo with file system the questions are very simple - does it feel that it takes too long to install or launch a program? Since a lot of us live in the Python world, python is known to have thousands of tiny files which are usually installed into a virtual environment, with conda being the choice of many as of this writing.\nIn one of the environments we have noticed that our developers‚Äô productivity was really bad on a shared filesystem because it was taking up to 30min to install a conda environment with various packages needed for using a certain ML-training framework, and we also noticed that python -c \"import torch' could take more than 20 seconds. This is about 5-10x slower than a fast local NVME-based filesystem would deliver. Obviously, this is bad. So I devised a perception test using time to measure the common activities. That way we could quickly tell if the proposed shared file system solution that we contemplated to switch to were significantly better. We didn‚Äôt want a solution that was 2x faster, we wanted a solution that was 10x better, because having an expensive developer wait for proverbial paint to dry is not a good thing for a business.\nSo here is the poor man‚Äôs benchmark that we used, so this is just an example. Surely if you think about the workflow of your developers you would quickly identify where things are slow and devise yours best fitting your needs.\nnote: To have a baseline to compare to do these timing tests on a recently manufactured local NVME. This way you know what the ceiling is, but with beware that many shared file systems won‚Äôt be able to match that.\nStep 1. Install conda onto the shared file system you want to test if it‚Äôs not there already.\nexport target_partition_path=/mnt/weka  # edit me!!!\nmkdir -p $target_partition_path/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O $target_partition_path/miniconda3/miniconda.sh\nbash $target_partition_path/miniconda3/miniconda.sh -b -u -p $target_partition_path/miniconda3\nrm -rf $target_partition_path/miniconda3/miniconda.sh\n$target_partition_path/miniconda3/bin/conda init bash\nbash\nnotes: - adapt target_partition_path and the miniconda download link if you aren‚Äôt on the x86 platform. - at the end we launch a new bash shell for conda setup to take an effect, you might need to tweak things further if you‚Äôre not a bash user - I trust you will know what to do.\nStep 2. Measure conda install time (write test)\nTime the creation of a new conda environment:\ntime conda create -y -n install-test python=3.9\nreal    0m29.657s\nuser    0m9.141s\nsys     0m2.861s\nTime the installation of some heavy pip packages:\nconda deactivate\nconda activate install-test\ntime pip install torch torchvision torchaudio\nreal    2m10.355s\nuser    0m50.547s\nsys     0m12.144s\nPlease note that this test is somewhat skewed since it also includes the packages download in it and depending on your incoming network speed it could be super fast or super slow and could impact the outcome. But once the downloaded packages are cached, in the case of conda they are also untarred, so if you try to install the packages the 2nd time the benchmark will no longer be fair as on a slow shared file system the untarring could be very slow and we want to catch that.\nI don‚Äôt worry about it because usually when the file system is very slow usually you can tell it‚Äôs very slow even if the downloads are slow, you just watch the progress and you can just tell.\nIf you do want to make this benchmark precise, you probably could keep the pre-downloaded conda packages and just deleting their untar‚Äôed dirs:\nfind $target_partition_path/miniconda3/pkgs -mindepth 1 -type d -exec rm -rf {} +\nin the case of pip it doesn‚Äôt untar anything, but just caches the wheels it downloaded, so the time pip install benchmark can definitely be more precise if you run it the 2nd time (the first time it‚Äôs downloaded, cached and installed, the second time it‚Äôs installed from cache. So you could do:\nconda create -y -n install-test python=3.9\nconda activate install-test\npip install torch torchvision torchaudio\nconda create -y -n install-test2 python=3.9\nconda activate install-test2\ntime pip install torch torchvision torchaudio\nAs you can see here we time only the 2nd time we install the pip packages.\nStep 3. Measure loading time after flushing the memory and file system caches (read test)\nsudo sync\necho 3 | sudo tee /proc/sys/vm/drop_caches\ntime python -c \"import torch\"\nAs you can see before we do the measurement we have to tell the OS to flush its memory and file system caches.\nIf you don‚Äôt have sudo access you can skip the command involving sudo, also sometimes the system is setup to work w/o sudo. If you can‚Äôt run the syncing and flushing of the file system caches you will just get incorrect results as the benchmark will be measuring the time to load already cached file system objects. To overcome this either ask your sysadmin to do it for you or simply come back in the morning while hopefully your file system caches other things and evicts the python packages, and then repeat the python one liner then with the hope those files are no longer in the cache.\nHere is how to see the caching effect:\n$ time python -c \"import torch\"\n\nreal    0m5.404s\nuser    0m1.761s\nsys     0m0.751s\n\n$ time python -c \"import torch\"\n\nreal    0m1.977s\nuser    0m1.623s\nsys     0m0.519s\n\n$ sudo sync\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n$ time python -c \"import torch\"\n\nreal    0m5.698s\nuser    0m1.712s\nsys     0m0.734s\nYou can see that the first time it wasn‚Äôt cached and took ~3x longer, then when I run it the second time. And then I told the system to flush memory and file system caches and you can see it was 3x longer again.\nI think it might be a good idea to do the memory and file system caching in the write tests again, since even there caching will make the benchmark appear faster than what it would be like in the real world where a new package is installed for the first time.\n\n\n\n\n\nHPC IO Benchmark Repository (mdtest has been merged into ior in 2017)\nDLIO\n\nXXX: expand on how these are used when I get a chance to try those\n\n\n\nHere are some published IO benchmarks:\n\nMLPerf via MLCommons publishes various hardware benchmarks that measure training, inference, storage and other tasks‚Äô performance. For example, here is the most recent as of this writing storage v0.5 results. Though I find the results are very difficult to make sense of - too many columns and no control whatsoever by the user, and each test uses different parameters - so how do you compare things.\n\nThen various benchmarks that you can run yourself:",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#why-pay-for-more-storage-when-you-can-easily-clean-it-up-instead",
    "href": "qmd/storage/index.html#why-pay-for-more-storage-when-you-can-easily-clean-it-up-instead",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Talking to a few storage providers I understood that many companies don‚Äôt bother cleaning up and just keep on buying more and more storage. If you‚Äôre not that company and want to keep things tidy in the following sections I will share how to easily prune various caches that many of us in the Python/Pytorch ecosphere use (and a lot of those will apply to other ecospheres).\n\n\nThe very popular HuggingFace Hub makes it super easy to download models and datasets and cache them locally. What you might not be aware of is that whenever a new revision of the model or a dataset is released, the old revisions remain on your disk - so over time you are likely to have a lot of dead weight.\nThe cached files are usually found at ~/.cache/huggingface but it‚Äôs possible to override those with HF_HOME environment variable and place them elsewhere if your /home/ doesn‚Äôt have space for huge files. (and in the past those were HUGGINGFACE_HUB_CACHE and TRANSFORMERS_CACHE and some others).\nThe other solution that requires no mucking with environment variables, which requires you to remember to set them, is to symlink your cache to another partition. You could do it for all of your caches:\nmkdir -p ~/.cache\nmv ~/.cache /some/path/\nln -s /some/path/.cache ~/.cache\nor just for HF hub caches:\nmkdir -p ~/.cache/huggingface\nmv ~/.cache/huggingface /some/path/\nln -s /some/path/cache/huggingface ~/.cache/cache/huggingface\nThe mkdir calls are there in case you have haven‚Äôt used the caches yet, so they weren‚Äôt there and they ensure the above code won‚Äôt fail.\nNow that you know where the caches are, you could, of course, nuke the whole cache every so often, but if these are huge models and datasets, and especially if there was some preprocessing done for the latter - you really won‚Äôt want to repeat those time consuming tasks again and again. So I will teach you how to use special tools provided by HuggingFace to do the cleanup.\nThe way revisions work on the HF hub is by pointing main to the latest revision of the files while keeping the old revisions around should anyone want to use the older revision for some reason. Chance are very high you always want the latest revision, and so here is how to delete all old revisions and only keeping main in a few quick steps without tedious manual editing.\nIn terminal A:\n$ pip install huggingface_hub[\"cli\"] -U\n$ huggingface-cli delete-cache --disable-tui\nFile to edit: /tmp/tmpundr7lky.txt\n0 revisions selected counting for 0.0. Continue ? (y/N)\nDo not answer the prompt and proceed with my instructions.\n(note your tmp file will have a different path, so adjust it below)\nIn terminal B:\n$ cp /tmp/tmpedbz00ox.txt cache.txt\n$ perl -pi -e 's|^#(.*\\(detached\\).*)|$1|' cache.txt\n$ cat cache.txt &gt;&gt;  /tmp/tmpundr7lky.txt\nThe perl one-liner uncommented out all lines that had (detached) in it - so can be wiped out. And then we pasted it back into the tmp file huggingface-cli expects to be edited.\nNow go back to terminal A and hit: N, Y, Y, so it looks like:\n0 revisions selected counting for 0.0. Continue ? (y/N) n\n89 revisions selected counting for 211.7G. Continue ? (y/N) y\n89 revisions selected counting for 211.7G. Confirm deletion ? (Y/n) y\nDone.\nIf you messed up with the prompt answering you still have cache.txt file which you can feed again to the new tmp file it‚Äôll create when you run huggingface-cli delete-cache --disable-tui again.\nattached as a snapshot as well as it‚Äôs easier to read on twitter, but use the message to copy-n-paste from.\nPlease note that you can also use this tool to choose which models or datasets to delete completely. You just need to open cache.txt in your editor and remove the # in front of lines that contain main in it for models/datasets you want to be deleted for you. and then repeat the process explained above minus the perl one liner which you‚Äôd replace with manual editing.\nAdditionally you will find that HF datasets have a ~/.cache/huggingface/datasets/downloads dir which often will contain a ton of leftovers from datasets downloads and their preprocessing, including various lock files. On one setup I found literally a few millions of files there. So here is how I clean those up:\nsudo find ~/.cache/huggingface/datasets/downloads -type f -mtime +3 -exec rm {} \\+\nsudo find ~/.cache/huggingface/datasets/downloads -type d -empty -delete\nThe first command leaves files that are younger than 3 days in place, in case someone is in the process of download/processing things and we don‚Äôt want to swipe the carpet from under their feet.\nAs usual you may need to adjust the paths if you placed your caches elsewhere.\n\n\n\nconda and pip will pile up more and more files on your system over time. conda is the worst because it keeps the untarred files which consume an insane amount of inodes and make backups and scans slow. pip at least caches just the wheels (tarred files).\nSo you can safely nuke these dirs:\nrm -rf ~/.cache/pip\nrm -rf ~/anaconda3/pkgs/\nMake sure edit the last command if your conda is installed elsewhere.\n\n\n\nIf you have more than 2 people working on the same system, you really want to avoid each person having their own cache of pip, conda, HF models, datasets and possibly other things. It is very easy to get each user‚Äôs setup to point to a shared cache.\nFor example, let‚Äôs say you make pip and conda caches under /data/cache like so:\nmkdir /data/cache/conda\nmkdir /data/cache/pip\nchmod a+rwx /data/cache/conda\nchmod a+rwx /data/cache/pip\nnow you just need to symlink from each user‚Äôs local cache to this shared cache:\nmkdir -p ~/.cache\n\nrm -rf ~/.cache/pip\nln -s /data/cache/pip ~/.cache/pip\n\nrm -rf ~/.conda/pkgs\nln -s /data/cache/conda/pkgs ~/.conda/pkgs\nnote that we wiped out the existing caches, but you could also move them to the shared cache instead - whatever works, you will want to periodically nuke those anyway.\nSo now when pip or conda will try to reach the user caches they will get redirected to the shared cache. If you have 20 people in the group that‚Äôs 20x less files - and this is very important because conda pkg files are untarred and take up a huge amount of inodes on the disk.\nSo the only issue with this approach is file permissions. If user A installs some packages, user B might not be able to read or write them.\nIf this is an isolated cluster where there are no malicious users you can simply ask everybody to use umask 000 in their ~/.bashrc or even configuring this setting system-wide via /etc/profile or /etc/bash.bashrc and different other shell config files if bash isn‚Äôt your shell of choice.\nOnce umask 000 is run, most files will be created with read/write perms so that all users can read/write each others files.\nOf course, if you are using a sort of HPC, where many unrelated groups use the same cluster this won‚Äôt work and then you would either use groups instead of making files read/write by all, with possibly setgid bit preset or using ACL . In any such environments there are always sysadmins so you can ask them how to setup a shared cache for your team and they will know what to do.\nAdditionally, recently some of these applications added tools to do the cleanup, e.g.¬†for conda and pip:\nconda clean --all -f -y\npip cache purge\n\n\n\nOf course, sooner or later, your partition will get bigger and bigger, and you will probably want to understand where data is leaking. Typically you will need to find the users who contribute to the most of data consumption and ask them to do some cleanups.\nSo for example to find which users consume the most disk run:\nsudo du -ahd1 /home/* | sort -rh\nit will sort the data by the worst offenders. If you want to help them out you could go into their dirs and analyse the data a level deeper:\nsudo du -ahd1 /home/*/* | sort -rh\nor for a specific user foo:\nsudo du -ahd1 /home/foo/* | sort -rh\nYou could also set disk usage quotas but usually this doesn‚Äôt work too well, because depending on the workflows of your company some users need to generate a lot more data then others, so they shouldn‚Äôt be punished for that with inability to do their work and have their job crash - which could have been run for many hours and all that work will be lost - so at the end of the day the company will be paying for the lost time.\nGetting users to be aware of them using too much disk space can be a very difficult task.\n\n\n\nAlso beware of inode usage, on some shared partitions on HPCs I have seen more than once cases where a job crashed not because there was no disk space left, but because the job used up the last inodes and the whole thing crashed.\nTo see inode usage, use df -i:\n$ /bin/df -hi\nFilesystem     Inodes IUsed IFree IUse% Mounted on\ntmpfs             16M  1.9K   16M    1% /run\n/dev/sda1         59M  4.1M   55M    7% /\n-h formats huge numbers into human-readable strings.\nSo here you can see the the / partition is using 7% of the total possible inodes.\nDepending on the type of filesystem in some cases it‚Äôs possible to add more inodes whereas in other cases it‚Äôs not possible.\nSo as part of your monitoring of disk space you also need to monitor inode usage as a critical resource.\n\n\n\nNormally compute nodes will use /tmp/ for temp files. The problem is on most set ups /tmp resides on the tiny / filesystem of each node (often &lt;100GB) and since /tmp/ only gets reset on reboot, this doesn‚Äôt get cleaned up between SLURM jobs and this leads to /tmp running out of space and so when you try to run something that let‚Äôs say untars a file you‚Äôre likely to run into:\nOSError: [Errno 28] No space left on device\nThe solution is to set in your SLURM launcher script.\nexport TMPDIR=/scratch\nNow, the slurm job will use a much larger /scratch instead of /tmp, so plenty of temp space to write too.\nfootnote: while /scratch is quite common - the mounted local SSD disk mount point could be named anything, e.g.¬†/localssd - it should be easy to see the right path by running df on one of the compute nodes.\nYou can also arrange for the SLURM setup to automatically clean up such folders on job‚Äôs termination.\n\n\n\nDo you have a problem when your team trains models and you constantly have to buy more storage because huge model checkpoints aren‚Äôt being offloaded to bucket storage fast enough?\nHere is a one-liner that will recursively analyze a path of your choice, find all the checkpoints, sum up their sizes and print the totals sorted by the biggest user, so that you could tell them to clean up their act :) Just edit /mypath to the actual path\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" | \\\nperl -nle 'chomp; ($uid,$size)=(stat($_))[4,7]; $x{$uid}+=$size;\nEND { map { printf qq[%-10s: %7.1fTB\\n], (getpwuid($_))[0], $x{$_}/2**40 }\nsort { $x{$b} &lt;=&gt; $x{$a} } keys %x }'\ngives:\nuser_a    :     2.5TB\nuser_c    :     1.6TB\nuser_b   :      1.2TB\nOf course, you can change the regex to match other patterns or you can remove it altogether to measure all files:\nfind /mypath/ | \\\nperl -nle 'chomp; ($uid,$size)=(stat($_))[4,7]; $x{$uid}+=$size;\nEND { map { printf qq[%-10s: %7.1fTB\\n], (getpwuid($_))[0], $x{$_}/2**40 }\nsort { $x{$b} &lt;=&gt; $x{$a} } keys %x }'\n\n\n\nContinuing the item from above, if you want to automatically delete old checkpoints instead (e.g.¬†those older than 30 days).\nFirst try to ensure the candidates are indeed good to delete:\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" -mtime +30\nand when you feel it‚Äôs safe to delete, only then add rm\nfind /mypath/ -regextype posix-egrep -regex \".*\\.(pt|pth|ckpt|safetensors)$\" -mtime +30 -exec rm {} +",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/storage/index.html#contributors",
    "href": "qmd/storage/index.html#contributors",
    "title": "üì¶ Storage",
    "section": "",
    "text": "Ross Wightman",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üì¶  Storage"
    ]
  },
  {
    "objectID": "qmd/training/checkpoints/index.html",
    "href": "qmd/training/checkpoints/index.html",
    "title": "",
    "section": "",
    "text": "üèãÔ∏è TrainingCheckpoints\n\n\n\n\n\nCheckpoints\n\ntorch-checkpoint-convert-to-bf16 - converts an existing fp32 torch checkpoint to bf16. If safetensors are found those are converted as well. Should be easily adaptable to other similar use cases.\ntorch-checkpoint-shrink.py - this script fixes checkpoints which for some reason stored tensors with storage larger than their view at the moment of saving. It clones the current view and re-saves them with just the storage of the current view.\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training",
      "Checkpoints"
    ]
  },
  {
    "objectID": "qmd/training/index.html",
    "href": "qmd/training/index.html",
    "title": "üèãÔ∏è Training",
    "section": "",
    "text": "Training\nSubsections:\n\nModel parallelism\nPerformance\nFault Tolerance\nReproducibility\nInstabilities\nCheckpoints\nTraining hyper-parameters and model initializations\nTensor precision / Data types\nEmulate a multi-node setup using just a single node - instructions on how to emulate a multi-node setup using just a single node - we use the deepspeed launcher here.\nRe-train HF hub models from scratch using finetuning examples\n\nTools:\n\nprintflock.py - a tiny library that makes your print calls non-interleaved in a multi-gpu environment.\nmulti-gpu-non-interleaved-print.py - a flock-based wrapper around print that prevents messages from getting interleaved when multiple processes print at the same time - which is the case with torch.distributed used with multiple-gpus.\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üèãÔ∏è  Training"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html",
    "href": "qmd/training/model-parallelism/index.html",
    "title": "",
    "section": "",
    "text": "QmdTrainingModel Parallelism",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#parallelism-overview",
    "href": "qmd/training/model-parallelism/index.html#parallelism-overview",
    "title": "",
    "section": "Parallelism overview",
    "text": "Parallelism overview\nIn the modern machine learning the various approaches to parallelism are used to:\n\nOvercome GPU memory limitations. Examples:\n\nfit very large models - e.g., t5-11b is 45GB in just model params\nfit very long sequences - e.g.,\n\nsignificantly speed up training - finish training that would take a year in hours\n\nWe will first discuss in depth various 1D parallelism techniques and their pros and cons and then look at how they can be combined into 2D and 3D parallelism to enable an even faster training and to support even bigger models. Various other powerful alternative approaches will be presented.\nWhile the main concepts most likely will apply to any other framework, this article is focused on PyTorch-based implementations.\nTwo main approaches are used to enable training and inferring models that are bigger than the accelerator‚Äôs memory: 1. 3D parallelism - very network efficient, but can be very invasive into the modeling code and require a lot more work to make it work correctly 2. ZeRO parallelism - not very network efficient, but requires close to zero changes to the modeling code and very easy to make to work.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#scalability-concepts",
    "href": "qmd/training/model-parallelism/index.html#scalability-concepts",
    "title": "",
    "section": "Scalability concepts",
    "text": "Scalability concepts\nThe following is the brief description of the main concepts that will be described later in depth in this document.\n\nData Parallelism (DP) - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.\nTensorParallelism (TP) - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on horizontal level.\nPipelineParallelism (PP) - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.\nZero Redundancy Optimizer (ZeRO) - Also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn‚Äôt need to be modified. It also supports various offloading techniques to compensate for limited GPU memory. Sharded DDP is another name for the foundational ZeRO concept as used by various other implementations of ZeRO.\nSequence Parallelism - training on long input sequences requires huge amounts of GPU memory. This technique splits the processing of a single sequence across multiple GPUs.\n\nThe introduction sections of this paper is probably one of the best explanations I have found on most common parallelism techniques Breadth-First Pipeline Parallelism.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#data-parallelism",
    "href": "qmd/training/model-parallelism/index.html#data-parallelism",
    "title": "",
    "section": "Data Parallelism",
    "text": "Data Parallelism\n\nDDP\nMost users with just 2 GPUs already enjoy the increased training speed up thanks to DataParallel (DP) and DistributedDataParallel (DDP) that are almost trivial to use. This is a built-in feature of Pytorch.\nFor details see DistributedDataParallel\n\n\nZeRO Data Parallelism\nZeRO-powered data parallelism (ZeRO-DP) is described on the following diagram from this blog post \nIt can be difficult to wrap one‚Äôs head around it, but in reality the concept is quite simple. This is just the usual DataParallel (DP), except, instead of replicating the full model params, gradients and optimizer states, each GPU stores only a slice of it. And then at run-time when the full layer params are needed just for the given layer, all GPUs synchronize to give each other parts that they miss - this is it.\nConsider this simple model with 3 layers, where each layer has 3 params:\nLa | Lb | Lc\n---|----|---\na0 | b0 | c0\na1 | b1 | c1\na2 | b2 | c2\nLayer La has weights a0, a1 and a2.\nIf we have 3 GPUs, the Sharded DDP (= Zero-DP) splits the model onto 3 GPUs like so:\nGPU0:\nLa | Lb | Lc\n---|----|---\na0 | b0 | c0\n\nGPU1:\nLa | Lb | Lc\n---|----|---\na1 | b1 | c1\n\nGPU2:\nLa | Lb | Lc\n---|----|---\na2 | b2 | c2\nIn a way this is the same horizontal slicing, as tensor parallelism, if you imagine the typical DNN diagram. Vertical slicing is where one puts whole layer-groups on different GPUs. But it‚Äôs just the starting point.\nNow each of these GPUs will get the usual mini-batch as it works in DP:\nx0 =&gt; GPU0\nx1 =&gt; GPU1\nx2 =&gt; GPU2\nThe inputs are unmodified - they think they are going to be processed by the normal model.\nFirst, the inputs hit the layer La.\nLet‚Äôs focus just on GPU0: x0 needs a0, a1, a2 params to do its forward path, but GPU0 has only a0 - it gets sent a1 from GPU1 and a2 from GPU2, bringing all pieces of the model together.\nIn parallel, GPU1 gets mini-batch x1 and it only has a1, but needs a0 and a2 params, so it gets those from GPU0 and GPU2.\nSame happens to GPU2 that gets input x2. It gets a0 and a1 from GPU0 and GPU1, and with its a2 it reconstructs the full tensor.\nAll 3 GPUs get the full tensors reconstructed and a forward happens.\nAs soon as the calculation is done, the data that is no longer needed gets dropped - it‚Äôs only used during the calculation. The reconstruction is done efficiently via a pre-fetch.\nAnd the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -&gt; Lb -&gt; La.\nTo me this sounds like an efficient group backpacking weight distribution strategy:\n\nperson A carries the tent\nperson B carries the stove\nperson C carries the axe\n\nNow each night they all share what they have with others and get from others what they don‚Äôt have, and in the morning they pack up their allocated type of gear and continue on their way. This is Sharded DDP / Zero DP.\nCompare this strategy to the simple one where each person has to carry their own tent, stove and axe, which would be far more inefficient. This is DataParallel (DP and DDP) in Pytorch.\nWhile reading the literature on this topic you may encounter the following synonyms: Sharded, Partitioned.\nIf you pay close attention the way ZeRO partitions the model‚Äôs weights - it looks very similar to tensor parallelism which will be discussed later. This is because it partitions/shards each layer‚Äôs weights, unlike vertical model parallelism which is discussed next.\nImplementations of ZeRO-DP stages 1+2+3: - DeepSpeed - PyTorch (originally it was implemented in FairScale and later it was upstreamed into the PyTorch core)\nDeepspeed ZeRO Integration: - HF Trainer integration - Accelerate - PyTorch Lightning - Determined.AI\nFSDP Integration: - HF Trainer integration - Accelerate - PyTorch Lightning\nImportant papers:\nDeepspeed: - ZeRO: Memory Optimizations Toward Training Trillion Parameter Models - ZeRO-Offload: Democratizing Billion-Scale Model Training - ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning - ZeRO++: Extremely Efficient Collective Communication for Giant Model Training - DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models\nPyTorch: - PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\nMain DeepSpeed ZeRO Resources: - Project‚Äôs github - Usage docs - API docs - Blog posts\n\nZeRO with multiple replicas\nBy default ZeRO uses all GPUs to create a single model replica - that‚Äôs the model is spread out across all gpus. Which leads to various limitations such as:\n\nthe global batch size is inflexible - it‚Äôs always a function of total_gpus*micro_batch_size - which on large clusters could lead to a huge global batch size which might be detrimental for efficient convergence. Granted one could use a tiny micro batch size to keep the global batch size in check, but this leads to smaller matrices on each GPU which results in less efficient compute\nthe much faster intra-node networking is not being benefited from since the slower inter-node network defines the overall speed of communications.\n\nZeRO++ solves the 2nd limitation by introducing Hierarchical Weight Partition for ZeRO (hpZ). In this approach instead of spreading whole model weights across all the gpus, each model replica is restricted to a single node. This increases the memory usage by the total number of nodes, but now the 2x all_gather calls to gather the sharded components are performed over a much faster intra-node connection. Only the reduce_scatter to aggregate and redistribute gradients is performed over the slower inter-node network.\nThe first limitation doesn‚Äôt exactly get fixed since the overall global batch size remains the same, but since each replica is more efficient and because the additional memory pressure is likely to limit the possible micro batch size on each gpu, this overall should improve the throughput of the system.\nPyTorch FSDP has this feature implemented in shardingStrategy.HYBRID_SHARD\nPapers:\n\nZeRO++: Extremely Efficient Collective Communication for Giant Model Training\nPyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n\n\nZeRO variations\nPublished papers that propose modifications to the ZeRO protocol:\n\nMiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud (2022)\nAMSP: Super-Scaling LLM Training via Advanced Model States Partitioning (2023)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#pipeline-parallelism-methods",
    "href": "qmd/training/model-parallelism/index.html#pipeline-parallelism-methods",
    "title": "",
    "section": "Pipeline Parallelism methods",
    "text": "Pipeline Parallelism methods\n\nNaive Model Parallelism (Vertical)\nNaive Model Parallelism (MP) is where one spreads groups of model layers across multiple GPUs. The mechanism is relatively simple - switch the desired layers .to() the desired devices and now whenever the data goes in and out those layers switch the data to the same device as the layer and leave the rest unmodified.\nWe refer to it as Vertical MP, because if you remember how most models are drawn, we slice the layers vertically. For example, if the following diagram shows an 8-layer model:\n===================  ===================\n|  0 | 1 | 2 | 3  |  |  4 | 5 | 6 | 7  |\n===================  ===================\n        gpu0                 gpu1\nwe just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1.\nNow while data travels from layer 0 to 1, 1 to 2 and 2 to 3 this is just the normal model. But when data needs to pass from layer 3 to layer 4 it needs to travel from GPU0 to GPU1 which introduces a communication overhead. If the participating GPUs are on the same compute node (e.g.¬†same physical machine) this copying is pretty fast, but if the GPUs are located on different compute nodes (e.g.¬†multiple machines) the communication overhead could be significantly larger.\nThen layers 4 to 5 to 6 to 7 are as a normal model would have and when the 7th layer completes we often need to send the data back to layer 0 where the labels are (or alternatively send the labels to the last layer). Now the loss can be computed and the optimizer can do its work.\nProblems: - the main deficiency and why this one is called ‚Äúnaive‚Äù MP, is that all but one GPU is idle at any given moment. So if 4 GPUs are used, it‚Äôs almost identical to quadrupling the amount of memory of a single GPU, and ignoring the rest of the hardware. Plus there is the overhead of copying the data between devices. So 4x 6GB cards will be able to accommodate the same size as 1x 24GB card using naive MP, except the latter will complete the training faster, since it doesn‚Äôt have the data copying overhead. But, say, if you have 40GB cards and need to fit a 45GB model you can with 4x 40GB cards (but barely because of the gradient and optimizer states) - shared embeddings may need to get copied back and forth between GPUs.\n\n\nPipeline Parallelism\nPipeline Parallelism (PP) is almost identical to a naive MP, but it solves the GPU idling problem, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process.\nThe following illustration from the GPipe paper shows the naive MP on the top, and PP on the bottom:\n\n\n\nmp-pp\n\n\nIt‚Äôs easy to see from the bottom diagram how PP has less dead zones, where GPUs are idle. The idle parts are referred to as the ‚Äúbubble‚Äù.\nBoth parts of the diagram show a parallelism that is of degree 4. That is 4 GPUs are participating in the pipeline. So there is the forward path of 4 pipe stages F0, F1, F2 and F3 and then the return reverse order backward path of B3, B2, B1 and B0.\nPP introduces a new hyper-parameter to tune and it‚Äôs chunks which defines how many chunks of data are sent in a sequence through the same pipe stage. For example, in the bottom diagram you can see that chunks=4. GPU0 performs the same forward path on chunk 0, 1, 2 and 3 (F0,0, F0,1, F0,2, F0,3) and then it waits for other GPUs to do their work and only when their work is starting to be complete, GPU0 starts to work again doing the backward path for chunks 3, 2, 1 and 0 (B0,3, B0,2, B0,1, B0,0).\nNote that conceptually this is the same concept as gradient accumulation steps (GAS). Pytorch uses chunks, whereas DeepSpeed refers to the same hyper-parameter as GAS.\nBecause of the chunks, PP introduces the concept of micro-batches (MBS). DP splits the global data batch size into mini-batches, so if you have a DP degree of 4, a global batch size of 1024 gets split up into 4 mini-batches of 256 each (1024/4). And if the number of chunks (or GAS) is 32 we end up with a micro-batch size of 8 (256/32). Each Pipeline stage works with a single micro-batch at a time.\nTo calculate the global batch size of the DP + PP setup we then do: mbs*chunks*dp_degree (8*32*4=1024).\nLet‚Äôs go back to the diagram.\nWith chunks=1 you end up with the naive MP, which is very inefficient. With a very large chunks value you end up with tiny micro-batch sizes which could be not every efficient either. So one has to experiment to find the value that leads to the highest efficient utilization of the gpus.\nWhile the diagram shows that there is a bubble of ‚Äúdead‚Äù time that can‚Äôt be parallelized because the last forward stage has to wait for backward to complete the pipeline, the purpose of finding the best value for chunks is to enable a high concurrent GPU utilization across all participating GPUs which translates to minimizing the size of the bubble.\nThe choice of the schedule is critical to the efficient performance, with the most common schedules being in the order of invention:\n\nsequential Gpipe: Efficient training of giant neural networks using pipeline parallelism\ninterleaved 1F1B Pipedream: Fast and efficient pipeline parallel dnn training\nlooped, depth-first Efficient large-scale language model training on gpu clusters using Megatron-LM\nbreadth-first Breadth-First Pipeline Parallelism\n\nHere is for example an interleaved pipeline:\n\n\n\ninterleaved-pipeline-execution\n\n\nHere the bubble (idle time) is further minimized by prioritizing backward passes.\nIt‚Äôs used by DeepSpeed, Varuna and SageMaker to name a few.\nVaruna further tries to improve the schedule by using simulations to discover the most efficient scheduling.\nThere are 2 groups of PP solutions - the traditional Pipeline API and the more modern solutions that make things much easier for the end user by helping to partially or fully automate the process:\n\nTraditional Pipeline API solutions:\n\n\nMegatron-LM\nDeepSpeed\nPyTorch\n\n\nModern solutions:\n\n\nPiPPy\nVaruna\nSagemaker\n\nProblems with traditional Pipeline API solutions: - have to modify the model quite heavily, because Pipeline requires one to rewrite the normal flow of modules into a nn.Sequential sequence of the same, which may require changes to the design of the model. - currently the Pipeline API is very restricted. If you had a bunch of python variables being passed in the very first stage of the Pipeline, you will have to find a way around it. Currently, the pipeline interface requires either a single Tensor or a tuple of Tensors as the only input and output. These tensors must have a batch size as the very first dimension, since pipeline is going to chunk the mini batch into micro-batches. Possible improvements are being discussed here https://github.com/pytorch/pytorch/pull/50693 - conditional control flow at the level of pipe stages is not possible - e.g., Encoder-Decoder models like T5 require special workarounds to handle a conditional encoder stage. - have to arrange each layer so that the output of one model becomes an input to the other model.\nI‚Äôm yet to try to experiment with Varuna and SageMaker but their papers report that they have overcome the list of problems mentioned above and that they require much smaller changes to the user‚Äôs model.\nImplementations: - Pytorch (initial support in pytorch-1.8, and progressively getting improved in 1.9 and more so in 1.10). Some examples - FairScale - DeepSpeed - Megatron-LM has an internal implementation - no API. - Varuna - SageMaker - this is a proprietary solution that can only be used on AWS. - OSLO - this is implemented based on the Hugging Face Transformers. - PiPPy: Pipeline Parallelism for PyTorch - automatic PP via torch.fx - nanotron",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#tensor-parallelism",
    "href": "qmd/training/model-parallelism/index.html#tensor-parallelism",
    "title": "",
    "section": "Tensor Parallelism",
    "text": "Tensor Parallelism\nIn Tensor Parallelism each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.\nIn this section we use concepts and diagrams from the Megatron-LM paper: Efficient Large-Scale Language Model Training on GPU Clusters.\nThe main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU.\nFollowing the Megatron‚Äôs paper notation, we can write the dot-product part of it as Y = GeLU(XA), where X and Y are the input and output vectors, and A is the weight matrix.\nIf we look at the computation in matrix form, it‚Äôs easy to see how the matrix multiplication can be split between multiple GPUs: \nIf we split the weight matrix A column-wise across N GPUs and perform matrix multiplications XA_1 through XA_n in parallel, then we will end up with N output vectors Y_1, Y_2, ..., Y_n which can be fed into GeLU independently: \nUsing this principle, we can update an MLP of arbitrary depth, without the need for any synchronization between GPUs until the very end, where we need to reconstruct the output vector from shards. The Megatron-LM paper authors provide a helpful illustration for that: \nParallelizing the multi-headed attention layers is even simpler, since they are already inherently parallel, due to having multiple independent heads! \nImportant: TP requires very fast network, and therefore since typically intra-node networks are much faster than inter-node networks it‚Äôs not advisable to do TP across nodes. Practically, if a node has 4 GPUs, the highest TP degree is therefore 4. If you need a TP degree of 8, you need to use nodes that have at least 8 GPUs.\nImportant: TP degree shouldn‚Äôt span across nodes. For example if the node has 8 gpus, TP degree should be no more than 8.\nTP can combined with other parallelization methods.\nAlternative names: - DeepSpeed calls it tensor slicing\nImplementations: - Megatron-LM has an internal implementation, as it‚Äôs very model-specific - PyTorch - SageMaker - this is a proprietary solution that can only be used on AWS. - OSLO has the tensor parallelism implementation based on the Transformers. - nanotron - parallelformers (only inference at the moment)",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#dppp",
    "href": "qmd/training/model-parallelism/index.html#dppp",
    "title": "",
    "section": "DP+PP",
    "text": "DP+PP\nThe following diagram from the DeepSpeed pipeline tutorial demonstrates how one combines DP with PP.\n\n\n\ndp-pp-2d\n\n\nHere it‚Äôs important to see how DP rank 0 doesn‚Äôt see GPU2 and DP rank 1 doesn‚Äôt see GPU3. To DP there is just GPUs 0 and 1 where it feeds data as if there were just 2 GPUs. GPU0 ‚Äúsecretly‚Äù offloads some of its load to GPU2 using PP. And GPU1 does the same by enlisting GPU3 to its aid.\nSince each dimension requires at least 2 GPUs, here you‚Äôd need at least 4 GPUs.\nImplementations: - DeepSpeed - Megatron-LM - Varuna - SageMaker - OSLO - nanotron",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#dppptp",
    "href": "qmd/training/model-parallelism/index.html#dppptp",
    "title": "",
    "section": "DP+PP+TP",
    "text": "DP+PP+TP\nTo get an even more efficient training a 3D parallelism is used where PP is combined with TP and DP. This can be seen in the following diagram.\n\n\n\ndp-pp-tp-3d\n\n\nThis diagram is from a blog post 3D parallelism: Scaling to trillion-parameter models, which is a good read as well.\nSince each dimension requires at least 2 GPUs, here you‚Äôd need at least 8 GPUs.\nImplementations: - DeepSpeed - DeepSpeed also includes an even more efficient DP, which they call ZeRO-DP. - Megatron-LM - Varuna - SageMaker - OSLO - nanotron",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#zero-dppptp",
    "href": "qmd/training/model-parallelism/index.html#zero-dppptp",
    "title": "",
    "section": "ZeRO DP+PP+TP",
    "text": "ZeRO DP+PP+TP\nOne of the main features of DeepSpeed is ZeRO, which is a super-scalable extension of DP. It has already been discussed in ZeRO Data Parallelism. Normally it‚Äôs a standalone feature that doesn‚Äôt require PP or TP. But it can be combined with PP and TP.\nWhen ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimizer sharding).\nWhile it‚Äôs theoretically possible to use ZeRO stage 2 (gradient sharding) with Pipeline Parallelism, it will have bad performance impacts. There would need to be an additional reduce-scatter collective for every micro-batch to aggregate the gradients before sharding, which adds a potentially significant communication overhead. By nature of Pipeline Parallelism, small micro-batches are used and instead the focus is on trying to balance arithmetic intensity (micro-batch size) with minimizing the Pipeline bubble (number of micro-batches). Therefore those communication costs are going to hurt.\nIn addition, there are already fewer layers than normal due to PP and so the memory savings won‚Äôt be huge. PP already reduces gradient size by 1/PP, and so gradient sharding savings on top of that are less significant than pure DP.\nZeRO stage 3 is not a good choice either for the same reason - more inter-node communications required.\nAnd since we have ZeRO, the other benefit is ZeRO-Offload. Since this is stage 1 optimizer states can be offloaded to CPU.\nImplementations: - Megatron-DeepSpeed and Megatron-Deepspeed from BigScience, which is the fork of the former repo. - OSLO\nImportant papers:\n\nUsing DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#sequence-parallelism",
    "href": "qmd/training/model-parallelism/index.html#sequence-parallelism",
    "title": "",
    "section": "Sequence Parallelism",
    "text": "Sequence Parallelism\nML tasks, such as DNA sequencing, may require training with very long sequence lengths (e.g.¬†256K), and even normal LLMs could be trained on sequences of 10k and longer.\nSelf-Attention, which is the key component of Transformers, suffers from quadratic memory requirements with respect to the sequence length, therefore when sequence length gets to a certain length, even a batch size of 1 might not be able to fit onto a single GPU and require additional partitioning along the sequence dimension. And once this is done, the sequence can be of any length.\nAs this type of parallelism is orthogonal to the other parallelization types described in this document, it can be combined with any of them, leading to 4D, ZeRO-DP+SP and other combinations.\n\nDeepspeed-Ulysses SP\nPaper: DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models\nIn this implementation 2 elements are sharded: 1. The multiple-head attention weights are split across the participating GPUs so that each GPU has a few sub-heads only. This is done when the model is created/loaded. This is somewhat similar to Tensor Parallelism. 2. During training each input sequence is partitioned into chunks and each chunk is sent to one of the GPUs, which reminds us of ZeRO-3 sharding, except instead of weights the inputs are sharded.\nDuring compute each sequence chunk is projected onto QKV and then gathered to the full sequence QKV on each device, computed on each device only for the subheads it owns and then gathered again into the full attention output for the MLP block.\n\n\n\ndeepspeed-ulysses sp\n\n\nsource\nOn the diagram: 1. Input sequences N are partitioned across P available devices. 2. Each local N/P partition of the input sequence is projected into queries (Q), keys (K) and values (V) embeddings. 3. Next, local QKV embeddings are gathered into global QKV through highly optimized all-to-all collectives between participating compute devices. 4. Then the attention computation per head is performed:\n\n\n\nmath\n\n\n\nAt the end another all-to-all collective transforms output context tensor of attention computation to sequence (N/P) parallel for subsequent operators (MLP MatMul, layer norm, etc.) in the remaining modules of transformer layer block.\n\nExample: Let‚Äôs consider seqlen=8K, num_heads=128 and a single node of num_gpus=8\n\neach GPU gets a 1K-long chunk of the original sequence (8K/8)\neach GPU gets assigned 16 sub-heads (128/8)\n\non gpu0 before forward the original sequence is gathered back into 8K tokens\nthe attention computation is done on the first 16 sub-heads the same logic is performed on the remaining 7 GPUs, each computing 8k attention over its 16 sub-heads\n\n\nYou can read the specifics of the very efficient comms here.\nDeepSpeed-Ulysses keeps communication volume consistent by increasing GPUs proportional to message size or sequence length.\n\n\nColossal-AI‚Äôs SP\nPaper: Sequence parallelism: Long sequence training from system perspective\nColossal-AI‚Äôs SP implementation uses ring self-attention, a ring-like communication collective in which query projections are local whereas key and values projections are transmitted in a ring-style to compute global attention, resulting in communication complexity linear in message size, M.\n\n\nMegatron-LM‚Äôs SP\nPaper: Reducing Activation Recomputation in Large Transformer Models\nMegatron-LM‚Äôs SP is tightly integrated with its TP. Megatron-LM partitions sequence along sequence dimensions and applies allgather and reduce scatter collective to aggregate QKV projections for attention computation. Its communication volume increases linearly with message size (M) regardless of number of compute devices.\nImplementations: - Megatron-LM - Deepspeed - Colossal-AI",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#flexflow",
    "href": "qmd/training/model-parallelism/index.html#flexflow",
    "title": "",
    "section": "FlexFlow",
    "text": "FlexFlow\nFlexFlow also solves the parallelization problem in a slightly different approach.\nPaper: ‚ÄúBeyond Data and Model Parallelism for Deep Neural Networks‚Äù by Zhihao Jia, Matei Zaharia, Alex Aiken\nIt performs a sort of 4D Parallelism over Sample-Operator-Attribute-Parameter.\n\nSample = Data Parallelism (sample-wise parallel)\nOperator = Parallelize a single operation into several sub-operations\nAttribute = Data Parallelism (length-wise parallel)\nParameter = Model Parallelism (regardless of dimension - horizontal or vertical)\n\nExamples: * Sample\nLet‚Äôs take 10 batches of sequence length 512. If we parallelize them by sample dimension into 2 devices, we get 10 x 512 which becomes be 5 x 2 x 512.\n\nOperator\n\nIf we perform layer normalization, we compute std first and mean second, and then we can normalize data. Operator parallelism allows computing std and mean in parallel. So if we parallelize them by operator dimension into 2 devices (cuda:0, cuda:1), first we copy input data into both devices, and cuda:0 computes std, cuda:1 computes mean at the same time.\n\nAttribute\n\nWe have 10 batches of 512 length. If we parallelize them by attribute dimension into 2 devices, 10 x 512 will be 10 x 2 x 256.\n\nParameter\n\nIt is similar with tensor model parallelism or naive layer-wise model parallelism.\n\n\n\nflex-flow-soap\n\n\nThe significance of this framework is that it takes resources like (1) GPU/TPU/CPU vs.¬†(2) RAM/DRAM vs.¬†(3) fast-intra-connect/slow-inter-connect and it automatically optimizes all these algorithmically deciding which parallelisation to use where.\nOne very important aspect is that FlexFlow is designed for optimizing DNN parallelizations for models with static and fixed workloads, since models with dynamic behavior may prefer different parallelization strategies across iterations.\nSo the promise is very attractive - it runs a 30min simulation on the cluster of choice and it comes up with the best strategy to utilise this specific environment. If you add/remove/replace any parts it‚Äôll run and re-optimize the plan for that. And then you can train. A different setup will have its own custom optimization.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#inter-node-speed-requirements-to-use-zero",
    "href": "qmd/training/model-parallelism/index.html#inter-node-speed-requirements-to-use-zero",
    "title": "",
    "section": "Inter-node speed requirements to use ZeRO",
    "text": "Inter-node speed requirements to use ZeRO\nThe ZeRO scalability protocol, be it Deepspeed ZeRO or PyTorch FSDP, requires a lot more inter-node traffic than TP+PP+DP solutions, and sometimes it can‚Äôt take advantage of the faster intra-node connectivity, and therefore if your inter-node network is slow your expensive GPUs might be massively bottlenecked by the comms.\nThe ZeRO protocol partially overlaps comms with compute, so ideally you want to get close to comms_time &lt;= compute_time. The overlap is not perfect, so there will be always some network bottleneck, but we want to make sure that comms_time is not much larger than compute_time.\nIn ZeRO-3, we have all_gather on weights in forward, then all_gather on weights in backward, last is reduce_scatter on gradients in backward. In total there are 3 global collective calls each sending a model size multiplied by how many bytes per parameter are used. e.g.¬†a 10B param model in bf16 under ZeRO-3 will need to send 1023=60GB of data.\nIn comparison DistributedDataParallel (DDP) uses a single all_reduce call, but which requires 2x data transmission, and so a 10B param model in bf16 under DDP will need to send 1022=40GB of data.\nZeRO-1 which only shards the optimiser states, like DDP, will too need to transmit 40GB of data (one all_gather and one reduce_scatter.)\nHere is how to calculate time in seconds for communication and compute:\n\ncomms_time = n_transmissions * n_bytes * model_size_in_B / inter-node-throughput_in_GBps\ncompute_time = n_passes * n_bytes * model_size_in_B * seqlen * global_batch_size / (total_gpus * 1e3 * tflops_wo_comms)\n\nThe compute time formula is a rough estimate which works for any Transformer-block based model. It ignores any small computations and includes only the massive matmuls.\nAs an experiment let‚Äôs use the data points from IDEFICS-80B training.\nWhen we trained IDEFICS-80B with a 340GBs EFA we were getting only 90TFLOPs w/ Deepspeed ZeRO-3 on A100s as compared to 150+TFLOPs one was getting with Megatron‚Äôs TP+PP+DP. and moreover a big chunk of the model was frozen as were building a new models based on one language and one vision model. So our multiplier was less than 3. On the other hand we were using activation recomputation to save memory, so this is an additional transmission of all model weights and to top it all off since nccl wasn‚Äôt supporting proper half-precision reduction we used fp32 for gradient reductions, so really our multiplier wasn‚Äôt 3 but more like 4.5.\nValues used for IDEFICS-80B training: - model_size_in_B = 80 - n_bytes = 2 in case of bf16 which is 2 bytes - n_transmissions = 3 in the case of ZeRO-3/FSDP (1x reduce_scatter + 2x all_gather (fwd + bwd)) and 2 in case of ZeRO-1 (1x reduce_scatter + 1x all_gather), - additionally, in the case of IDEFICS-80B we decided to reduce grads in fp32 to minimize NCCL accumulation loss, so we actually had n_transmissions*n_bytes=3*2+2=4*2 for the additional 2 bytes but since half the model was frozen only about half of gradients were sent, so we still have the multiplier of 3. - n_passes = 4 with activation recomputation, or 3 w/o it. The model has to do only 1x compute per forward and 2x per backward (since the grads are calculated twice - once wrt inputs and once wrt weights). And with activation recomputation one more forward is done. - total_gpus = 512 - global_batch_size = 3584 - seqlen = 1024 - inter-node-throughput_in_GBps = 42.5 (340Gbps) (AWS EFA v1) -tflops_wo_comms is the tflops w/o the communication overhead. Not theoretical peak as that is unachievable, but perhaps 75% in the case of A100@BF16 - so 312*0.75=234 TFLOPS\nWe derived 340Gbps inter-node network throughput using all_reduce_bench.py which by default uses a payload of 4GB. In the case of IDEFICS-80B we had 80 layers, so approximately each layer was 1B params large. Which means that each layer was sending 2GB of data for bf16 tensors and 4GB of data with fp32 tensors, which matches the network benchmark. If you were to have a much smaller layer size, I‚Äôd recommend adapting the benchmark to that size. For example, if your layer size was only 100M param large, then your payload would be 0.2GB for bf16 tensors. As this is an order of magnitude smaller, the network is likely to give you a lower bandwidth, and you should use that in your calculations.\nfootnote: if parts of your model are frozen, then there will be less data sent in syncing the gradients. in IDEFICS we had more than half of the model frozen, so when grads were reduced we only had about half the traffic.\nWhich gives us:\n\ncomms = 3 * 2 * 80 / 42.5 = 11 sec\ncompute = 4 * 2 * 80 * 1024 * 3584 / (512 * 1e3 * 250) = 18 sec\n\nIf we check against our IDEFICS-80B logs, which had each iteration at about 49 seconds.\nSo the good news is that the math checks out as comms + compute are in the ballpark of the measured time, except\nWe can do another sanity check by feeding the compute formulae 90 TFLOPS that we logged, in which case:\n\ncompute = 4 * 2 * 80 * 1024 * 3584 / (512 * 1e3 * 90) = 51 sec\n\nand so 49 and 51 secs are pretty close. Except this tells us nothing since the logged TFLOPS were calculated using this formula, so, of course, it should match.\nWhat I‚Äôd expect in the best case is where I have used close to theoretical peak TFLOPS in the formula and received the compute estimate to be about the same as the actual compute time measured on the system. Remember that since comms are interleaved with compute, when we measure forward+backward wallclock time it includes comms in it.\nWhat‚Äôs the conclusion? I‚Äôd say more investigation is needed as clearly there are additional hidden bottlenecks here. I no longer have access to this setup to investigate, so I will repeat this exercise afresh when I train another largish model and share the updated math with you. But this workout should give you a feeling for what‚Äôs going on behind the scenes and how all these numbers work together.\nAlso this discussion didn‚Äôt include into the math gradient accumulation steps (GAS). In the case of IDEFICS-80B it wasn‚Äôt used. If GAS&gt;1 the theoretical compute time doesn‚Äôt change, but comms time instead of 3*2*M/GBps would become GAS*3*2*M/GBps. The weights gathering via all_gather for forward and backward would transpire as many times as there are gradient accumulation steps. In theory for grads it‚Äôd need to happen only once, but since there is no place to store intermediary grads of the gathered weight on each GPU it‚Äôll have to be reduced GAS times as well. This is for ZeRO-2 and ZeRO-3. For ZeRO-1 GAS&gt;1 requires no additional comms.\nWe also didn‚Äôt discuss the DataLoader as a potential bottleneck here, but we tested that it was under 1 sec, i.e.¬†a very small overhead.\nGoing back to comms math, we also didn‚Äôt take into an account various hardware latencies, but when dealing with a large payloads they shouldn‚Äôt add up a significant additional overhead.\nAnd now you know how long it‚Äôll take to transmit that many GBs over the network of your system. For example, if the network were to be 5x slower than the one we used for IDEFICS-80B training, that is 8.5GBps (68Gbps) then:\n\ncomms = 3 * 2 * 80 / 8.5 = 56 sec\n\nwhich would definitely be a huge bottleneck compared to the faster compute.\nIf the network were to be 5x faster, that is 212GBs (1700Gbps) then:\n\ncomms = 3 * 2 * 80 / 212 = 2 sec\n\nwhich would be insignificant comparatively to the compute time, especially if some of it is successfully overlapped with the commute.\nAlso the Deepspeed team empirically benchmarked a 176B model on 384 V100 GPUs (24 DGX-2 nodes) and found that:\n\nWith 100 Gbps IB, we only have &lt;20 TFLOPs per GPU (bad)\nWith 200-400 Gbps IB, we achieve reasonable TFLOPs around 30-40 per GPU (ok)\nFor 800 Gbps IB, we reach 40+ TFLOPs per GPU (excellent)\n\nTo remind the peak TFLOPS for NVIDIA V100 at fp16 is 125 TFLOPS.\nBut be careful here - this benchmark is for V100s! Which is about 2-3x slower than A100, and 4-8x slower than H100 for half-precision. So the comms have to be at least 4-8x faster for H100 nodes to match the above table at half precision. We need more benchmarks with more recent hardware.\nfootnote: the 2-3x range is because the official specs claim 3x TFLOPS increase for V100-&gt;A100, and A100-&gt;H100 each, but users benchmarking the difference report at most 2.5x improvements.\nThey also noticed that when training at scale, the communication overhead is more pronounced with small micro-batch size per GPU. And we may not be able to increase micro-batch size since global-batch size is often fixed to achieve good model convergence rate. This is solved by the recently introduced ZeRO++.\nFinally, when doing the math above you need to know the actual bandwidth you get on your setup - which changes with payload size - the larger the payload the better the bandwidth. To get this information you need to look at your reduce_bucket_size and prefetch_bucket_size settings in the Deepspeed configuration file for reduction and prefetch correspondingly. The default is 0.5B params, which is 1GB in half-precision (0.5B x 2 bytes), or 2GB (0.5B x 4 bytes) if you use fp32 precision. So in order to measure the actual throughput you need to run an all_reduce benchmark with that payload and see what bandwidth gets reported. Then you can feed it to the calculations above.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#which-strategy-to-use-when",
    "href": "qmd/training/model-parallelism/index.html#which-strategy-to-use-when",
    "title": "",
    "section": "Which Strategy To Use When",
    "text": "Which Strategy To Use When\nHere is a very rough outline at which parallelism strategy to use when. The first on each list is typically faster.\n‚á® Single GPU\n\nModel fits onto a single GPU:\n\nNormal use\n\nModel doesn‚Äôt fit onto a single GPU:\n\nZeRO + Offload CPU and optionally NVMe\nas above plus Memory Centric Tiling (see below for details) if the largest layer can‚Äôt fit into a single GPU\n\nLargest Layer not fitting into a single GPU:\n\n\nZeRO - Enable Memory Centric Tiling (MCT). It allows you to run arbitrarily large layers by automatically splitting them and executing them sequentially. MCT reduces the number of parameters that are live on a GPU, but it does not affect the activation memory. As this need is very rare as of this writing a manual override of torch.nn.Linear needs to be done by the user.\n\n‚á® Single Node / Multi-GPU\n\nModel fits onto a single GPU:\n\nDDP - Distributed DP\nZeRO - may or may not be faster depending on the situation and configuration used\n\nModel doesn‚Äôt fit onto a single GPU:\n\nPP\nZeRO\nTP\n\nWith very fast intra-node connectivity of NVLINK or NVSwitch all three should be mostly on par, without these PP will be faster than TP or ZeRO. The degree of TP may also make a difference. Best to experiment to find the winner on your particular setup.\nTP is almost always used within a single node. That is TP size &lt;= gpus per node.\nLargest Layer not fitting into a single GPU:\n\nIf not using ZeRO - must use TP, as PP alone won‚Äôt be able to fit.\nWith ZeRO see the same entry for ‚ÄúSingle GPU‚Äù above\n\n\n‚á® Multi-Node / Multi-GPU\n\nIf the model fits into a single node first try ZeRO with multiple replicas, because then you will be doing ZeRO over the faster intra-node connectivity, and DDP over slower inter-node\nWhen you have fast inter-node connectivity:\n\nZeRO - as it requires close to no modifications to the model\nPP+TP+DP - less communications, but requires massive changes to the model\n\nwhen you have slow inter-node connectivity and still low on GPU memory:\n\nDP+PP+TP+ZeRO-1",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/model-parallelism/index.html#contributors",
    "href": "qmd/training/model-parallelism/index.html#contributors",
    "title": "",
    "section": "Contributors",
    "text": "Contributors\nSamyam Rajbhandari, Horace He, Siddharth Singh, Olatunji Ruwase,",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Model Parallelism"
    ]
  },
  {
    "objectID": "qmd/training/reproducibility/index.html",
    "href": "qmd/training/reproducibility/index.html",
    "title": "",
    "section": "",
    "text": "QmdTrainingReproducibility",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Reproducibility"
    ]
  },
  {
    "objectID": "qmd/training/reproducibility/index.html#achieve-determinism-in-randomness-based-software",
    "href": "qmd/training/reproducibility/index.html#achieve-determinism-in-randomness-based-software",
    "title": "",
    "section": "Achieve determinism in randomness based software",
    "text": "Achieve determinism in randomness based software\nWhen debugging always set a fixed seed for all the used Random Number Generators (RNG) so that you get the same data / code path on each re-run.\nThough with so many different systems it can be tricky to cover them all. Here is an attempt to cover a few:\nimport random, torch, numpy as np\ndef enforce_reproducibility(use_seed=None):\n    seed = use_seed if use_seed is not None else random.randint(1, 1000000)\n    print(f\"Using seed: {seed}\")\n\n    random.seed(seed)    # python RNG\n    np.random.seed(seed) # numpy RNG\n\n    # pytorch RNGs\n    torch.manual_seed(seed)          # cpu + cuda\n    torch.cuda.manual_seed_all(seed) # multi-gpu - can be called without gpus\n    if use_seed: # slower speed! https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark     = False\n\n    return seed\na few possible others if you use those subsystems/frameworks instead:\n    torch.npu.manual_seed_all(seed)\n    torch.xpu.manual_seed_all(seed)\n    tf.random.set_seed(seed)\nWhen you rerun the same code again and again to solve some problem set a specific seed at the beginning of your code with:\nenforce_reproducibility(42)\nBut as it mentions above this is for debug only since it activates various torch flags that help with determinism but can slow things down so you don‚Äôt want this in production.\nHowever, you can call this instead to use in production:\nenforce_reproducibility()\ni.e.¬†w/o the explicit seed. And then it‚Äôll pick a random seed and log it! So if something happens in production you can now reproduce the same RNGs the issue was observed in. And no performance penalty this time, as the torch.backends.cudnn flags are only set if you provided the seed explicitly. Say it logged:\nUsing seed: 1234\nyou then just need to change the code to:\nenforce_reproducibility(1234)\nand you will get the same RNGs setup.\nAs mentioned in the first paragraphs there could be many other RNGs involved in a system, for example, if you want the data to be fed in the same order for a DataLoader you need to have its seed set as well.\nAdditional resources: - Reproducibility in pytorch",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Reproducibility"
    ]
  },
  {
    "objectID": "qmd/training/reproducibility/index.html#reproduce-the-software-and-system-environment",
    "href": "qmd/training/reproducibility/index.html#reproduce-the-software-and-system-environment",
    "title": "",
    "section": "Reproduce the software and system environment",
    "text": "Reproduce the software and system environment\nThis methodology is useful when discovering some discrepancy in outcomes - quality or a throughput for example.\nThe idea is to log the key components of the environment used to launch a training (or inference) so that if at a later stage it needs to be reproduced exactly as it was it can be done.\nSince there is a huge variety of systems and components being used it‚Äôs impossible to prescribe a way that will always work. So let‚Äôs discuss one possible recipe and you can then adapt it to your particular environment.\nThis is added to your slurm launcher script (or whatever other way you use to launch the training) - this is Bash script:\nSAVE_DIR=/tmp # edit to a real path\nexport REPRO_DIR=$SAVE_DIR/repro/$SLURM_JOB_ID\nmkdir -p $REPRO_DIR\n# 1. modules (writes to stderr)\nmodule list 2&gt; $REPRO_DIR/modules.txt\n# 2. env\n/usr/bin/printenv | sort &gt; $REPRO_DIR/env.txt\n# 3. pip (this includes devel installs SHA)\npip freeze &gt; $REPRO_DIR/requirements.txt\n# 4. uncommitted diff in git clones installed into conda\nperl -nle 'm|\"file://(.*?/([^/]+))\"| && qx[cd $1; if [ ! -z \"\\$(git diff)\" ]; then git diff &gt; \\$REPRO_DIR/$2.diff; fi]' $CONDA_PREFIX/lib/python*/site-packages/*.dist-info/direct_url.json\nAs you can see this recipe is used in a SLURM environment, so every new training will dump the environment specific to the SLURM job.\n\nWe save which modules were loaded, e.g.¬†in cloud cluster/HPC setups you‚Äôre like to be loading the CUDA and cuDNN libraries using this\nIf you don‚Äôt use modules then remove that entry\nWe dump the environment variables. This can be crucial since a single env var like LD_PRELOAD or LD_LIBRARY_PATH could make a huge impact on performance in some environments\nWe then dump the conda environment packages and their versions - this should work with any virtual python environment.\nIf you use a devel install with pip install -e . it doesn‚Äôt know anything about the git clone repository it was installed from other than its git SHA. But the issue is that it‚Äôs likely that you have modified the files locally and now pip freeze will miss those changes. So this part will go through all packages that are not installed into the conda environment (we find them by looking inside site-packages/*.dist-info/direct_url.json)\n\nAn additionally useful tool is conda-env-compare.pl which helps you to find out the exact differences 2 conda environments have.\nAnecdotally, me and my colleague were getting very different training TFLOPs on a cloud cluster running the exact same code - literally launching the same slurm script from the same shared directory. We first compared our conda environments using conda-env-compare.pl and found some differences - I installed the exact packages she had to match her environment and it was still showing a huge performance difference. We then compared the output of printenv and discovered that I had LD_PRELOAD set up and she didn‚Äôt - and that made a huge difference since this particular cloud provider required multiple env vars to be set to custom paths to get the most of their hardware.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "Qmd",
      "Training",
      "Reproducibility"
    ]
  },
  {
    "objectID": "qmd/orchestration/index.html",
    "href": "qmd/orchestration/index.html",
    "title": "üéª Orchestration",
    "section": "",
    "text": "See Slurm/\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üéª  Orchestration"
    ]
  },
  {
    "objectID": "qmd/insights/index.html",
    "href": "qmd/insights/index.html",
    "title": "üß† Insights",
    "section": "",
    "text": "This chapter is one person‚Äôs opinionated overview of the ML/AI Engineering reality, which may or may not be another person‚Äôs reality. The intention is to help you start asking the right questions and get your ML Engineering needs met.\nSee The AI Battlefield Engineering ‚Äì What You Need to Know\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2024,\n  author = {Foreman, Sam and Bekman, Stas},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam, and Stas Bekman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://samforeman.me.",
    "crumbs": [
      "[{{< iconify line-md twitter >}}]{style='font-size: 1.15em;'}",
      "üß†  Insights"
    ]
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "",
    "section": "",
    "text": "TODO\n\nDone:\n\nüìÇ compute/\nüìÇ debug/\nüìÇ images/\nüìÇ insights/\nüìÇ network/\n\nTo Do:\n\nüìÇ orchestration/\nüìÇ performance/\nüìÇ resources/\nüìÇ storage/\nüìÇ testing/\nüìÇ training/\nüìÇ transformers/\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{bekman2024,\n  author = {Bekman, Stas and Foreman, Sam},\n  title = {ML {Engineering}},\n  date = {2024-02-13},\n  url = {https://saforem2.github.io/ml-engineering},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBekman, Stas, and Sam Foreman. 2024. ‚ÄúML Engineering.‚Äù\nFebruary 13, 2024. https://saforem2.github.io/ml-engineering."
  }
]