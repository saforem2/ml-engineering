<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-02-13">

<title>ML Engineering - 🛜 Network</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../qmd/network/benchmarks/index.html" rel="next">
<link href="../../qmd/insights/ai-battlefield.html" rel="prev">
<link href="../.././favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../../site_libs/quarto-contrib/iconify-1.0.8/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
</script>
<link href="https://pvinis.github.io/iosevka-webfont/3.4.1/iosevka.css" rel="stylesheet">


<link rel="stylesheet" href="../../css/default.css">
<link rel="stylesheet" href="../../css/callouts.css">
<meta property="og:title" content="Sam Foreman">
<meta property="og:description" content="Machine Learning Engineering Open Book">
<meta property="og:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta property="og:site_name" content="ML Engineering">
<meta name="twitter:title" content="Sam Foreman">
<meta name="twitter:description" content="Machine Learning Engineering Open Book">
<meta name="twitter:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="ML Engineering">
<meta name="citation_author" content="Stas Bekman">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-02-13">
<meta name="citation_cover_date" content="2024-02-13">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-02-13">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/ml-engineering">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">ML Engineering</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/saforem2"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:twitter"></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/ml-engineering"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-gear"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/blob/main/index.qmd">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/issues/new/choose">
            New Issue
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../qmd/network/index.html">🛜 Network</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/performance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏎️ Performance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📓 Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/testing/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">✏️ Testing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/transformers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🤗 Transformers</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/compute/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">💻 Compute</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/compute/cpu-memory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU memory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/compute/cpu/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/compute/accelerator/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accelerators</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Nvidia</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/compute/accelerator/nvidia/debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Troubleshooting NVIDIA GPUs</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/debug/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🐛  Debugging</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/tiny-scripts/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Back up of scripts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/make-tiny-models-tokenizers-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Faster debug and development with tiny models, tokenizers and datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/nccl-performance-debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NCCL: Debug and Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debugging PyTorch programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debug Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/torch-distributed-hanging-solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/debug/underflow_overflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Underflow and Overflow Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/insights/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🧠  Insights</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/insights/ai-battlefield.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🪖 The AI Battlefield</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/network/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">🛜 Network</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/network/benchmarks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networking Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/network/benchmarks/results/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Network Benchmarks Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/network/benchmarks/results/disable-nvlink.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Disabling NVLink Benchmark</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/orchestration/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎻  Orchestration</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/orchestration/slurm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Working in SLURM Environment</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/orchestration/slurm/admin.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Administration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/orchestration/slurm/launchers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Launchers with SLURM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/orchestration/slurm/performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/orchestration/slurm/users.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM for users</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/storage/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📦  Storage</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
 <span class="menu-text">Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false">
 <span class="menu-text">Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/storage/benchmarks/results/hope-2023-12-20-14-37-02-331702-summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">fio benchmark results for hope on 2023-12-20-14:37:02</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏋️  Training</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/dtype.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tensor precision / Data types</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/emulate-multi-node.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Emulate a multi-node setup using just a single node</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/hparams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Selecting Training Hyper-Parameters And Model Initializations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/checkpoints/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Checkpoints</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/fault-tolerance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/model-parallelism/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/performance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Software Tune Up For The Best Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/reproducibility/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/re-train-hub-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Re-train HF Hub Models From Scratch Using Finetuning Examples</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../qmd/training/instabilities/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Avoiding, Recovering From and Understanding Instabilities</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../qmd/training/instabilities/training-loss-patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Understanding Training Loss Patterns</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#inter-node-and-intra-node-networking-hardware" id="toc-inter-node-and-intra-node-networking-hardware" class="nav-link active" data-scroll-target="#inter-node-and-intra-node-networking-hardware">Inter-node and intra-node Networking Hardware</a>
  <ul class="collapse">
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary">Glossary</a></li>
  <li><a href="#understanding-why-inter-node-network-speed-is-of-a-huge-importance" id="toc-understanding-why-inter-node-network-speed-is-of-a-huge-importance" class="nav-link" data-scroll-target="#understanding-why-inter-node-network-speed-is-of-a-huge-importance">Understanding why inter-node network speed is of a huge importance</a>
  <ul class="collapse">
  <li><a href="#the-basics" id="toc-the-basics" class="nav-link" data-scroll-target="#the-basics">The basics</a></li>
  <li><a href="#gpu-training" id="toc-gpu-training" class="nav-link" data-scroll-target="#gpu-training">1-GPU training</a></li>
  <li><a href="#single-node-training" id="toc-single-node-training" class="nav-link" data-scroll-target="#single-node-training">Single node training</a></li>
  <li><a href="#multiple-node-training" id="toc-multiple-node-training" class="nav-link" data-scroll-target="#multiple-node-training">Multiple node training</a></li>
  <li><a href="#large-model-training" id="toc-large-model-training" class="nav-link" data-scroll-target="#large-model-training">Large model training</a></li>
  <li><a href="#unidirectional-vs-bidirectional-duplex" id="toc-unidirectional-vs-bidirectional-duplex" class="nav-link" data-scroll-target="#unidirectional-vs-bidirectional-duplex">Unidirectional vs Bidirectional (Duplex)</a></li>
  </ul></li>
  <li><a href="#intra-node-networking" id="toc-intra-node-networking" class="nav-link" data-scroll-target="#intra-node-networking">Intra-node networking</a>
  <ul class="collapse">
  <li><a href="#pcie" id="toc-pcie" class="nav-link" data-scroll-target="#pcie">PCIe</a></li>
  <li><a href="#nvlink" id="toc-nvlink" class="nav-link" data-scroll-target="#nvlink">NVLink</a></li>
  <li><a href="#nvswitch" id="toc-nvswitch" class="nav-link" data-scroll-target="#nvswitch">NVSwitch</a></li>
  <li><a href="#infinity-fabric-xgmi" id="toc-infinity-fabric-xgmi" class="nav-link" data-scroll-target="#infinity-fabric-xgmi">Infinity Fabric / xGMI</a></li>
  <li><a href="#gaudi2" id="toc-gaudi2" class="nav-link" data-scroll-target="#gaudi2">Gaudi2</a></li>
  </ul></li>
  <li><a href="#numa-affinity" id="toc-numa-affinity" class="nav-link" data-scroll-target="#numa-affinity">NUMA Affinity</a></li>
  <li><a href="#inter-node-networking" id="toc-inter-node-networking" class="nav-link" data-scroll-target="#inter-node-networking">Inter-node networking</a>
  <ul class="collapse">
  <li><a href="#efa" id="toc-efa" class="nav-link" data-scroll-target="#efa">EFA</a></li>
  <li><a href="#infiniband" id="toc-infiniband" class="nav-link" data-scroll-target="#infiniband">InfiniBand</a></li>
  <li><a href="#gaudi2-1" id="toc-gaudi2-1" class="nav-link" data-scroll-target="#gaudi2-1">Gaudi2</a></li>
  <li><a href="#hpe-slingshot-interconnect" id="toc-hpe-slingshot-interconnect" class="nav-link" data-scroll-target="#hpe-slingshot-interconnect">HPE Slingshot interconnect</a></li>
  <li><a href="#omnipath" id="toc-omnipath" class="nav-link" data-scroll-target="#omnipath">OmniPath</a></li>
  </ul></li>
  <li><a href="#important-nuances" id="toc-important-nuances" class="nav-link" data-scroll-target="#important-nuances">Important nuances</a>
  <ul class="collapse">
  <li><a href="#real-network-throughput" id="toc-real-network-throughput" class="nav-link" data-scroll-target="#real-network-throughput">Real network throughput</a></li>
  <li><a href="#latency" id="toc-latency" class="nav-link" data-scroll-target="#latency">Latency</a></li>
  <li><a href="#proprietary-network-hardware-and-nccl" id="toc-proprietary-network-hardware-and-nccl" class="nav-link" data-scroll-target="#proprietary-network-hardware-and-nccl">Proprietary network hardware and NCCL</a></li>
  <li><a href="#node-proximity" id="toc-node-proximity" class="nav-link" data-scroll-target="#node-proximity">Node Proximity</a></li>
  <li><a href="#shared-internode-network" id="toc-shared-internode-network" class="nav-link" data-scroll-target="#shared-internode-network">Shared internode network</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/qmd/network/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/qmd/network/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">🛜 Network</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="https://github.com/saforem2/ml-engineering/blob/main/qmd/network/index.qmd"><i class="bi"></i></button></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading"></div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 13, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="inter-node-and-intra-node-networking-hardware" class="level1">
<h1>Inter-node and intra-node Networking Hardware</h1>
<p><strong>Subsections</strong>:</p>
<ul>
<li><a href="benchmarks">Benchmarks</a></li>
</ul>
<p>This chapter is a WIP</p>
<p>It’s not enough to buy/rent expensive accelerators to train and infer models fast. You need to ensure that your <a href="../storage">storage IO</a>, <a href="../compute/cpu">CPU</a> and network are fast enough to “feed the accelerator furnace”. If this is not ensured then the expensive accelerators will be underutilized leading to lost $$, slower training time and inference throughput. While it can be any other of the mentioned components, the network is often the bottleneck during the training (assume your DataLoader is fast).</p>
<p>If your model fits on a single accelerator, you have little to worry about. But nowadays most models require several accelerators to load and LLM/VLM models require multiple compute nodes for training and some even for inference.</p>
<p>Most compute nodes contain 8 accelerators, some 4, others 16, and even more accelerators and recently there are some that have one super-accelerator per node.</p>
<p>When the model spans several accelerators and doesn’t leave a single node all you need to worry about is fast <a href="#intra-node-networking">Intra-node networking</a>. As soon as the model requires several nodes, which is often the case for training as one can use multiple replicas to parallelize and speed up the training, then fast <a href="#inter-node-networking">Inter-node networking</a> becomes the key.</p>
<p>This article covers both types of networking hardware, reports their theoretical and effective bandwidths and explains how they inter-play with each other.</p>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<ul>
<li>DMA: Direct Memory Access</li>
<li>EFA: Elastic Fabric Adapter</li>
<li>HCA: Host Channel Adapter</li>
<li>IB: Infiniband</li>
<li>MFU: Model Flops Utilization (e.g.&nbsp;<code>mfu=0.5</code> at half-precision on A100 comes from getting 156TFLOPs, because peak half-precision spec is 312TFLOPS, and thus <code>156/312=0.5</code>)</li>
<li>NIC: Network Interface Card</li>
<li>OPA: Omni-Path Architecture</li>
<li>RoCE: RDMA over Converged Ethernet</li>
<li>RoE: RDMA over Ethernet</li>
<li>VPI: Virtual Protocol Interconnect</li>
<li>RDMA: Remote Direct Memory Access</li>
<li>xGMI: Socket to Socket Global Memory Interface</li>
</ul>
<p>Speed-related: - Bi-directional, Duplex: a transmission from one point to another in both directions A &lt;-&gt; B, typically 2x speed of unidirectional - GBps, GB/s: Gigabytes per secs (1GBps = 8Gbps) transferred in a channel - GT/s: GigaTransfers per second - the number of operations transferring data that occur in each second. - Gbps, Gb/s: Gigabits per secs (1Gbps = 1/8GBps) transferred in a channel - Unidirectional: a transmission from one point to another in one direction A -&gt; B</p>
</section>
<section id="understanding-why-inter-node-network-speed-is-of-a-huge-importance" class="level2">
<h2 class="anchored" data-anchor-id="understanding-why-inter-node-network-speed-is-of-a-huge-importance">Understanding why inter-node network speed is of a huge importance</h2>
<p>This is probably one of the most important multi-segment section that you really want to understand well. While it seeks out to show how important the inter-node speed is, to build up the case it’ll teach on the way many important training-related concepts.</p>
<section id="the-basics" class="level3">
<h3 class="anchored" data-anchor-id="the-basics">The basics</h3>
<p>First, let’s get a bit of a feeling what all those Gbps/GBps practically mean.</p>
<p>If your model is 80B parameter large, and you need to transmit every parameter or a gradient on the network even once in float32 (fp32) format, which requires 4 bytes per parameter, so you need to send <code>80*4</code> 320GB of data, or 2560Gb (<code>*8</code>). If your network’s bandwidth is 200Gbps it will take 12.8 seconds (<code>2560/200</code>) to transmit. And if you had 1600Gbps network then it’d take only 1.6 seconds. Why does it matter?</p>
</section>
<section id="gpu-training" class="level3">
<h3 class="anchored" data-anchor-id="gpu-training">1-GPU training</h3>
<p>Let’s start with a much smaller model of say 2B params, to train it you’d need at least <a href="../training/performance/README.md#anatomy-of-models-memory-usage">18 bytes per parameter</a> in mixed half precision. So <code>18*2</code> 36GB of memory just for model weights, optimizer states and gradients. Plus you need additional memory for activations and it’ll depend on the batch size and sequence length. But with 80GB A100 GPU we can definitely train this model on a single GPU.</p>
<p>We then assume for the moment that the DataLoader is fast enough to be negligible in duration compared to the compute time. And thus we get a close to a perfect MFU (Model FLOPs Utilization):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[DL][</span>  compute  ]<span class="pp">[</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]<span class="pp">[</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">---------------------------------------------------</span><span class="op">&gt;</span> time</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which means that the GPU just needs to do many matmuls and it’d do it amazing fast. In this situation you get the highest ROI (Return on Investment).</p>
</section>
<section id="single-node-training" class="level3">
<h3 class="anchored" data-anchor-id="single-node-training">Single node training</h3>
<p>The previous situation was fantastic due to the close to perfect MFU, but you realize that the training on a single GPU is going to take quite some time, since we are in AI race you’d probably want to finish the training sooner than later. So you’d ask - can I train the model on 8 GPUs instead, and the answer would be - yes, of course. With one caveat - at the end of each iteration you’d need to sync the gradients between the 8 processes (each process for a single GPU), so that each participating process of the training can benefit from what the other 7 have learned during the last iteration.</p>
<p>footnote: You could, of course, use less than 8 GPUs, it is just that most NVIDIA GPU-based compute nodes these days have 8 GPUs so why not get the best return on investment.</p>
<p>footnote: in the ideal world the training on 1 gpu for 8 durations of time, should cost the same as training on 8 gpus for 1 duration of time. That’s one would expect to spend the same $$ and to finish 8 times faster. But because of data synchronization requirements.</p>
<p>If the experimental model still contains 2B params like in the previous section and grads are in fp32 then the training program needs to send 8GB (<code>2G * 4B</code>) of data on every iteration. Moreover, since syncing the gradients requires an <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#collective-communication"><code>all_reduce</code> collective</a> collective - it needs to transmit the data twice - the first time sending the gradient data by each gpu, computing the sum of gradients and send this value back to each participating gpu so that each training process will benefit from the learning advancements each of its peers made in the last iteration.</p>
<p>Here is the all-reduce collective visualized:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/all-reduce-collective.png" class="img-fluid figure-img"></p>
<figcaption>all-reduce</figcaption>
</figure>
</div>
<p>(<a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#collective-communication">source</a>)</p>
<p>So we need to send 8GB twice, which means we need to send 16GB of data.</p>
<p>footnote: and to be exact the 2x comms volume for all-reduce is really <code>2*(n-1)/n</code> where n is the number of participating gpus. So if n=2, the coefficient is just 1 since <code>2*(2-1)/2=1</code> and 1.75 for n=8 since <code>2*(8-1)/8=1.75</code> and it becomes already very close to 2 at n=64.</p>
<p>footnote: there is also the important issue of latency of the network - which is multiplied several times due to how data is gathered from all participating gpus. But, given that here we are moving a very large payload the latency contributes a very small overhead and for simplicity can be ignored.</p>
<p>How long will it take to send 16GB of data?</p>
<ul>
<li>A100 @ 300GBps: <code>16/300</code> = 0.053 secs</li>
<li>H100 @ 450GBps: <code>16/450</code> = 0.035 secs</li>
</ul>
<p>which is incredibly fast!</p>
<p>And here is how our timeline will look like:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[DL][</span>  compute ]<span class="pp">[</span><span class="ss">comms</span><span class="pp">][</span><span class="ss">DL</span><span class="pp">]</span>[  compute ]<span class="pp">[</span><span class="ss">comms</span><span class="pp">][</span><span class="ss">DL</span><span class="pp">]</span>[  compute ]<span class="pp">[</span><span class="ss">comms</span><span class="pp">]</span><span class="kw">|</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-----------------------------------------------------------------------</span><span class="op">&gt;</span> time</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span><span class="op">&lt;</span>---- <span class="ex">iteration</span> <span class="at">----</span><span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>---- <span class="ex">iteration</span> <span class="at">----</span><span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>---- <span class="ex">iteration</span> <span class="at">-----</span><span class="op">&gt;|</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>oh and this whole synchronization protocol is called DDP (<a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a>) in the PyTorch lingo.</p>
<section id="comms-and-compute-overlap" class="level4">
<h4 class="anchored" data-anchor-id="comms-and-compute-overlap">Comms and compute overlap</h4>
<p>Even with this really fast comms the network still creates a bottleneck and leads to a short idling of the gpus. To solve this issue the advanced algorithms implement an overlap of comms and compute. Until now we approached the problem as one single transmission, but in reality each model is made of many layers and each layer can transmit the gradients it has computed, while the next layer is computing its gradients. So if you look at the level of the model, what happens in the <code>backward</code> path is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">[</span>   compute   ][   compute   <span class="er">][</span>   <span class="ex">compute</span>   ]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>               <span class="ex">[comms]</span>        <span class="pp">[</span><span class="ss">comms</span><span class="pp">]</span>        <span class="pp">[</span><span class="ss">comms</span><span class="pp">]</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">---------------------------------------------</span><span class="op">&gt;</span> time</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>- <span class="ex">layer</span> <span class="at">-1</span> <span class="at">-</span><span class="op">&gt;|&lt;</span>- layer <span class="at">-2</span> <span class="at">-</span><span class="op">&gt;|&lt;</span>- layer <span class="at">-3</span> <span class="at">-</span><span class="op">&gt;|</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>so once the last layer (-1) computed its gradients it all-reduces them while the 2nd to last layer performs its <code>backward</code>, and so on, until the first layer finished with gradients and it finally sends its gradients out.</p>
<p>So now you understand how overlapping works, So we can now update our bigger picture diagram to be:</p>
<p>Now our timing diagram becomes very similar to the diagram we had for a single gpu:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[DL][</span>  compute  ]<span class="pp">[</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]<span class="pp">[</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">[</span>  comms <span class="bu">]</span>       <span class="bu">[</span>  comms]        [  comms]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="er">---------------------------------------------------&gt;</span> <span class="er">time</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>--iteration--<span class="op">&gt;|</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and we hope that comms are faster than DL+compute, since if they aren’t faster than we have the following gpu idling gaps:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[DL][</span>  compute  ]<span class="pp">[</span><span class="ss">idle</span><span class="pp">][</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]<span class="pp">[</span><span class="ss">idle</span><span class="pp">][</span><span class="ss">DL</span><span class="pp">]</span>[  compute  ]<span class="pp">[</span><span class="ss">idle</span><span class="pp">]</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">[</span>         comms       ][         comms       <span class="er">][</span>         <span class="ex">comms</span>       ]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="ex">----------------------------------------------------------------------</span><span class="op">&gt;</span> time</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span><span class="op">&lt;</span>---  <span class="ex">iteration</span>  <span class="at">---</span><span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>---  <span class="ex">iteration</span>  <span class="at">---</span><span class="op">&gt;|</span><span class="kw">|</span><span class="op">&lt;</span>---  <span class="ex">iteration</span>  <span class="at">---</span><span class="op">&gt;|</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="calculating-tflops" class="level4">
<h4 class="anchored" data-anchor-id="calculating-tflops">Calculating TFLOPS</h4>
<p>Calculating TFLOPS answers the question of how long will it take to perform a compute.</p>
<p>There is a bit of nomenclature confusion here as TFLOPS as the final <code>s</code> sometimes means <code>sec</code> and at other times just <code>ops</code>.</p>
<p>For example, when you read, the <a href="https://www.nvidia.com/en-us/data-center/a100/#specifications">A100 spec</a> the TFLOPS there means TeraFloatingPointOperations per second.</p>
<p>So let’s define these abbreviations exactly:</p>
<ul>
<li>TFLOPS - TeraFLoatingpointOPerations per Second (another way is TFLOP/s)</li>
<li>TFLOP - TeraFLoatingpointOPerations (or TFLOPs - lower case <code>s</code> but it’s already confusing)</li>
</ul>
<p>Also see the <a href="https://en.wikipedia.org/wiki/FLOPS">wiki page</a> for more clarifications.</p>
<p>For GPT-family of decoder transformers models we can use the math described in this <a href="https://github.com/bigscience-workshop/bigscience/tree/master/math#calculate-tflops">BLOOM-176 docs</a>:</p>
<p>Here is how many TFLOP are processed per second:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">tflops</span> = model_size_in_B <span class="pp">*</span> 4 <span class="pp">*</span> 2 <span class="pp">*</span> seqlen <span class="pp">*</span> global_batch_size / <span class="er">(</span><span class="ex">time_in_sec_per_interation</span> <span class="pp">*</span> total_gpus <span class="pp">*</span> 1e3<span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This formula assume one uses <a href="../training/performance/README.md#gradient-checkpointing">activation recomputation</a> which saves GPU memory while introducing a smallish overhead. If one doesn’t use it then replace <code>4</code> with <code>3</code> as the model has to do only 1x compute per <code>forward</code> and 2x per <code>backward</code> (since the grads are calculated twice - once for inputs and once for weights). With activation recomputation the <code>forward</code> is done twice and thus you have an additional path which leads to a multiplier of <code>4</code> instead of <code>3</code></p>
<p>footnote: activation recomputation and gradient checkpointing both refer to the same technique.</p>
<p>so let’s remove the time component, which will give us the total TFLOP</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">tflop</span> = model_size_in_B <span class="pp">*</span> 4 <span class="pp">*</span> 2 <span class="pp">*</span> seqlen <span class="pp">*</span> global_batch_size / <span class="er">(</span><span class="ex">total_gpus</span> <span class="pp">*</span> 1e3<span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So let’s say we have: - <code>seqlen=2048</code> (sequence length) - <code>global_batch_size=16</code></p>
<p>and we already defined: - <code>total_gpus=8</code> - <code>model_size_in_B=2</code></p>
<p>This gives us:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">tflops</span> = 2 <span class="pp">*</span> 4 <span class="pp">*</span> 2 <span class="pp">*</span> 2048 <span class="pp">*</span> 16 / <span class="er">(</span><span class="ex">8</span> <span class="pp">*</span> 1e3<span class="kw">)</span> <span class="ex">=</span> 65.536 TFLOP</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So if we do a mixed half-precision training and most of the operations are done in half-precision then we can roughly say that we do <a href="https://www.nvidia.com/en-us/data-center/a100/#specifications">312 TFLOPS on A100</a> and usually a well optimized framework on a well-tuned hardware will do at least 50% MFU - that is it’ll be able to compute at about 1/2 peak performance.</p>
<p>footnote: It’s a ~3x <a href="https://www.nvidia.com/en-us/data-center/h100">989 TFLOPS on H100</a> (scroll to the end) and also it shows a misleading 2x numbers for sparsity so you have to mentally divide it by 2.</p>
<p>So continuing this train of thought it means that the setup will have about 156TFLOPS - and so it’ll take 0.42 secs to process a single iteration (2x <code>forward</code> and 2x <code>backward</code> compute) if we ignore the overhead of the DataLoader (which we hope is close to instant).</p>
<p>Earlier we said that a typical A100 node has an intra-node NVLink connection of 300GBps, and thus we said that to send 16GB of grads will take <code>16/300</code> = 0.053 secs.</p>
<p>And we measured our compute to be 0.42 secs, so here we have a problem as <code>0.053 &gt; 0.42</code> so the comms will be slower than compute and the network is a bottleneck.</p>
<p>You can now do several thought experiments - for example if you halve the batch size or the sequence length you will halve the compute time.</p>
<p>footnote: this is a very rough suggestions since GPUs work the fastest when the matrices they multiple are huge. But this is good enough for a simplified thought experiment we are having here. In reality halving the dimension will not halve the compute time.</p>
<p>OK, but hopefully at this point it’s quite clear that if you remain at the boundaries of a single node, you don’t need to worry about your GPUs idling.</p>
<p>But what if you want to speed up the training even more and throw say 4x 8-gpu nodes at it. (and of course you don’t have a choice but to use multiple nodes if you have a much larger model). Suddenly, the comms can become an even bigger bottleneck.</p>
</section>
</section>
<section id="multiple-node-training" class="level3">
<h3 class="anchored" data-anchor-id="multiple-node-training">Multiple node training</h3>
<p>So here we are continuing with the idea of 2B param model and we will now use 32 gpus across 4 nodes to speed up the training even more.</p>
<p>While each group of 8 gpus is still connected with super-fast NVLink technology, the inter-node connections are usually in an order of magnitude slower.</p>
<p>Let’s say you have a 200Gbps connection. Let’s repeat the math from the previous section of how long it’ll take to reduce 16GB of gradients.</p>
<p>16GB is 128Gb, and so at 200Gbps this will take 0.64 seconds.</p>
<p>And if stick to the compute taking 0.42 seconds, here we end up with comms taking longer than compute since <code>0.64 &gt; 0.42</code>.</p>
<p>Let’s bring both use cases together:</p>
<table class="table">
<thead>
<tr class="header">
<th>nodes</th>
<th>comms</th>
<th>compute</th>
<th>comms is a bottleneck</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.027</td>
<td>0.42</td>
<td>no</td>
</tr>
<tr class="even">
<td>4</td>
<td>0.64</td>
<td>0.42</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p>on this 200Gbps inter-node setup the comms are 23x slower than the same performed on an intra-node NVlink connections.</p>
<p>In this case even though we still have the much faster NVLink connection, we don’t really benefit from it, since the whole ensemble communicates at the speed of the slowest link. And that slowest link is the inter-node connection.</p>
<p>So in this particular situation if you were able to get a 400Gbps inter-node the speed would double and the comms will finish in 0.32 secs and thus will be faster than that 0.42 secs the compute would take.</p>
<p>footnote: you will never be able to get the advertised speed fully on the application level, so if it’s advertised as 400Gbps in the best case expect to get 320Gbps (about 80%). So make sure to take this into the account as well. Moreover, depending on the payload of each collective - the smaller the payload the smaller the actual network throughput will be.</p>
<p>And remember this was all handling a pretty tiny as considered these days 2B param model.</p>
<p>Now do the same math with 20B and 200B parameter model and you will see that you need to have a much much faster inter-node connectivity to efficiently scale.</p>
</section>
<section id="large-model-training" class="level3">
<h3 class="anchored" data-anchor-id="large-model-training">Large model training</h3>
<p>Of course, when we train large models we don’t use DDP, because we simply can’t fit the whole model on a single gpu so various other techniques are used. The details are discussed in a dedicated chapter on <a href="../training/model-parallelism">Model Parallelism</a>, but the only important thing to understand immediately is that all scalability techniques incur a much larger comms overhead, because they all need to communicate a lot more than just gradients. and therefore the amount of traffic on the network can easily grow 3x and more as compared to the DDP protocol overhead we have been exploring so far.</p>
<p>It can be difficult to do even approximate math as we did in this chapter, because the actual compute time depends on the efficiency of the chosen framework, how well it was tuned, how fast the DataLoader can feed the batches and many other things, therefore there is no standard MFU that one can use in the math and you will discover your MFU when you configure and run the first few steps of the large model training. and then you will read the <a href="../training/performance">Performance chapters</a> and improve your MFU even more.</p>
<p>As I have shown in these sections it should be possible to be able to do a back-of-envelope calculations once you understand the specific scalability technique and its networking costs, so that you could know ahead of time which Inter-node network speed you need to require from your acquisition manager. Of course, you also need to understand the particular model architecture and calculate how many TFLOP it will take to do a single iteration.</p>
</section>
<section id="unidirectional-vs-bidirectional-duplex" class="level3">
<h3 class="anchored" data-anchor-id="unidirectional-vs-bidirectional-duplex">Unidirectional vs Bidirectional (Duplex)</h3>
<p>Most benchmarking / bandwidth measurement tools will report a unidirectional bandwidth. So be careful when you look at unidirectional vs.&nbsp;bidirectional (duplex) speeds. Typically the latter is ~2x faster.</p>
<p>If you measure the bandwidth on your setup and it’s about 40% of the advertised speed, carefully check if the advertised speed said duplex and if so half that and then your measured bandwidth should now be about 80% which is expected.</p>
<p>case study: for a while I couldn’t understand why when I run the nccl-tests all_reduce benchmark on an A100 node with advertised 600GBps intra-node speed I was getting only 235Gbps (40%) until Horace He kindly pointed out that I should be looking at unidirectional speed which is 300GBps, and then I get 80% of the theoretical spec which checks out.</p>
</section>
</section>
<section id="intra-node-networking" class="level2">
<h2 class="anchored" data-anchor-id="intra-node-networking">Intra-node networking</h2>
<p>There are multiple platforms/solutions out there that provide intra-node networking:</p>
<ol type="1">
<li>Generic: <a href="#pcie">PCIe</a></li>
<li>NVIDIA: <a href="#nvlink">NVLink</a> and <a href="#nvswitch">NVSwitch</a></li>
<li>AMD: <a href="#infinity-fabric--xgmi">Infinity Fabric</a></li>
<li>Intel: <a href="#gaudi2">Gaudi2</a></li>
</ol>
<p>footnote: In the following sections pay close attention that 1 GBps = 8 Gbps.</p>
<p>footnote: also pay close attention to when the spec says unidirectional vs bidirectional (duplex) speeds - if you read an online spec and it doesn’t explicitly declare the directionality - look for an answer. I had to research many docs to figure it out in some of the tables below as some vendors conveniently omit this crucial information. I even had to edit a few wiki pages to add the missing information. Remember that for the vendors the bigger the better so almost always they will use the duplex number, which is 2x larger than unidirectional one.</p>
<section id="pcie" class="level3">
<h3 class="anchored" data-anchor-id="pcie">PCIe</h3>
<p><a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a> is a high-speed serial computer expansion bus standard that can be found even on the cheapest computer desktop.</p>
<table class="table">
<colgroup>
<col style="width: 21%">
<col style="width: 26%">
<col style="width: 11%">
<col style="width: 23%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Interconnect</th>
<th style="text-align: right;">Lane/Direction</th>
<th style="text-align: right;">Lanes</th>
<th style="text-align: right;">Unidirection</th>
<th style="text-align: right;">Duplex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">PCIe 4</td>
<td style="text-align: right;">~2.0 GBps</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">31 GBps</td>
<td style="text-align: right;">62 GBps</td>
</tr>
<tr class="even">
<td style="text-align: left;">PCIe 5</td>
<td style="text-align: right;">~4.0 GBps</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">63 GBps</td>
<td style="text-align: right;">126 GBps</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PCIe 6</td>
<td style="text-align: right;">~7.5 GBps</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">121 GBps</td>
<td style="text-align: right;">241 GBps</td>
</tr>
<tr class="even">
<td style="text-align: left;">PCIe 7</td>
<td style="text-align: right;">~15.0 GBps</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">242 GBps</td>
<td style="text-align: right;">484 GBps</td>
</tr>
</tbody>
</table>
<p>If one compares the latest generations of different intra-node technologies (see the following sections) PCIe is usually an order of magnitude behind.</p>
</section>
<section id="nvlink" class="level3">
<h3 class="anchored" data-anchor-id="nvlink">NVLink</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/NVLink">NVLink</a> is a wire-based serial multi-lane near-range communications link developed by Nvidia. Here is the <a href="https://blogs.nvidia.com/blog/2023/03/06/what-is-nvidia-nvlink/">What Is NVLink</a> blog post with more background on it.</li>
</ul>
<p>I found the wiki pages quite difficult to follow, so I will try to help bring clarity into this.</p>
<p>Effective payload rate of Intra-node GPU-to-GPU communication hardware:</p>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Interconnect</th>
<th style="text-align: right;">Lane/Direction</th>
<th style="text-align: right;">Lanes</th>
<th style="text-align: right;">Links</th>
<th style="text-align: right;">Unidirection</th>
<th style="text-align: right;">Duplex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">NVlink 2</td>
<td style="text-align: right;">6.250 GBps</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">150 GBps</td>
<td style="text-align: right;">300 GBps</td>
</tr>
<tr class="even">
<td style="text-align: left;">NVlink 3</td>
<td style="text-align: right;">6.250 GBps</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">300 GBps</td>
<td style="text-align: right;">600 GBps</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NVlink 4</td>
<td style="text-align: right;">6.250 GBps</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">450 GBps</td>
<td style="text-align: right;">900 GBps</td>
</tr>
</tbody>
</table>
<p>NVlink 2, 3 and 4 use the same hardware of 4 lanes of 6.250 GBps each per link. Each has a unidirectional bandwidth of 25GB/s per link, and therefore 50GB/s per duplex link. The only difference is in the number of links:</p>
<ul>
<li>NVLink 2 has 6 links =&gt; <code>25* 6</code>=&gt; 150 GBps unidirectional and 300 GBps bi-directional</li>
<li>NVLink 3 has 12 links =&gt; <code>25*12</code>=&gt; 300 GBps unidirectional and 600 GBps bi-directional</li>
<li>NVLink 4 has 18 links =&gt; <code>25*18</code>=&gt; 450 GBps unidirectional and 900 GBps bi-directional</li>
</ul>
<p>The largest PCIe 16x slot has 16 lanes. Smaller slots have less lanes, 1x == 1 lane.</p>
<p>As of this writing NVIDIA Hopper nodes typically come equipped with PCIe 5 and NVLink 4. So there NVlink is 7x faster than PCIe.</p>
<p>Let’s look at several examples of nodes and correlate the theory with reality.</p>
<p>If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span> topo <span class="at">-m</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and it will tell you how the GPUs are inter-connected.</p>
<p>On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>        <span class="ex">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU0</span>     X      NV2     0-23            N/A</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU1</span>    NV2      X      0-23            N/A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>on a different machine w/o NVLink you may see:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>        <span class="ex">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU0</span>     X      PHB     0-11            N/A</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU1</span>    PHB      X      0-11            N/A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The report includes this legend:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>  <span class="ex">X</span>    = Self</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="ex">SYS</span>  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes <span class="er">(</span><span class="ex">e.g.,</span> QPI/UPI<span class="kw">)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">NODE</span> = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PHB</span>  = Connection traversing PCIe as well as a PCIe Host Bridge <span class="er">(</span><span class="ex">typically</span> the CPU<span class="kw">)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PXB</span>  = Connection traversing multiple PCIe bridges <span class="er">(</span><span class="ex">without</span> traversing the PCIe Host Bridge<span class="kw">)</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PIX</span>  = Connection traversing at most a single PCIe bridge</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">NV#</span>  = Connection traversing a bonded set of <span class="co"># NVLinks</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So the first report <code>NV2</code> tells us the GPUs are interconnected with 2 NVLinks, and the second report <code>PHB</code> we have a typical consumer-level PCIe+Bridge setup.</p>
<p>Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g.&nbsp;NVLink), others slower (e.g.&nbsp;PHB).</p>
<p>Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training.</p>
<p>Now, let’s look at the topology of the A100 and H100 nodes:</p>
<ul>
<li>A100 topology:</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nvidia-smi topo <span class="at">-m</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>      <span class="ex">GPU0</span>  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU0</span>   X    NV12  NV12  NV12  NV12  NV12  NV12  NV12   0-23         0</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU1</span>  NV12   X    NV12  NV12  NV12  NV12  NV12  NV12   0-23         0</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU2</span>  NV12  NV12   X    NV12  NV12  NV12  NV12  NV12   0-23         0</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU3</span>  NV12  NV12  NV12   X    NV12  NV12  NV12  NV12   0-23         0</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU4</span>  NV12  NV12  NV12  NV12   X    NV12  NV12  NV12  24-47         1</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU5</span>  NV12  NV12  NV12  NV12  NV12   X    NV12  NV12  24-47         1</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU6</span>  NV12  NV12  NV12  NV12  NV12  NV12   X    NV12  24-47         1</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU7</span>  NV12  NV12  NV12  NV12  NV12  NV12  NV12   X    24-47         1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can see there are 12 NVLinks and 2 NUMA Groups (2 CPUs w/ 24 cores each)</p>
<ul>
<li>H100 topology:</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nvidia-smi topo <span class="at">-m</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>      <span class="ex">GPU0</span>  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7  CPU Affinity  NUMA Affinity</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU0</span>   X    NV18  NV18  NV18  NV18  NV18  NV18  NV18   0-51         0</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU1</span>  NV18   X    NV18  NV18  NV18  NV18  NV18  NV18   0-51         0</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU2</span>  NV18  NV18   X    NV18  NV18  NV18  NV18  NV18   0-51         0</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU3</span>  NV18  NV18  NV18   X    NV18  NV18  NV18  NV18   0-51         0</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU4</span>  NV18  NV18  NV18  NV18   X    NV18  NV18  NV18  52-103        1</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU5</span>  NV18  NV18  NV18  NV18  NV18   X    NV18  NV18  52-103        1</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU6</span>  NV18  NV18  NV18  NV18  NV18  NV18   X    NV18  52-103        1</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU7</span>  NV18  NV18  NV18  NV18  NV18  NV18  NV18   X    52-103        1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can see there are 18 NVLinks and 2 NUMA Groups (2 CPUs w/ 52 cores each)</p>
<p>Of course, other A100 and H100s node reports may vary, e.g.&nbsp;different number of cpu cores.</p>
</section>
<section id="nvswitch" class="level3">
<h3 class="anchored" data-anchor-id="nvswitch">NVSwitch</h3>
<p><a href="https://www.nvidia.com/en-us/data-center/nvlink/">NVSwitch</a> can connect more than 8 GPUs at the speed of <a href="#nvlink">NVLink</a>. It’s advertised to connect up to 256 GPUs in the future generations of the switch.</p>
<p>The benefit of connecting more than 8 GPUs at the speed of NVLink is that it allows all-to-all GPU communications at a much faster speed than any intra-node hardware can provide. And with ever increasing compute speeds the network is the likely bottleneck leading to underutilized super-expensive GPUs.</p>
<p>For example, in the universe of Tensor Parallelism (Megatron), one doesn’t use TP degree of more than 8, because TP is only efficient at NVLink speed. ZeRO-DP (Depspeed/FSDP) would also run much faster if the whole cluster uses NVLink speed and involves no slow inter-node connections.</p>
<p>The <a href="https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/">NVIDIA DGX H100</a> has a 3.6 TBps of full-duplex NVLink Network bandwidth provided by 72 NVLinks (NVLink 4). The normal NVlink 4 has 18 NVLinks (0.9 TBps duplex). So this setup has 4 switches (<code>18*4=72</code>) and therefore <code>0.9*4=3.6</code> TBps. Note, that this server has 8 GPUs, so here we get a much faster intra-node communications as compared to the standard NVlink 4.0 which provides only 0.9 TBps all-to-all connectivity for 8 GPUs.</p>
<p>NVIDIA DGX A100 has 6 switches of 12 NVlinks for a total of 72.</p>
<p><a href="https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/">DGX H100 SuperPOD</a> combines 32 DGX H100 servers, for a total of 256 GPUs. It looks like here they use only half the NVLinks they used for a single DGX H100, so only 1.8 GBps per node, for a total of 57.6 GBps in total.</p>
</section>
<section id="infinity-fabric-xgmi" class="level3">
<h3 class="anchored" data-anchor-id="infinity-fabric-xgmi">Infinity Fabric / xGMI</h3>
<p>AMD MI* Accelerators Intra-node communication is performed by AMD Infinity Fabric, which is also known as xGMI (Socket to Socket Global Memory Interface).</p>
<p>This is AMD’s answer to <a href="#nvlink">NVLink</a>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Interconnect</th>
<th style="text-align: right;">Link/Direction</th>
<th style="text-align: right;">Links</th>
<th style="text-align: right;">Unidirection</th>
<th style="text-align: right;">Duplex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MI250x</td>
<td style="text-align: right;">50 GBps</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">350 GBps</td>
<td style="text-align: right;">700 GBps</td>
</tr>
<tr class="even">
<td style="text-align: left;">MI300x</td>
<td style="text-align: right;">64 GBps</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">448 GBps</td>
<td style="text-align: right;">896 GBps</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/amd-infinity-arch-MI300X.png" class="img-fluid figure-img"></p>
<figcaption>AMD Infinity Platform Architecture</figcaption>
</figure>
</div>
<p>Platform specs: - <a href="https://www.amd.com/en/products/accelerators/instinct/mi200/mi250x.html">MI250X</a> - <a href="https://www.amd.com/en/products/accelerators/instinct/mi300/platform.html">MI300x</a></p>
</section>
<section id="gaudi2" class="level3">
<h3 class="anchored" data-anchor-id="gaudi2">Gaudi2</h3>
<p>According to <a href="https://habana.ai/wp-content/uploads/2023/10/HLS-Gaudi2_Datasheet_10_23.pdf">Gaudi2 spec</a>, these servers provide 8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps and each card connected with each of the other 7 cards at 262.5 GBps.</p>
</section>
</section>
<section id="numa-affinity" class="level2">
<h2 class="anchored" data-anchor-id="numa-affinity">NUMA Affinity</h2>
<p><a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Non-uniform memory access (NUMA)</a> is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. As modern servers have more than one CPU to get the best performance GPUs residing in the same block as the corresponding CPU should have the processes bound to that NUMA node.</p>
<p>Here is a typical A100 8x GPUs server, as visualized by <a href="https://github.com/open-mpi/hwloc">hwloc</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/a100-server-hwloc.png" class="img-fluid figure-img"></p>
<figcaption>a100 server numa nodes</figcaption>
</figure>
</div>
<p>As you can see it has 2 CPUs, each defining a NUMA block, and each such block contains a group of 4 GPUs. The GPUs are the grey blocks that say <code>CoProc</code> with 108 compute units (SMs) and 79GB of memory.</p>
<p>footnote: was generated by <code>lstopo a100.png</code></p>
<section id="software-tools" class="level4">
<h4 class="anchored" data-anchor-id="software-tools">Software Tools</h4>
<p>note-to-self: probably belongs in its own chapter?</p>
<section id="hwloc" class="level5">
<h5 class="anchored" data-anchor-id="hwloc">hwloc</h5>
<p>https://github.com/open-mpi/hwloc</p>
<p>The Hardware Locality (hwloc) software project aims at easing the process of discovering hardware resources in parallel architectures. It offers command-line tools and a C API for consulting these resources, their locality, attributes, and interconnection. hwloc primarily aims at helping high-performance computing (HPC) applications, but is also applicable to any project seeking to exploit code and/or data locality on modern computing platforms.</p>
<p>Diagnostics: to take a snapshot of the server NUMA topology and save it as an image (supports many other formats)</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">lstopo</span> a100.png</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>NUMA node binding: <code>hwloc-bind</code> - binding processes, threads and memory</p>
<p>Bind an existing process to a specific NUMA node:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">hwloc-bind</span> <span class="at">--pid</span> 1234 numa:0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Similar software: <code>numactl</code>/<code>libnuma</code></p>
<p>Some useful suggestions in <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-non-uniform-memory-access-numa-controls">pytorch docs</a></p>
</section>
</section>
</section>
<section id="inter-node-networking" class="level2">
<h2 class="anchored" data-anchor-id="inter-node-networking">Inter-node networking</h2>
<p>As inter-node hardware is about of an order of magnitude slower than intra-node hardware in this universe Gbps are used instead of GBps. (1 GBps = 8 Gbps)</p>
<p>When it comes to inter-node networking hardware, there are the well established InfiniBand from NVIDIA and a few other players and there are many new comers that mainly are coming from compute cloud providers who can’t compete on the slim margin renting out someone else’s hardware so they build their own (EFA, and others not yet disclosed).</p>
<section id="efa" class="level3">
<h3 class="anchored" data-anchor-id="efa">EFA</h3>
<p><a href="https://aws.amazon.com/hpc/efa/">Elastic Fabric Adapter (EFA)</a> is a recent technology created by AWS.</p>
<ul>
<li>EFA v1 0.4 Tbps (effective 340 Gbps for all_reduce tests) (P4 AWS instances)</li>
<li>EFA v2 3.2 Tbps (since Q3-2023, P5 AWS instances)</li>
</ul>
</section>
<section id="infiniband" class="level3">
<h3 class="anchored" data-anchor-id="infiniband">InfiniBand</h3>
<p>Now <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> (IB) has been around for a few decades so there are many available configurations that can be found out there. So that if someone says they have InfiniBand that is insufficient information. What you need to know is the signaling rate and the number of IB links.</p>
<p>Here are the most recent signaling rates which you are likely to see in the current hardware offerings:</p>
<p>Signaling rate of uni-directional links in Gbps: | Links | EDR | HDR | NDR | XDR | GDR | | —-: | –: | –: | –: | –: | –: | | 1 | 25 | 50 | 100 | 200 | 400 | | 4 | 100 | 200 | 400 | 800 | 1600 | | 8 | 200 | 400 | 800 | 1600 | 3200 | | 12 | 300 | 600 | 1200 | 2400 | 4800 |</p>
<p>Latency in usecs: | EDR | HDR | NDR | XDR | GDR | | –: | –: | –: | –: | –: | | 0.5 | 0.6 | ?? | ?? | ?? |</p>
<p><code>??</code> = NDR and later didn’t publish latency data</p>
<p>InfiniBand provides <a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>.</p>
<p>Here are some examples of NVIDIA devices with the fastest IB:</p>
<ul>
<li>One configuration of NVIDIA DGX H100 comes with 8x NVIDIA ConnectX-7 Ethernet/InfiniBand ports each of 200Gbps, for a total of 1.6 Gbps to connect with other DGX servers.</li>
<li>For DGX H100 SuperPOD the ConnectX-7s across all 32 DGX servers and associated InfiniBand switches provide 25.6 TBps of full duplex bandwidth for use within the pod or for scaling out the multiple SuperPODs - that is an equivalent of 0.8 TBps per node (6.4Tbps!).</li>
</ul>
</section>
<section id="gaudi2-1" class="level3">
<h3 class="anchored" data-anchor-id="gaudi2-1">Gaudi2</h3>
<p>According to <a href="https://habana.ai/wp-content/uploads/2023/10/HLS-Gaudi2_Datasheet_10_23.pdf">Gaudi2 spec</a>, these servers provide 24 NICs of 100GbE RoCE v2 ROMA for a total of 2.4Tbps of inter-node connectivity with other Gaudi2 servers.</p>
</section>
<section id="hpe-slingshot-interconnect" class="level3">
<h3 class="anchored" data-anchor-id="hpe-slingshot-interconnect">HPE Slingshot interconnect</h3>
<p><a href="https://www.hpe.com/ca/en/compute/hpc/slingshot-interconnect.html">HPE Slingshot interconnect</a> seems to be used by HPCs. As of this writing it provides 200Gbps per link. Some HPCs use 4 of those links to build 800Gbps interconnects, and, of course, with more links will deliver a higher overall bandwidth.</p>
</section>
<section id="omnipath" class="level3">
<h3 class="anchored" data-anchor-id="omnipath">OmniPath</h3>
<p><a href="https://en.wikipedia.org/wiki/Omni-Path">OmniPath Architecture</a> (OPA). Originally by Intel, the technology got sold to Cornelis Networks.</p>
<p>case study: I used this technology at JeanZay HPC in France in 2022. It was only 135Gbps and while the vendor tried to fix it a year later it was still the same speed. Hopefully the issue has been resolved and the speed is much faster nowadays. Because it was so slow we had to use <a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">Megatron-Deepspeed</a> for training BLOOM-176B instead of the much easier to use DeepSpeed ZeRO).</p>
<p>As of this writing I see that the product comes with either 100 or 200Gbps bandwidth. So it’s unlikely you will see anybody offering this solution for ML workloads, unless they manage to install many NICs perhaps?</p>
<p>Omni-Path provides <a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>.</p>
</section>
</section>
<section id="important-nuances" class="level2">
<h2 class="anchored" data-anchor-id="important-nuances">Important nuances</h2>
<section id="real-network-throughput" class="level3">
<h3 class="anchored" data-anchor-id="real-network-throughput">Real network throughput</h3>
<p>The network throughput in the advertised spec and the actual throughput will never be the same. In the best case you can expect about 80-90% of the advertised spec.</p>
<p>Then the network throughput will depend on the size of payload being sent during each communication. The higher the payload the higher the throughput will be.</p>
<p>Let’s demonstrate this using <a href="https://github.com/NVIDIA/nccl-tests">nccl-tests</a> on a single A100 node</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ./build/all_reduce_perf <span class="at">-b</span> 32k <span class="at">-e</span> 16G <span class="at">-f</span> 2 <span class="at">-g</span> 8 <span class="at">-n</span> 50</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>           <span class="fu">size</span>    time   algbw   busbw</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>            <span class="kw">(</span><span class="ex">B</span><span class="kw">)</span>    <span class="kw">(</span><span class="ex">us</span><span class="kw">)</span>  <span class="kw">(</span><span class="ex">GB/s</span><span class="kw">)</span>  <span class="kw">(</span><span class="ex">GB/s</span><span class="kw">)</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>         <span class="ex">32_768</span>   43.83    0.75    1.31</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>         <span class="ex">65_536</span>   46.80    1.40    2.45</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="ex">131_072</span>   51.76    2.53    4.43</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="ex">262_144</span>   61.38    4.27    7.47</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="ex">524_288</span>   80.40    6.52   11.41</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>       <span class="ex">1048_576</span>   101.9   10.29   18.00</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>       <span class="ex">2097_152</span>   101.4   20.68   36.18</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>      <span class="ex">4_194_304</span>   101.5   41.33   72.33</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>      <span class="ex">8_388_608</span>   133.5   62.82  109.93</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>     <span class="ex">16_777_216</span>   276.6   60.66  106.16</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>     <span class="ex">33_554_432</span>   424.0   79.14  138.49</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>     <span class="ex">67_108_864</span>   684.6   98.02  171.54</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">134_217_728</span>  1327.6  101.10  176.92</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">268_435_456</span>  2420.6  110.90  194.07</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="ex">536_870_912</span>  4218.4  127.27  222.72</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>  <span class="ex">1_073_741_824</span>  8203.9  130.88  229.04</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>  <span class="ex">2_147_483_648</span>   16240  132.23  231.41</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">4_294_967_296</span>   32136  133.65  233.88</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>  <span class="ex">8_589_934_592</span>   64074  134.06  234.61</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a> <span class="ex">17_179_869_184</span>  127997  134.22  234.89</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>footnote: I massaged the output to remove unwanted columns and made the size more human readable</p>
<p>This benchmark run an <code>all_reduce</code> collective for various payload sizes from 32KB to 16GB. The value that we care about is the <code>busbw</code> - this column tells us the real network throughput as explained <a href="https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#bus-bandwidth">here</a>.</p>
<p>As you can see for payloads smaller than 8MB the throughput is very low - and it starts saturating around payload size of 536MB. It’s mostly because of latency. Reducing a single 4GB payload is much faster than 1000x 4MB payloads.</p>
<p>Here is a benchmark that demonstrates that: <a href="benchmarks/all_reduce_latency_comp.py">all_reduce_latency_comp.py</a>. Let’s run it on the same A100 node:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> python <span class="at">-u</span> <span class="at">-m</span> torch.distributed.run <span class="at">--nproc_per_node</span><span class="op">=</span>8 all_reduce_latency_comp.py</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-----------</span> 1x 4.0GB <span class="at">----------------</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a> <span class="ex">busbw:</span> 1257.165 Gbps</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="ex">-----------</span> 1000x 0.004GB <span class="at">----------------</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a> <span class="ex">busbw:</span> 374.391 Gbps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It’s easy to see that it’s about 3x slower in this particular case to send the same payload but in 1000 smaller chunks.</p>
<p>So when you calculate how long does it take to <code>all_reduce</code> a given payload size, you need to use the corresponding <code>busbw</code> entry (after of course you have run this benchmark on your particular hardware/environment).</p>
<p>Figuring out the payload can be tricky since it’d depend on the implementation of the framework. Some implementations will reduce each weight’s gradient alone which obvious would lead to a very small payload and the network will be very slow. Other implementations bucket multiple gradients together before reducing those, increasing the payload and minimizing the latency impact.</p>
<p>But let’s go back to the benchmark results table. This test was done on an A100 node that runs NVLink advertised as uni-directional 300GBs so we get about 78% of the theoretical speed with 17GB payload and more than that the benchmark crashes. It can be seen from the last few rows of the table that not much more can be squeezed.</p>
<p>We can also run <a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest">p2pBandwidthLatencyTest</a> which performs a low-level p2p benchmark:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./p2pBandwidthLatencyTest</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Unidirectional</span> P2P=Enabled Bandwidth <span class="er">(</span><span class="ex">P2P</span> Writes<span class="kw">)</span> <span class="ex">Matrix</span> <span class="er">(</span><span class="ex">GB/s</span><span class="kw">)</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>   <span class="ex">D\D</span>     0      1      2      3      4      5      6      7</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>     <span class="ex">0</span> 1581.48 274.55 275.92 272.02 275.35 275.28 273.62 273.20</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>     <span class="ex">1</span> 274.70 1581.48 275.33 272.83 275.38 273.70 273.45 273.70</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>     <span class="ex">2</span> 274.81 276.90 1594.39 272.66 275.39 275.79 273.97 273.94</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>     <span class="ex">3</span> 273.25 274.87 272.12 1545.50 274.38 274.37 274.22 274.38</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>     <span class="ex">4</span> 274.24 275.15 273.44 271.57 1584.69 275.76 275.04 273.49</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>     <span class="ex">5</span> 274.37 275.77 273.53 270.84 274.59 1583.08 276.04 273.74</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>     <span class="ex">6</span> 275.61 274.86 275.47 273.19 272.58 275.69 1586.29 274.76</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>     <span class="ex">7</span> 275.26 275.46 275.49 273.61 275.50 273.28 272.24 1591.14</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As you can see in the Unidirectional section of the report we do get 274 GBps out of the advertised 300GBps (~91%).</p>
<p>Please note that when I re-run this same test on H100s (NVLink 4.0) I got a much worse efficiency:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Unidirectional</span> P2P=Enabled Bandwidth <span class="er">(</span><span class="ex">P2P</span> Writes<span class="kw">)</span> <span class="ex">Matrix</span> <span class="er">(</span><span class="ex">GB/s</span><span class="kw">)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>   <span class="ex">D\D</span>     0      1      2      3      4      5      6      7</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>     <span class="ex">0</span> 2494.51 364.13 375.99 378.03 376.77 376.71 374.85 375.66</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>     <span class="ex">1</span> 375.18 2533.95 376.08 374.98 376.21 375.96 375.76 375.12</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>     <span class="ex">2</span> 363.43 393.28 2532.67 376.35 377.14 376.47 375.76 375.48</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>     <span class="ex">3</span> 369.90 375.92 393.63 2525.38 376.58 375.88 376.13 377.01</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>     <span class="ex">4</span> 376.20 376.28 375.20 393.52 2526.02 375.82 375.05 376.10</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>     <span class="ex">5</span> 376.26 376.60 375.54 375.52 376.81 2521.18 376.37 376.60</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>     <span class="ex">6</span> 374.31 376.19 376.80 376.32 376.83 376.44 2529.85 376.39</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>     <span class="ex">7</span> 376.17 376.49 376.53 374.95 376.30 376.82 375.71 2519.78</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So 376GBps out of 450GBps is 83% (not very good).</p>
<p>Bottom line - in this particular setup: 1. if you have huge payloads you will be able to use about 80% of the advertised 300GBps 2. if the payload of each communication is smallish it could be far far lower.</p>
<p>This graph is also helpful to demonstrate how the actual bandwidth changes with the size of the message:</p>
<p><img src="images/ccgrid11-uni-direction-bandwidth.png" class="img-fluid" alt="Low-level Uni-directional Bandwidth Measurements"> (<a href="https://ieeexplore.ieee.org/document/5238655">source</a>)</p>
<p>Another tool for bandwidth measurements on NVIDIA GPUs is <a href="https://github.com/NVIDIA/nvbandwidth">NVIDIA/nvbandwidth</a>.</p>
</section>
<section id="latency" class="level3">
<h3 class="anchored" data-anchor-id="latency">Latency</h3>
<p><img src="images/ccgrid11-low-level-latency.png" class="img-fluid" alt="Low-level Latency Measurements"> (<a href="https://ieeexplore.ieee.org/document/5238655">source</a>)</p>
<p>XXX: integrate/expand</p>
</section>
<section id="proprietary-network-hardware-and-nccl" class="level3">
<h3 class="anchored" data-anchor-id="proprietary-network-hardware-and-nccl">Proprietary network hardware and NCCL</h3>
<p>Proprietary network hardware vendors like AWS (EFA) don’t disclose their secrets and therefore the public libraries like <a href="https://github.com/NVIDIA/nccl">nccl</a> cannot support those out of the box. These vendors have to supply their own versions of the network collective libraries to be used by users of their hardware.</p>
<p>Originally proprietary hardware vendors used the trick of telling the users to use <code>LD_LIBRARY_PATH</code> and/or <code>LD_PRELOAD</code> to dynamically overload <code>libnccl.so</code> to get their custom version loaded into PyTorch or another framework. But recently NCCL developed a <a href="https://github.com/NVIDIA/nccl/tree/master/ext-net">NCCL Net Plugin</a> which should be used now instead. This feature was added in NCCL v2.12.</p>
<p>Now, when NCCL is initialized, it will look for a <code>libnccl-net.so</code> library and dynamically load it, then look for symbols inside the library. That’s where proprietary hardware vendors should now put their custom APIs. This library, of course, should still be either in <code>LD_LIBRARY_PATH</code> or the <code>/etc/ld.so.conf</code> config.</p>
<p>For more information about dynamic library loading see <a href="https://github.com/stas00/the-art-of-debugging/tree/master/compiled-programs#shared-libraries-ldsoconf-nm-unresolved-symbols-ldd-ld_library_path-ld_preload">this section</a>.</p>
</section>
<section id="node-proximity" class="level3">
<h3 class="anchored" data-anchor-id="node-proximity">Node Proximity</h3>
<p>If you get 2 random nodes from the cloud they may not reside on the same subnet and there will be an additional latency incurred for all transmissions.</p>
<p>You want to make sure that the nodes used for a single training all reside on the same subnet/spine so they are all one hop away from each other.</p>
<p>When you plan to eventually have a large cluster but starting small make sure that your provider can expand the cluster while keeping all the nodes close to each other.</p>
<p>Here are the cloud-specific ways of accomplishing node proximity:</p>
<ul>
<li>Azure: <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/availability-set-overview?source=recommendations">availability set</a></li>
<li>GCP: <a href="https://cloud.google.com/compute/docs/instances/use-compact-placement-policies">compact placement policies</a></li>
</ul>
<p>Depending on the type of package you have or what type of machines you rent - you may or may not be able to use those.</p>
</section>
<section id="shared-internode-network" class="level3">
<h3 class="anchored" data-anchor-id="shared-internode-network">Shared internode network</h3>
<p>If you use a shared HPC environment, or even if you have your own cluster but sharing it with your colleagues expect the network bandwidth to be unreliable and fluctuate at different time of the day.</p>
<p>This situation unfortunately makes it extremely difficult to finetune the performance of your training setup. Since every time you run a test the TFLOPs will vary, so how do you do the optimization? Unfortunately I don’t have a magic trick here. If you have a working solution please kindly share.</p>
<p>case study: we had this issue at JeanZay HPC when we were doing preliminary experiments before we started training BLOOM-176B. As that HPC has many users it was pretty much impossible to do speed optimizations, as even running the exact same setup again and again gave different throughput results. Luckily just before we launched BLOOM-176B training we were given an exclusive access to the new at that time A100 partition so we were the only users and we were able to greatly optimize the throughput.</p>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bekman2024,
  author = {Bekman, Stas and Foreman, Sam},
  title = {ML {Engineering}},
  date = {2024-02-13},
  url = {https://saforem2.github.io/ml-engineering},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bekman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Bekman, Stas, and Sam Foreman. 2024. <span>“ML Engineering.”</span>
February 13, 2024. <a href="https://saforem2.github.io/ml-engineering">https://saforem2.github.io/ml-engineering</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../qmd/insights/ai-battlefield.html" class="pagination-link  aria-label=" 🪖="" the="" ai="" battlefield"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">🪖 The AI Battlefield</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../qmd/network/benchmarks/index.html" class="pagination-link" aria-label="Networking Benchmarks">
        <span class="nav-page-text">Networking Benchmarks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a href="https://saforem2.github.io/ml-engineering">ML-Engineering</a></p>
</div>   
    <div class="nav-footer-center">
<p>2024</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/qmd/network/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/qmd/network/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p><a href="https://github.com/saforem2/ml-engineering"><i class="fa-brands fa-github" aria-label="github"></i></a></p>
</div>
  </div>
</footer>




</body></html>