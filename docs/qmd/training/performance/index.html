<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-02-20">

<title>ML Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../qmd/training/re-train-hub-models.html" rel="next">
<link href="../../../qmd/training/reproducibility/index.html" rel="prev">
<link href="../../.././favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/iconify-1.0.8/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
</script>
<link href="https://pvinis.github.io/iosevka-webfont/3.4.1/iosevka.css" rel="stylesheet">


<link rel="stylesheet" href="../../../css/default.css">
<link rel="stylesheet" href="../../../css/callouts.css">
<meta property="og:title" content="Sam Foreman">
<meta property="og:description" content="Machine Learning Engineering Open Book">
<meta property="og:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta property="og:site_name" content="ML Engineering">
<meta name="twitter:title" content="Sam Foreman">
<meta name="twitter:description" content="Machine Learning Engineering Open Book">
<meta name="twitter:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="ML Engineering">
<meta name="citation_author" content="Stas Bekman">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-02-20">
<meta name="citation_cover_date" content="2024-02-20">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-02-20">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/ml-engineering">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">ML Engineering</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/saforem2"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:twitter"></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/ml-engineering"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-gear"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/blob/main/index.qmd">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/issues/new/choose">
            New Issue
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../qmd/training/index.html">🏋️ Training</a></li><li class="breadcrumb-item"><a href="../../../qmd/training/performance/index.html">Software Tune Up For The Best Performance</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📓 Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/testing/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">✏️ Testing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/transformers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🤗 Transformers</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/insights/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🧠  Insights</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/insights/ai-battlefield.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🪖 The AI Battlefield</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏋️  Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/dtype.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tensor precision / Data types</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/dtype-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tensor precision / Data types</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/emulate-multi-node.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Emulate a multi-node setup using just a single node</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/hparams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Selecting Training Hyper-Parameters And Model Initializations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/model-parallelism/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/checkpoints/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Checkpoints</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/reproducibility/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/performance/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Software Tune Up For The Best Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/re-train-hub-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Re-train HF Hub Models From Scratch Using Finetuning Examples</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/training/instabilities/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Avoiding, Recovering From and Understanding Instabilities</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/instabilities/training-loss-patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Understanding Training Loss Patterns</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/training/fault-tolerance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/training/fault-tolerance/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/network/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inter-node and Intra-Node Networking Hardware</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/network/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🛜 Network</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/network/benchmarks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networking Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/network/benchmarks/results/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Network Benchmarks Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/network/benchmarks/results/disable-nvlink.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Disabling NVLink Benchmark</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/storage/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📦  Storage</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">
 <span class="menu-text">Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false">
 <span class="menu-text">Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/storage/benchmarks/results/hope-2023-12-20-14-37-02-331702-summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">fio benchmark results for hope on 2023-12-20-14:37:02</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/compute/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">💻 Compute</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/compute/cpu/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/compute/cpu-memory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU memory</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/compute/accelerator/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accelerators</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/compute/accelerator/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accelerators</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false">
 <span class="menu-text">Nvidia</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/compute/accelerator/nvidia/debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Troubleshooting NVIDIA GPUs</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/orchestration/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎻  Orchestration</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/orchestration/slurm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Working in SLURM Environment</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/orchestration/slurm/admin.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Administration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/orchestration/slurm/launchers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Launchers with SLURM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/orchestration/slurm/performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/orchestration/slurm/users.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM for users</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../qmd/debug/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🐛  Debugging</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/tiny-scripts/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Back up of scripts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/make-tiny-models-tokenizers-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Faster debug and development with tiny models, tokenizers and datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/nccl-performance-debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NCCL: Debug and Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debugging PyTorch programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debug Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/torch-distributed-hanging-solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../qmd/debug/underflow_overflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Underflow and Overflow Detection</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#software-tune-up-for-the-best-performance" id="toc-software-tune-up-for-the-best-performance" class="nav-link active" data-scroll-target="#software-tune-up-for-the-best-performance">Software Tune Up For The Best Performance</a>
  <ul class="collapse">
  <li><a href="#macs-vs-flop-vs-flops-vs-flops" id="toc-macs-vs-flop-vs-flops-vs-flops" class="nav-link" data-scroll-target="#macs-vs-flop-vs-flops-vs-flops">MACs vs FLOP vs FLOPS vs FLOP/s</a></li>
  <li><a href="#tflops-as-a-performance-metric" id="toc-tflops-as-a-performance-metric" class="nav-link" data-scroll-target="#tflops-as-a-performance-metric">TFLOPS as a performance metric</a></li>
  <li><a href="#how-to-improve-speed-and-save-memory" id="toc-how-to-improve-speed-and-save-memory" class="nav-link" data-scroll-target="#how-to-improve-speed-and-save-memory">How To Improve Speed and Save Memory</a>
  <ul class="collapse">
  <li><a href="#anatomy-of-models-operations" id="toc-anatomy-of-models-operations" class="nav-link" data-scroll-target="#anatomy-of-models-operations">Anatomy of Model’s Operations</a></li>
  <li><a href="#anatomy-of-models-memory-usage" id="toc-anatomy-of-models-memory-usage" class="nav-link" data-scroll-target="#anatomy-of-models-memory-usage">Anatomy of Model’s Memory Usage</a></li>
  <li><a href="#additional-gpu-memory-usage" id="toc-additional-gpu-memory-usage" class="nav-link" data-scroll-target="#additional-gpu-memory-usage">Additional GPU memory usage</a></li>
  <li><a href="#batch-sizes" id="toc-batch-sizes" class="nav-link" data-scroll-target="#batch-sizes">Batch sizes</a></li>
  <li><a href="#gradient-accumulation" id="toc-gradient-accumulation" class="nav-link" data-scroll-target="#gradient-accumulation">Gradient Accumulation</a></li>
  <li><a href="#gradient-checkpointing" id="toc-gradient-checkpointing" class="nav-link" data-scroll-target="#gradient-checkpointing">Gradient checkpointing</a></li>
  <li><a href="#memory-efficient-optimizers" id="toc-memory-efficient-optimizers" class="nav-link" data-scroll-target="#memory-efficient-optimizers">Memory-efficient optimizers</a></li>
  </ul></li>
  <li><a href="#model-execution-speed" id="toc-model-execution-speed" class="nav-link" data-scroll-target="#model-execution-speed">Model execution speed</a>
  <ul class="collapse">
  <li><a href="#forward-vs-backward-execution-speed" id="toc-forward-vs-backward-execution-speed" class="nav-link" data-scroll-target="#forward-vs-backward-execution-speed"><code>forward</code> vs <code>backward</code> Execution Speed</a></li>
  </ul></li>
  <li><a href="#memory-profiler-tools" id="toc-memory-profiler-tools" class="nav-link" data-scroll-target="#memory-profiler-tools">Memory profiler tools</a></li>
  <li><a href="#vector-and-matrix-size-divisibility" id="toc-vector-and-matrix-size-divisibility" class="nav-link" data-scroll-target="#vector-and-matrix-size-divisibility">Vector and matrix size divisibility</a>
  <ul class="collapse">
  <li><a href="#tile-and-wave-quantization" id="toc-tile-and-wave-quantization" class="nav-link" data-scroll-target="#tile-and-wave-quantization">Tile and wave quantization</a></li>
  <li><a href="#number-and-size-of-attention-heads" id="toc-number-and-size-of-attention-heads" class="nav-link" data-scroll-target="#number-and-size-of-attention-heads">Number and size of attention heads</a></li>
  <li><a href="#flash-attention" id="toc-flash-attention" class="nav-link" data-scroll-target="#flash-attention">Flash attention</a></li>
  <li><a href="#final-recommendations-for-sizing" id="toc-final-recommendations-for-sizing" class="nav-link" data-scroll-target="#final-recommendations-for-sizing">Final recommendations for sizing</a></li>
  </ul></li>
  <li><a href="#contributors" id="toc-contributors" class="nav-link" data-scroll-target="#contributors">Contributors</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/qmd/training/performance/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/qmd/training/performance/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><div class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../qmd/training/index.html">🏋️ Training</a></li><li class="breadcrumb-item"><a href="../../../qmd/training/performance/index.html">Software Tune Up For The Best Performance</a></li></ol></nav><div class="quarto-title-tools-only"><h1></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="https://github.com/saforem2/ml-engineering/blob/main/qmd/training/performance/index.qmd"><i class="bi"></i></button></div></div>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading"></div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 20, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="software-tune-up-for-the-best-performance" class="level1">
<h1>Software Tune Up For The Best Performance</h1>
<p>The faster you can make your model to train the sooner the model will finish training, which is important not only to being first to publish something, but also potentially saving a lot of money.</p>
<p>In general maximizing throughput is all about running many experiments and measuring the outcome and choosing the one that is superior.</p>
<p>In certain situations your modeling team may ask you to choose some hyper parameters that will be detrimental to throughput but overall beneficial for the overall model’s success.</p>
<section id="macs-vs-flop-vs-flops-vs-flops" class="level2">
<h2 class="anchored" data-anchor-id="macs-vs-flop-vs-flops-vs-flops">MACs vs FLOP vs FLOPS vs FLOP/s</h2>
<p>This section is here to try to disambiguate the common performance metric definitions and their relationship to each other.</p>
<p><strong>MAC vs FLOP</strong>:</p>
<ul>
<li><p>1 FLOP (FLoating point OPeration) can be one of addition, subtraction, multiplication, or division operation.</p></li>
<li><p>1 MAC (Multiply-ACCumulate) operation is a multiplication followed by an addition, that is: <code>a * b + c</code></p></li>
</ul>
<p>Thus 1 MAC = 2 FLOPs. It’s also quite common for modern hardware to perform 1 MAC in a single clock cycle.</p>
<p>Please note that to calculate the number of MACs in relationship to FLOPs the reverse logic applies, that is MACs = 0.5 FLOPs - it’s somewhat confusing since we have just said that 1 MAC = 2 FLOPs, but it checks out - observe: 100 FLOPs = 50 MACs - because there are 2 FLOPs in each MAC.</p>
<p>Moreover, while 1 MAC = 2 FLOPs, the reverse isn’t necessarily true. That is 2 FLOPs isn’t necessarily equal to 1 MAC. For example, if you did <code>.5*.6</code> 100 times it’d be 100 FLOPs, which here would equal to 100 MACs, because here only the multiply part of the MAC is executed.</p>
<p><strong>FLOP vs FLOPS vs FLOP/s</strong></p>
<ul>
<li><p>1 FLOP (FLoating point OPeration) is any floating point addition, subtraction, multiplication, or division operation.</p></li>
<li><p>1 FLOPS (FLoating point OPeration per Second) is how many floating point operations were performed in 1 second - see <a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a></p></li>
</ul>
<p>Further you will find the following abbreviations: GFLOPS = Giga FLOPS, TFLOPS = Tera FLOPS, etc., since it’s much easier to quickly grasp 150TFLOPS rather than 150000000000000FLOPS.</p>
<p>There is an ambiguity when FLOPS is used in writing - sometimes people use it to indicate the total quantity of operations, at other times it refers to operations per second. The latter is the most common usage and that is the definition used in this book.</p>
<p>In scientific writing FLOP/s is often used to clearly tell the reader that it’s operations per second. Though this particular approach is hard to convert to a variable name since it still becomes <code>flops</code> when illegal characters are removed.</p>
<p>In some places you might also see FLOPs, which again could mean either, since it’s too easy to flip lower and upper case <code>s</code>.</p>
<p>If the definition is ambiguous try to search for context which should help to derive what is meant:</p>
<ul>
<li>If it’s a math equation and there is a division by time you know it’s operations per second.</li>
<li>If speed or performance is being discussed it usually refers to operations per second.</li>
<li>If it talks about the amount of compute required to do something it refers to the total amount of operations.</li>
</ul>
</section>
<section id="tflops-as-a-performance-metric" class="level2">
<h2 class="anchored" data-anchor-id="tflops-as-a-performance-metric">TFLOPS as a performance metric</h2>
<p>Before you start optimizing the performance of your training setup you need a metric that you can use to see whether the throughput is improving or not. You can measure seconds per iteration, or iterations per second, or some other such timing, but there is a more useful metric that measures TFLOPS.</p>
<p>Measuring TFLOPS is superior because without it you don’t know whether you are close to the best performance that can be achieved or not. This measurement gives you an indication of how far you’re from the peak performance reported by the hardware manufacturer.</p>
<p>In this section I will use BLOOM’s training for the exemplification. We used 80GB A100 NVIDIA GPUs and we trained in mixed bf16 regime. So let’s look at the <a href="https://www.nvidia.com/en-us/data-center/a100/">A100 spec</a> which tells us:</p>
<pre><code>BFLOAT16 Tensor Core    312 TFLOPS</code></pre>
<p>Therefore we now know that if we were to only run <code>matmul</code> on huge bf16 matrices of very specific dimensions without copying to and from the device we should get around 312 TFLOPS max.</p>
<p>Practically though, due to disk IO, communications and copying data from the GPU’s memory to its computing unit overhead and because we can’t do everything in bf16 and at times we have to do math in fp32 (or tf32) we can really expect much less than that. The realistic value will vary from accelerator to accelerator, but for A100 in 2022 getting above 50% (155 TFLOPS) was an amazing sustainable throughput for a complex 384 GPUs training setup.</p>
<p>footnote: in 2023 the invention of flash attention and other techniques have pushed the bar to more than 50%.</p>
<p>When we first started tuning things up we were at &lt;100 TFLOPS and a few weeks later when we launched the training we managed to get 150 TFLOPS.</p>
<p>The important thing to notice here is that we knew that we can’t push it further by much and we knew that there was no more point to try and optimize it even more.</p>
<p>So a general rule of thumb for when you prepare for a massive model training - ask around what’s the top TFLOPS one can expect to get with a given accelerator on a multi-node setup with the specified precision - and optimize until you get close to that. Once you did stop optimizing and start training.</p>
<p>footnote: For 80GB A100s in 2022 that was 155, in 2023 it has been pushed to about 180 TFLOPS.</p>
<p>footnote: When calculating TFLOPS it’s important to remember that the math is different if <a href="#gradient-checkpointing">Gradient checkpointing</a> are enabled, since when it’s activated more compute is used and it needs to be taken into an account. Usually the cost is of an additional forward path, but recently better methods have been found that saves some of that recomputation.</p>
<p>For decoder transformer models the following is an estimation formula which slightly under-reports the real TFLOPS:</p>
<p>TFLOPS: <code>model_size_in_B * 4 * 2 * seqlen * global_batch_size / (time_in_sec_per_interation * total_gpus * 1e3)</code></p>
<p>The factor of 4 is used with activation/gradient checkpointing, otherwise it will be 3. For 100B+ models, activation checkpointing will almost always be on.</p>
<p>So the <code>3*2</code> is often called “model FLOPs” and <code>4*2</code> - “hardware FLOPs”, correlating to MFU and HFU (model and hardware FLOPS per second divided by the accelerator’s theoretical peak FLOPS)</p>
<pre><code>perl -le '$ng=64; $ms=52; $gbs=1024; $sp=127; $seqlen=2048; print $ms*4*2*$seqlen*$gbs / ( $sp * $ng * 1e3)'</code></pre>
<p>(ng = total gpus, ms = model size in B, gbs = global batch size, sp = throughput in seconds)</p>
<p>Here is the same formula using <code>bash</code> env vars and which breaks down GBS into <code>MBS*DP*GAS</code> (GAS in this case corresponded to <code>pp_chunks</code> which was the number of chunks in the pipeline, but normally GAS just stands for Gradient Accumulation Steps):</p>
<pre><code>echo "($MSIZE*4*2*SEQLEN*$MICRO_BATCH_SIZE*$DP_SIZE*$GAS)/($THROUGHPUT*$NNODES*4*1000)" | bc -l</code></pre>
<p>The exact formula is in Equation 3 of Section 5.1 of the <a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a> paper. You can see the code <a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/251">here</a>.</p>
<p>footnote: For Inference only it’d be: <code>24Bsh^2 + 4Bs^2h</code> floating point operations per layer.</p>
</section>
<section id="how-to-improve-speed-and-save-memory" class="level2">
<h2 class="anchored" data-anchor-id="how-to-improve-speed-and-save-memory">How To Improve Speed and Save Memory</h2>
<p>The more GPU memory you have for your batch size (BS) the more efficient the GPUs will be at performing compute, and the faster you will complete your task since you will be able to go through data faster.</p>
<p>Of course, this section is crucial for when you get GPU OOM with even BS=1 and you don’t want to rent/buy more hardware.</p>
<p>Here is an overview of what features can help to either improve speed or save memory</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Speed</th>
<th style="text-align: left;">Memory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gradient accumulation</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Gradient checkpointing</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mixed precision training</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
</tr>
<tr class="even">
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Optimizer choice</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;">DataLoader</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DeepSpeed Zero</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Flash Attention</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
</tbody>
</table>
<section id="anatomy-of-models-operations" class="level3">
<h3 class="anchored" data-anchor-id="anatomy-of-models-operations">Anatomy of Model’s Operations</h3>
<p>Transformers architecture includes 3 main groups of operations grouped below by compute-intensity.</p>
<ol type="1">
<li><p><strong>Tensor Contractions</strong></p>
<p>Linear layers and components of Multi-Head Attention all do batched <strong>matrix-matrix multiplications</strong>. These operations are the most compute-intensive part of training a transformer.</p></li>
<li><p><strong>Statistical Normalizations</strong></p>
<p>Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more <strong>reduction operations</strong>, the result of which is then applied via a map.</p></li>
<li><p><strong>Element-wise Operators</strong></p>
<p>These are the remaining operators: <strong>biases, dropout, activations, and residual connections</strong>. These are the least compute-intensive operations.</p></li>
</ol>
<p>This knowledge can be helpful to know when analyzing performance bottlenecks.</p>
<p>This summary is derived from <a href="https://arxiv.org/abs/2007.00072">Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020</a></p>
</section>
<section id="anatomy-of-models-memory-usage" class="level3">
<h3 class="anchored" data-anchor-id="anatomy-of-models-memory-usage">Anatomy of Model’s Memory Usage</h3>
<p>We’ve seen that training the model uses much more memory than just putting the model on the GPU. This is because there are many components during training that use GPU memory. The components on GPU memory are the following:</p>
<ol type="1">
<li>model weights</li>
<li>optimizer states</li>
<li>gradients</li>
<li>forward activations saved for gradient computation</li>
<li>temporary buffers</li>
<li>functionality-specific memory</li>
</ol>
<p>A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory and temp memory.</p>
<p>Let’s look at the details.</p>
<p><strong>Model Weights:</strong></p>
<ul>
<li>4 bytes * number of parameters for fp32 training</li>
<li>6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16/bf16 in memory)</li>
</ul>
<p><strong>Optimizer States:</strong></p>
<ul>
<li>8 bytes * number of parameters for normal AdamW (maintains 2 states)</li>
<li>4 bytes * number of parameters for AdamW running at bf16. See <a href="https://github.com/huggingface/transformers/pull/21312">this work</a> that uses <code>AnyPrecisionAdamW</code>.</li>
<li>4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state) or LION, or Adafactor (and others) (Adafactor uses some additional memory beside 4 bytes)</li>
<li>2 bytes * number of parameters for 8-bit AdamW optimizers like <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
</ul>
<p><strong>Gradients</strong></p>
<ul>
<li>4 bytes * number of parameters for either fp32 precision and in some frameworks with mixed half-precision precision training.</li>
<li>2 bytes * number of parameters for non-mixed half-precision and in some frameworks with mixed half-precision precision training.</li>
</ul>
<p><strong>Forward Activations</strong></p>
<ul>
<li>size depends on many factors, the key ones being sequence length, hidden size and batch size.</li>
</ul>
<p>There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation.</p>
<p><strong>Temporary Memory</strong></p>
<p>Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it’s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.</p>
<p><strong>Functionality-specific memory</strong></p>
<p>Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.</p>
<p>For <strong>inference</strong>, the math is very similar to training, minus optimizer states and gradients. And for model weights there is just a single multiplier of the number of parameters:</p>
<ul>
<li>6 bytes in mixed precision (4+2)</li>
<li>4 bytes in fp32</li>
<li>2 bytes in half precision</li>
<li>1 byte in quantized int8 precision</li>
</ul>
<p>Another excellent resource that takes you through the memory needs and other requirements is <a href="https://blog.eleuther.ai/transformer-math/">Transformer Math 101</a>.</p>
<p>The <a href="https://github.com/EleutherAI/cookbook">EAI cookbook</a> contains a set of <a href="https://github.com/EleutherAI/cookbook/tree/main/calc">calculation scripts</a> that can output the theoretical memory overhead for a given training or inference calculation run based on your configuration and setup.</p>
<p>There is a very handy <a href="https://vram.asmirnov.xyz/">GPU VRAM Estimator</a> from Alexander Smirnov, and <a href="https://asmirnov.xyz/vram">the notes to how it works</a>.</p>
</section>
<section id="additional-gpu-memory-usage" class="level3">
<h3 class="anchored" data-anchor-id="additional-gpu-memory-usage">Additional GPU memory usage</h3>
<p>In addition to the memory usage described in the previous section, there are other consumers of the GPU memory - so you never get the full memory for your model’s use.</p>
<section id="preloaded-cuda-kernels-memory-usage" class="level4">
<h4 class="anchored" data-anchor-id="preloaded-cuda-kernels-memory-usage">Preloaded CUDA kernels memory usage</h4>
<p>When PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU’s total available memory.</p>
<p>The size of allocated memory for cuda kernels varies between different GPUs, and also it can be different between pytorch versions. Let’s allocate a 4-byte tensor on cuda and check how much GPU memory is used up upfront.</p>
<p>With <code>pytorch==1.10.2</code>:</p>
<pre><code>$ CUDA_MODULE_LOADING=EAGER python -c "import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \
used=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')"
pt=1.10.2: used=1.78GB, free=77.43GB, total=79.21GB</code></pre>
<p>With <code>pytorch==1.13.1</code>:</p>
<pre><code>$ CUDA_MODULE_LOADING=EAGER python -c "import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \
used=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')"
pt=1.13.1: used=0.90GB, free=78.31GB, total=79.21GB</code></pre>
<p>The older pytorch “wasted” 1.78GB of A100, the newer only 0.9GB, thus saving a whooping 0.9GB, which can be the saving grace for the OOM situations.</p>
<p><code>CUDA_MODULE_LOADING=EAGER</code> is needed in the recent pytorch version if we want to force cuda kernels pre-loading, which are otherwise lazy-loaded on demand. But do not use this setting in production since it’s likely to use more memory than needed. The whole point of lazy-loading is to load only the kernels that are needed.</p>
<p>With <code>pytorch==2.1.1</code>:</p>
<pre><code>$ CUDA_MODULE_LOADING=EAGER python -c "import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \
used=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')"
pt=2.1.1+cu121: used=0.92GB, free=78.23GB, total=79.15GB</code></pre>
<p>As compared to the lazy mode:</p>
<pre><code>$ python -c "import torch; x=torch.ones(1).cuda(); free, total = map(lambda x: x/2**30, torch.cuda.mem_get_info()); \
used=total-free; print(f'pt={torch.__version__}: {used=:0.2f}GB, {free=:0.2f}GB, {total=:0.2f}GB')"
pt=2.1.1+cu121: used=0.47GB, free=78.68GB, total=79.15GB</code></pre>
<p>There is a 450MB difference, but here we only loaded kernels to do <code>torch.ones</code> - the actual memory allocated at run time with other code using torch API will be somewhere between 0.47 and 0.92GB.</p>
</section>
<section id="memory-fragmentation" class="level4">
<h4 class="anchored" data-anchor-id="memory-fragmentation">Memory fragmentation</h4>
<p>As the model allocates and frees tensors, the memory could fragment. That is there could be enough free memory to allocate, say, 1GB of contiguous memory, but it could be available in 100s of small segments spread out through the memory and thus even though the memory is available it can’t be used unless very small allocations are made.</p>
<p>Environment variable <code>PYTORCH_CUDA_ALLOC_CONF</code> comes to help and allows you to replace the default memory allocation mechanisms with more efficient ones. For more information see <a href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management">Memory management</a>.</p>
</section>
</section>
<section id="batch-sizes" class="level3">
<h3 class="anchored" data-anchor-id="batch-sizes">Batch sizes</h3>
<p>First, there are usually two batch sizes:</p>
<ol type="1">
<li><p>micro batch size (MBS), also known as batch size per gpu - this is how many samples a single gpu consumes during a model’s single <code>forward</code> call.</p></li>
<li><p>global batch size (GBS) - this is the total amount of samples consumed between two optimizer steps across all participating GPUs.</p></li>
</ol>
<p>Model replica is how many gpus are needed to fit the full model.</p>
<ul>
<li>If the model fits into a single GPU a model replica takes 1 GPU. Usually then one can use multiple GPUs to perform <a href="../../training/model-parallelism#data-parallelism">Data Parallelism</a></li>
<li>If the model doesn’t fit into a single GPU, it’d usually require some sort of sharding technique - it can be <a href="../../training/model-parallelism#tensor-parallelism">Tensor Parallelism</a> (TP), <a href="../../training/model-parallelism#pipeline-parallelism">Pipeline Parallelism</a> (PP), or <a href="../../training/model-parallelism#zero-data-parallelism">ZeRO Data Parallelism</a> (ZeRO-DP).</li>
</ul>
<p>You can have as many data streams as there are replicas. Which is the same as the value of DP. - So in the simple case of a model fitting into a single GPU. There are as many data streams as there are GPUs. DP=N_GPUS - when the model doesn’t fit onto a single GPU, then <code>DP=N_GPUs/(TP*PP)</code> in the case of 3D parallelism and DP=ZeRO-DP in the case of ZeRO parallelism.</p>
<p>Going back to our global batch size (GBS) it’s calculated as:</p>
<pre><code>GBS = MBS*DP</code></pre>
<p>So if you have 8 gpus (N_GPUS=8) and your MBS=4 and you do DP you end up with having GBS=32 because:</p>
<pre><code>GBS = MBS*DP = 4*8 = 32</code></pre>
<p>If you use TP with a degree of 2 (TP=2) and PP with a degree of 2 (PP=2) this means each model replica takes 4 gpus (<code>TP*PP</code>), and thus with N_GPUS=8</p>
<pre><code>DP = N_GPUS/(TP*PP) = 8 / (2*2) = 2</code></pre>
<p>and now GBS becomes:</p>
<pre><code>GBS = MBS*DP = 4*2 = 8</code></pre>
<p>If your training setup requires <a href="#gradient-accumulation">Gradient Accumulation</a>, one usually defines the interval of how many steps to wait before performing a gradient accumulation. The term is usually Gradient Accumulation Steps (GAS). If GAS=4 (i.e.&nbsp;sync grads every 4 steps) and TP=1, PP=1 and DP=8:</p>
<pre><code>DP = N_GPUS/(TP*PP) = 8 / (1*1) = 8
GBS = MBS*DP*GAS = 4*8*4 = 128</code></pre>
<p>Typically you want to make the micro batch size as large as possible so that the GPU memory is close to being full, but not too full.</p>
<p>With large models usually there is not much free GPU memory left to have a large micro batch size, therefore every additional sample you can fit is important.</p>
<p>While it’s super important that sequence length and hidden size and various other hyper parameters are high multiples of 2 (64, 128 and higher) to achieve the highest performance, because in most models the batch dimension is flattened with the sequence length dimension during the compute the micro batch size alignment usually has little to no impact on performance.</p>
<p>Therefore if you tried to fit a micro batch size of 8 and it OOM’ed, but 7 fits - use the latter rather than 4. The higher the batch size the more samples you will be able to fit into a single step.</p>
<p>Of course, when using hundreds of GPUs your global batch size may become very large. In that case you might use a smaller micro batch size or use less GPUs or switch to a different form of data parallelism so that the GPUs work more efficiently.</p>
</section>
<section id="gradient-accumulation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-accumulation">Gradient Accumulation</h3>
<p>The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model’s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU’s memory. In turn, however, the added forward and backward passes can slow down the training a bit.</p>
<p>Gradient Accumulation Steps (GAS) is the definition of how many steps are done w/o updating the model weights.</p>
<p>When using Pipeline parallelism a very large Gradient Accumulation is a must to keep the <a href="../../training/model-parallelism/README.md#naive-model-parallelism-vertical">pipeline’s bubble to the minimum</a>.</p>
<p>Since the optimizer step isn’t performed as often with gradient accumulation there is an additional speed up here as well.</p>
<p>The following benchmarks demonstrate how increasing the gradient accumulation steps improves the overall throughput (20-30% speedup):</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537">RTX-3090</a></li>
<li><a href="https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957">A100</a></li>
</ul>
<p>When <a href="../../training/model-parallelism#data-parallelism">data parallelism</a> is used gradient accumulation further improves the training throughput because it reduces the number of gradient reduction calls, which is typically done via the <code>all_reduce</code> collective which costs a 2x size of gradients to be reduced. So for example, if one goes from GAS=1 to GAS=8 in <code>DistributedDataParallelism</code> (DDP) the network overhead is reduced by 8x times, which on a slow inter-node network can lead to a noticeable improvement in the training throughput.</p>
</section>
<section id="gradient-checkpointing" class="level3">
<h3 class="anchored" data-anchor-id="gradient-checkpointing">Gradient checkpointing</h3>
<p>Gradient Checkpointing is also known as Activation Recompution, Activation Checkpointing and Checkpoint Activations.</p>
<p>This methodology is only relevant for training, and not during inference.</p>
<p>Enabling gradient checkpointing allows one to trade training throughput for accelerator memory. When this feature is activated instead of remembering the outputs of, say, transformer blocks until the <code>backward</code> pass is done, these outputs are dropped. This frees up huge amounts of accelerator memory. But, of course, a <code>backward</code> pass is not possible without having the outputs of <code>forward</code> pass, and thus they have to be recalculated.</p>
<p>This, of course, can vary from model to model, but typically one pays with about 20-25% decrease in throughput, but since a huge amount of gpu memory is liberated, one can now increase the batch size per gpu and thus overall improve the effective throughput of the system. In some cases this allows you to double or quadruple the batch size if you were already able to do a small batch size w/o OOM. (Recent papers report as high as 30-40% additional overhead.)</p>
<p>Activation checkpointing and gradient checkpointing are 2 terms for the same methodology.</p>
<p>For example, in HF Transformers models you do <code>model.gradient_checkpointing_enable()</code> to activate it in your custom Trainer or if you use the HF Trainer then you’d activate it with <code>--gradient_checkpointing 1</code>.</p>
<p>XXX: expand on new tech from the paper: <a href="https://arxiv.org/abs/2205.05198">Reducing Activation Recomputation in Large Transformer Models</a> which found a way to avoid most activation recomputations and thus save both memory and compute.</p>
</section>
<section id="memory-efficient-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="memory-efficient-optimizers">Memory-efficient optimizers</h3>
<p>The most common optimizer is Adam. It and its derivatives all use 8 bytes per param (2x fp32 tensors - one for each momentum), which account for almost half the memory allocation for the model, optimizer and gradients. So at times using other optimizers may save the day, if they successfully train that is. Not all optimizers are suitable for all training tasks.</p>
<p>4-byte optimizers:</p>
<ul>
<li><p>There are optimizers like Adafactor that need only 4 bytes. Same goes for the recently invented <a href="https://arxiv.org/abs/2302.06675">LION optimizer</a>.</p></li>
<li><p><code>AnyPrecisionAdamW</code>. Some courageous souls try to do the whole training in BF16 (not mixed precision!), including the optimizer and thus need only 4 bytes per parameter for optim states. See <a href="https://github.com/huggingface/transformers/pull/21312">this work</a>. Hint: this optimizer requires Kahan summation and/or stochastic rounding, see <a href="https://arxiv.org/abs/2010.06192">Revisiting BFloat16 Training (2020)</a>. You need only 8 bytes per parameter for weights, optim states and gradients here! Instead of 18!</p></li>
</ul>
<p>2-byte optimizers:</p>
<ul>
<li>There are quantized solutions like <code>bnb.optim.Adam8bit</code> which uses only 2 bytes instead of 8 (1 byte per momentum). You can get it from <a href="https://github.com/TimDettmers/bitsandbytes">here</a>. Once installed, if you’re using HF Trainer, you can enable it on with just passing <code>--optim adamw_bnb_8bit</code>!</li>
</ul>
<p>For speed comparisons see <a href="https://github.com/huggingface/transformers/issues/22101">this benchmark</a> Speed-wise:<code>apex</code>’s <code>apex.optimizers.FusedAdam</code> optimizer is so far the fastest implementation of Adam. Since pytorch-2.0 <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">torch.optim.AdamW</a> added support for <code>fused=True</code> option, which brings it almost on par with <code>apex.optimizers.FusedAdam</code>.</p>
</section>
</section>
<section id="model-execution-speed" class="level2">
<h2 class="anchored" data-anchor-id="model-execution-speed">Model execution speed</h2>
<section id="forward-vs-backward-execution-speed" class="level3">
<h3 class="anchored" data-anchor-id="forward-vs-backward-execution-speed"><code>forward</code> vs <code>backward</code> Execution Speed</h3>
<p>For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward (e.g.&nbsp;activation forward reads once, writes once, activation backward reads twice, <code>gradOutput</code> and output of the forward, and writes once, <code>gradInput</code>).</p>
</section>
</section>
<section id="memory-profiler-tools" class="level2">
<h2 class="anchored" data-anchor-id="memory-profiler-tools">Memory profiler tools</h2>
<p>In this chapter we discussed the theoretical math of how much this or that feature should consume in MBs of memory. But often in reality things aren’t exactly the same. So you plan for a certain model size and batch sizes but when you come to use it suddenly there is not enough memory. So you need to work with your actual code and model and see which part takes how much memory and where things got either miscalculated or some additional missed overhead hasn’t been accounted for.</p>
<p>You’d want to use some sort of memory profiler for that purpose. There are various memory profilers out there.</p>
<p>One useful tool that I developed for quick and easy profiling of each line or block of code is <a href="https://github.com/stas00/ipyexperiments">IPyExperiments</a>. You just need to load your code into a jupyter notebook and it’ll automatically tell you how much CPU/GPU memory each block allocates/frees. So e.g.&nbsp;if you want to see how much memory loading a model took, and then how much extra memory a single inference step took - including peak memory reporting.</p>
</section>
<section id="vector-and-matrix-size-divisibility" class="level2">
<h2 class="anchored" data-anchor-id="vector-and-matrix-size-divisibility">Vector and matrix size divisibility</h2>
<p>The paper, <a href="https://arxiv.org/abs/2401.14489">The Case for Co-Designing Model Architectures with Hardware</a> investigates the effects of transformer sizing on the underlying hardware. The <a href="https://github.com/EleutherAI/cookbook/tree/main/benchmarks/sizing">associated scripts</a> allow you to run the benchmarks yourself if you’re running on hardware besides NVIDIA V100/A100.</p>
<p>One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model.</p>
<p>For fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features">input/output neuron counts</a> and <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size">batch size</a>.</p>
<p><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc">Tensor Core Requirements</a> define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it’s 64!</p>
<p>For parameters that are small, there is also <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization">Dimension Quantization Effects</a> to consider, this is where tiling happens and the right multiplier can have a significant speedup.</p>
<p><a href="https://arxiv.org/abs/2401.14489">The Case for Co-Designing Model Architectures with Hardware</a> provides much greater detail on tile/wave quantization and the number of attention heads, but the highlights are:</p>
<section id="tile-and-wave-quantization" class="level3">
<h3 class="anchored" data-anchor-id="tile-and-wave-quantization">Tile and wave quantization</h3>
<p>Notation:</p>
<ul>
<li><code>a</code>: Number of attention heads</li>
<li><code>h</code>: Hidden dimension size</li>
<li><code>s</code>: Sequence length</li>
<li><code>b</code>: Microbatch size</li>
<li><code>t</code>: Tensor-parallel size</li>
</ul>
<p>First, some background.</p>
<p>NVIDIA GPUs divide the output matrix into regions or tiles as shown in the below figure and schedule them to one of the available streaming multiprocessors (SM) on the GPU (e.g., A100 GPUs have 108 SMs). Each tile or thread block is processed in a Tensor Core, which NVIDIA introduced for fast tensor operations. NVIDIA Tensor Cores are only available for GEMMs with appropriate dimensions. Tensor Cores can be fully utilized when GEMM dimensions <code>m</code>, <code>k</code>, and <code>n</code> are multiples of 16 bytes and 128 bytes for V100 and A100 GPUs, respectively. Since a FP16 element is 2 bytes, this corresponds to dimension sizes that are multiples of 8 and 64 elements, respectively. If these dimension sizes are not possible, Tensor Cores perform better with larger multiples of 2 bytes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/tiling.png" class="img-fluid figure-img"></p>
<figcaption>tiling</figcaption>
</figure>
</div>
<p>There are multiple tile sizes that the kernel can choose from. If the GEMM size does not divide evenly into the tile size, there will be wasted compute, where the thread block must execute fully on the SM, but only part of the output is necessary. This is called the <strong>tile quantization</strong> effect, as the output is quantized into discrete tiles.</p>
<p>Another quantization effect is called <strong>wave quantization</strong>. As the thread blocks are scheduled to SMs, only 108 thread blocks at a time may be scheduled. If, for example, 109 thread blocks must be scheduled, two rounds, or waves, of thread blocks must be scheduled to GPU. The first wave will have 108 thread blocks, and the second wave will have 1. The second wave will have almost the same latency as the first, but with a small fraction of the useful compute. As the matrix size increases, the last or tail wave grows. The throughput will increase, until a new wave is required. Then, the throughput will drop.</p>
<p>What this means for transformers, is that for a given ratio of <code>h/a</code>, one needs to ensure they’re on the crest of a wave. If you’re using NVIDIA V100/A100 GPUs, we’ve already done this work for you in https://arxiv.org/pdf/2401.14489.pdf</p>
<p>An example of this for 32 attention heads:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/wave-quant.png" class="img-fluid figure-img"></p>
<figcaption>wave quantization</figcaption>
</figure>
</div>
<p>More powers of 2 in <code>h/a</code> helps!</p>
</section>
<section id="number-and-size-of-attention-heads" class="level3">
<h3 class="anchored" data-anchor-id="number-and-size-of-attention-heads">Number and size of attention heads</h3>
<p>Generally, it’s most computationally efficient to keep the ratio of <code>h/a</code> as large as possible without accuracy degradation. A good figure from <a href="https://arxiv.org/abs/2401.14489">The Case for Co-Designing Model Architectures with Hardware</a> showing this effect is:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/attention-less-heads.png" class="img-fluid figure-img"></p>
<figcaption>attention heads</figcaption>
</figure>
</div>
</section>
<section id="flash-attention" class="level3">
<h3 class="anchored" data-anchor-id="flash-attention">Flash attention</h3>
<p>If you’re using <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention</a>, good news! These MHA sizing constraints are taken care of for you. Your only constraint is to have a large enough ratio of <code>h/a</code> to saturate your GPU cores:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/flash-attention.png" class="img-fluid figure-img"></p>
<figcaption>flash attention</figcaption>
</figure>
</div>
</section>
<section id="final-recommendations-for-sizing" class="level3">
<h3 class="anchored" data-anchor-id="final-recommendations-for-sizing">Final recommendations for sizing</h3>
<p>The full recommendations are: 1. Vocab size divisible by 64 2. Microbatch size as large as possible 3. <code>b*s</code>, <code>h/a</code>, and <code>h/t</code> should be divisible by a power of 2 4. <code>(b*a)/t</code> should be an integer 5. <code>t</code> should be small as possible</p>
</section>
</section>
<section id="contributors" class="level2">
<h2 class="anchored" data-anchor-id="contributors">Contributors</h2>
<p><a href="https://github.com/Quentin-Anthony">Quentin Anthony</a></p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bekman2024,
  author = {Bekman, Stas and Foreman, Sam},
  title = {ML {Engineering}},
  date = {2024-02-20},
  url = {https://saforem2.github.io/ml-engineering},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bekman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Bekman, Stas, and Sam Foreman. 2024. <span>“ML Engineering.”</span>
February 20, 2024. <a href="https://saforem2.github.io/ml-engineering">https://saforem2.github.io/ml-engineering</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../qmd/training/reproducibility/index.html" class="pagination-link  aria-label=" reproducibility"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Reproducibility</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../qmd/training/re-train-hub-models.html" class="pagination-link" aria-label="Re-train HF Hub Models From Scratch Using Finetuning Examples">
        <span class="nav-page-text">Re-train HF Hub Models From Scratch Using Finetuning Examples</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a href="https://saforem2.github.io/ml-engineering">ML-Engineering</a></p>
</div>   
    <div class="nav-footer-center">
<p>2024</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/qmd/training/performance/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/qmd/training/performance/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p><a href="https://github.com/saforem2/ml-engineering"><i class="fa-brands fa-github" aria-label="github"></i></a></p>
</div>
  </div>
</footer>




</body></html>