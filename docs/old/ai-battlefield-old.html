<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-02-20">

<title>ML Engineering - 🪖 The AI Battlefield</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href=".././favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/iconify-1.0.8/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
</script>
<link href="https://pvinis.github.io/iosevka-webfont/3.4.1/iosevka.css" rel="stylesheet">


<link rel="stylesheet" href="../css/default.css">
<link rel="stylesheet" href="../css/callouts.css">
<meta property="og:title" content="Sam Foreman">
<meta property="og:description" content="Machine Learning Engineering Open Book">
<meta property="og:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta property="og:site_name" content="ML Engineering">
<meta name="twitter:title" content="Sam Foreman">
<meta name="twitter:description" content="Machine Learning Engineering Open Book">
<meta name="twitter:image" content="https://github.com/saforem2/ml-engineering/blob/main/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="ML Engineering">
<meta name="citation_author" content="Stas Bekman">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-02-20">
<meta name="citation_cover_date" content="2024-02-20">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-02-20">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/ml-engineering">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=The climate risk &amp;amp;amp; resilience portal (ClimRR) metadata and data dictionary;,citation_author=C. Burdi;,citation_author=Wall. T Branham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://dub.sh/ClimRR-Metadata;">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Energy Justice Analysis of Climate Data with ClimRR;,citation_author=Sam Foreman;,citation_publication_date=2023-08-07;,citation_cover_date=2023-08-07;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/climate-analysis;,citation_language=en;">
<meta name="citation_reference" content="citation_author=Sam Foreman;,citation_publication_date=2023-08-19;,citation_cover_date=2023-08-19;,citation_year=2023;,citation_fulltext_html_url=https://saforem2.github.io/l2hmc-qcd;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Deep learning hamiltonian monte carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;">
<meta name="citation_reference" content="citation_title=MLMC: Machine learning monte carlo for lattice gauge theory;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James Osborn;,citation_publication_date=00;,citation_cover_date=00;,citation_year=0;,citation_conference_title=40th international symposium on lattice field theory (lattice 2023) (batavia, IL, united states, 07/31/2023 - 08/04/2023);">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">ML Engineering</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/saforem2"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:twitter"></iconify-icon></span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/saforem2/ml-engineering"> 
<span class="menu-text"><span style="font-size: 1.15em;"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon></span></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-gear"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/blob/main/index.qmd">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/saforem2/ml-engineering/issues/new/choose">
            New Issue
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">🪖 The AI Battlefield</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/resources/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📓 Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/testing/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">✏️ Testing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/transformers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🤗 Transformers</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/insights/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🧠  Insights</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/insights/ai-battlefield.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🪖 The AI Battlefield</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/training/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🏋️  Training</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/dtype.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tensor precision / Data types</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/dtype-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tensor precision / Data types</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/emulate-multi-node.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Emulate a multi-node setup using just a single node</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/hparams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Selecting Training Hyper-Parameters And Model Initializations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/model-parallelism/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/checkpoints/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Checkpoints</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/reproducibility/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/performance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Software Tune Up For The Best Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/re-train-hub-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Re-train HF Hub Models From Scratch Using Finetuning Examples</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/training/instabilities/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Avoiding, Recovering From and Understanding Instabilities</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/instabilities/training-loss-patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Understanding Training Loss Patterns</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/training/fault-tolerance/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/training/fault-tolerance/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fault Tolerance</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/network/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inter-node and Intra-Node Networking Hardware</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/network/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🛜 Network</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/network/benchmarks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networking Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/network/benchmarks/results/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Network Benchmarks Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/network/benchmarks/results/disable-nvlink.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Disabling NVLink Benchmark</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/storage/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📦  Storage</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">
 <span class="menu-text">Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false">
 <span class="menu-text">Results</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/storage/benchmarks/results/hope-2023-12-20-14-37-02-331702-summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">fio benchmark results for hope on 2023-12-20-14:37:02</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/compute/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">💻 Compute</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/compute/cpu/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/compute/cpu-memory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CPU memory</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/compute/accelerator/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accelerators</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/compute/accelerator/index-old.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accelerators</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false">
 <span class="menu-text">Nvidia</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/compute/accelerator/nvidia/debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Troubleshooting NVIDIA GPUs</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/orchestration/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🎻  Orchestration</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/orchestration/slurm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Working in SLURM Environment</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/orchestration/slurm/admin.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Administration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/orchestration/slurm/launchers/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Launchers with SLURM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/orchestration/slurm/performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/orchestration/slurm/users.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SLURM for users</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../qmd/debug/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🐛  Debugging</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/tiny-scripts/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Back up of scripts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/make-tiny-models-tokenizers-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Faster debug and development with tiny models, tokenizers and datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/nccl-performance-debug.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NCCL: Debug and Performance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debugging PyTorch programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Debug Tools</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/torch-distributed-hanging-solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diagnosing Hangings and Deadlocks in Multi-Node Multi-GPU Python Programs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/debug/underflow_overflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Underflow and Overflow Detection</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-ai-battlefield-engineering---what-you-need-to-know" id="toc-the-ai-battlefield-engineering---what-you-need-to-know" class="nav-link active" data-scroll-target="#the-ai-battlefield-engineering---what-you-need-to-know">The AI Battlefield Engineering - What You Need To Know</a>
  <ul class="collapse">
  <li><a href="#basics" id="toc-basics" class="nav-link" data-scroll-target="#basics">Basics</a>
  <ul class="collapse">
  <li><a href="#whats-important-in-the-ai-race" id="toc-whats-important-in-the-ai-race" class="nav-link" data-scroll-target="#whats-important-in-the-ai-race">What’s important in the AI race?</a></li>
  <li><a href="#what-are-the-needs-of-llm-training" id="toc-what-are-the-needs-of-llm-training" class="nav-link" data-scroll-target="#what-are-the-needs-of-llm-training">What are the needs of LLM training?</a></li>
  <li><a href="#what-are-the-workhorses-of-ml" id="toc-what-are-the-workhorses-of-ml" class="nav-link" data-scroll-target="#what-are-the-workhorses-of-ml">What are the workhorses of ML?</a></li>
  <li><a href="#ai-driving-entities" id="toc-ai-driving-entities" class="nav-link" data-scroll-target="#ai-driving-entities">AI driving entities</a></li>
  <li><a href="#information-sharing" id="toc-information-sharing" class="nav-link" data-scroll-target="#information-sharing">Information sharing</a></li>
  <li><a href="#the-ai-bubble" id="toc-the-ai-bubble" class="nav-link" data-scroll-target="#the-ai-bubble">The AI bubble</a></li>
  </ul></li>
  <li><a href="#ml-engineers-heaven-and-hell" id="toc-ml-engineers-heaven-and-hell" class="nav-link" data-scroll-target="#ml-engineers-heaven-and-hell">ML Engineer’s heaven and hell</a>
  <ul class="collapse">
  <li><a href="#ml-engineers-heaven" id="toc-ml-engineers-heaven" class="nav-link" data-scroll-target="#ml-engineers-heaven">ML Engineer’s heaven</a></li>
  <li><a href="#ml-engineers-hell" id="toc-ml-engineers-hell" class="nav-link" data-scroll-target="#ml-engineers-hell">ML Engineer’s hell</a></li>
  </ul></li>
  <li><a href="#getting-compute" id="toc-getting-compute" class="nav-link" data-scroll-target="#getting-compute">Getting compute</a>
  <ul class="collapse">
  <li><a href="#renting-on-the-cloud" id="toc-renting-on-the-cloud" class="nav-link" data-scroll-target="#renting-on-the-cloud">Renting on the cloud</a></li>
  <li><a href="#using-hpc" id="toc-using-hpc" class="nav-link" data-scroll-target="#using-hpc">Using HPC</a></li>
  <li><a href="#buying-hardware" id="toc-buying-hardware" class="nav-link" data-scroll-target="#buying-hardware">Buying hardware</a></li>
  <li><a href="#managing-compute" id="toc-managing-compute" class="nav-link" data-scroll-target="#managing-compute">Managing compute</a></li>
  </ul></li>
  <li><a href="#the-needs-of-technology" id="toc-the-needs-of-technology" class="nav-link" data-scroll-target="#the-needs-of-technology">The needs of technology</a>
  <ul class="collapse">
  <li><a href="#can-you-feed-the-furnace-fast-enough" id="toc-can-you-feed-the-furnace-fast-enough" class="nav-link" data-scroll-target="#can-you-feed-the-furnace-fast-enough">Can you feed the furnace fast enough?</a></li>
  <li><a href="#tflops" id="toc-tflops" class="nav-link" data-scroll-target="#tflops">TFLOPS</a></li>
  <li><a href="#model-flops-utilization-mfu" id="toc-model-flops-utilization-mfu" class="nav-link" data-scroll-target="#model-flops-utilization-mfu">Model Flops Utilization (MFU)</a></li>
  <li><a href="#moving-bits" id="toc-moving-bits" class="nav-link" data-scroll-target="#moving-bits">Moving bits</a></li>
  </ul></li>
  <li><a href="#key-hardware-components" id="toc-key-hardware-components" class="nav-link" data-scroll-target="#key-hardware-components">Key hardware components</a>
  <ul class="collapse">
  <li><a href="#accelerators" id="toc-accelerators" class="nav-link" data-scroll-target="#accelerators">Accelerators</a></li>
  <li><a href="#network" id="toc-network" class="nav-link" data-scroll-target="#network">Network</a></li>
  <li><a href="#storage" id="toc-storage" class="nav-link" data-scroll-target="#storage">Storage</a></li>
  <li><a href="#cpu-memory" id="toc-cpu-memory" class="nav-link" data-scroll-target="#cpu-memory">CPU Memory</a></li>
  <li><a href="#cpu" id="toc-cpu" class="nav-link" data-scroll-target="#cpu">CPU</a></li>
  </ul></li>
  <li><a href="#impress-others-with-your-ml-instant-math" id="toc-impress-others-with-your-ml-instant-math" class="nav-link" data-scroll-target="#impress-others-with-your-ml-instant-math">Impress others with your ML instant math</a>
  <ul class="collapse">
  <li><a href="#tell-how-many-gpus-do-you-need-in-5-secs" id="toc-tell-how-many-gpus-do-you-need-in-5-secs" class="nav-link" data-scroll-target="#tell-how-many-gpus-do-you-need-in-5-secs">Tell how many GPUs do you need in 5 secs</a></li>
  </ul></li>
  <li><a href="#traps-to-be-aware-of" id="toc-traps-to-be-aware-of" class="nav-link" data-scroll-target="#traps-to-be-aware-of">Traps to be aware of</a>
  <ul class="collapse">
  <li><a href="#say-no-to-will-make-a-reasonable-effort-to-contracts" id="toc-say-no-to-will-make-a-reasonable-effort-to-contracts" class="nav-link" data-scroll-target="#say-no-to-will-make-a-reasonable-effort-to-contracts">Say no to “will make a reasonable effort to …” contracts</a></li>
  <li><a href="#beware-of-hardware-and-software-lock-in-scenarios" id="toc-beware-of-hardware-and-software-lock-in-scenarios" class="nav-link" data-scroll-target="#beware-of-hardware-and-software-lock-in-scenarios">Beware of hardware and software lock-in scenarios</a></li>
  <li><a href="#dont-buy-what-you-dont-really-need" id="toc-dont-buy-what-you-dont-really-need" class="nav-link" data-scroll-target="#dont-buy-what-you-dont-really-need">Don’t buy what you don’t really need</a></li>
  </ul></li>
  <li><a href="#unsolicited-advice" id="toc-unsolicited-advice" class="nav-link" data-scroll-target="#unsolicited-advice">Unsolicited advice</a>
  <ul class="collapse">
  <li><a href="#fomo-and-avoiding-depression" id="toc-fomo-and-avoiding-depression" class="nav-link" data-scroll-target="#fomo-and-avoiding-depression">FOMO and avoiding depression</a></li>
  <li><a href="#dont-try-to-know-everything" id="toc-dont-try-to-know-everything" class="nav-link" data-scroll-target="#dont-try-to-know-everything">Don’t try to know everything</a></li>
  <li><a href="#dont-beat-yourself-up-when-using-half-baked-software" id="toc-dont-beat-yourself-up-when-using-half-baked-software" class="nav-link" data-scroll-target="#dont-beat-yourself-up-when-using-half-baked-software">Don’t beat yourself up when using half-baked software</a></li>
  </ul></li>
  <li><a href="#contributors" id="toc-contributors" class="nav-link" data-scroll-target="#contributors">Contributors</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/old/ai-battlefield-old.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/old/ai-battlefield-old.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="ai-battlefield-old.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">🪖 The AI Battlefield</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="https://github.com/saforem2/ml-engineering/blob/main/old/ai-battlefield-old.qmd"><i class="bi"></i></button></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading"></div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 20, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="the-ai-battlefield-engineering---what-you-need-to-know" class="level1">
<h1>The AI Battlefield Engineering - What You Need To Know</h1>
<p>This chapter is one person’s opinionated overview of the ML/AI Engineering reality, which may or may not be another person’s reality. The intention is to help you start asking the right questions and get your ML Engineering needs met.</p>
<section id="basics" class="level2">
<h2 class="anchored" data-anchor-id="basics">Basics</h2>
<section id="whats-important-in-the-ai-race" class="level3">
<h3 class="anchored" data-anchor-id="whats-important-in-the-ai-race">What’s important in the AI race?</h3>
<p>Training:</p>
<ol type="1">
<li>How fast one can train a better model (first to market advantage)</li>
<li>How much $ was spent (do we still have money left to pay salaries to talent after training?)</li>
</ol>
<p>Inference:</p>
<ol type="1">
<li>Fast latency (users are used to msec response times and will leave if the response takes seconds)</li>
<li>Fast throughput (how many concurrent queries can be processed)</li>
<li>How much $ is being spent per user (can we rent more GPUs to acquire more users and/or improve (1) and (2) ?)</li>
</ol>
</section>
<section id="what-are-the-needs-of-llm-training" class="level3">
<h3 class="anchored" data-anchor-id="what-are-the-needs-of-llm-training">What are the needs of LLM training?</h3>
<ol type="1">
<li>Fast compute massively dominated by matrix multiplications</li>
<li>Fast enough memory, IO, network and CPU to feed the compute</li>
</ol>
<p>Corollary: If when you buy or rent hardware you invest in the fastest accelerators, but cheap out on any of the other components you wasted $ and you might not win the race as it’ll take longer to train.</p>
</section>
<section id="what-are-the-workhorses-of-ml" class="level3">
<h3 class="anchored" data-anchor-id="what-are-the-workhorses-of-ml">What are the workhorses of ML?</h3>
<ul>
<li><p>An accelerator or a processing unit is what does most of the work.</p></li>
<li><p>Since ML does a lot of parallel processing (<a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>) GPUs were used at the beginning, but now you additionally have TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs, etc. Recent CPUs are becoming used as accelerators as well, especially for inference.</p></li>
</ul>
<p><a href="../compute/accelerator">More details</a>.</p>
</section>
<section id="ai-driving-entities" class="level3">
<h3 class="anchored" data-anchor-id="ai-driving-entities">AI driving entities</h3>
<ul>
<li>AI companies - train models/build products around self-trained or trained-by-others’ models, in-house research.</li>
<li>Academia - does massive research and write papers. Lots of new ideas are generated.</li>
<li>AI enthusiasts - lots of good will available, some pull resources/talents together to train open access models, with donated compute by HPCs and an occasional cloud, or a university cluster.</li>
<li>Entrepreneurs - lots of low hanging fruit to pick - creative reselling of services, making ML-driven apps, and using various ingenious combinations of available resources to create amazing outcomes.</li>
</ul>
</section>
<section id="information-sharing" class="level3">
<h3 class="anchored" data-anchor-id="information-sharing">Information sharing</h3>
<ul>
<li>It’s very surprising that almost everybody involved in the domain of AI shares a lot of the discoveries with the community.</li>
<li>Surely, companies don’t disclose all of their IP, but a lot of it does get shared in the form of knowledge or model weights</li>
<li>Companies that publish a lot of IP and models tend to attract higher quality talent.</li>
<li>Twitter seems to be the central platform where one must be to follow what’s going on</li>
</ul>
</section>
<section id="the-ai-bubble" class="level3">
<h3 class="anchored" data-anchor-id="the-ai-bubble">The AI bubble</h3>
<ul>
<li><p>The <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">Dot-com bubble</a> occurred during 1995-2000. And a very similar situation is happening right now in the AI space.</p></li>
<li><p>There is a lot of money available to create new startups or boost the existing companies. It’s relatively easy to raise millions of dollars.</p></li>
<li><p>As we are in the wild-wild-west stage of the AI industry it’s very difficult to predict the future, and so pretty much anything goes as far as startup ideas go, as long as it sounds reasonable.</p></li>
<li><p>What distinguishes the AI bubble from the Dot-com bubble, is that one didn’t actually need much money to operate a Dot-com company - most of the raised money went to marketing and some to staff, barely any to compute. AI companies need millions of dollars because training LLMs requires an insane amount of compute, and that compute is very expensive. e.g.&nbsp;1x NVIDIA H100 costs ~$30k and a company may need 512 of those, which is $15M (not counting the other hardware components and related costs)!</p></li>
</ul>
</section>
</section>
<section id="ml-engineers-heaven-and-hell" class="level2">
<h2 class="anchored" data-anchor-id="ml-engineers-heaven-and-hell">ML Engineer’s heaven and hell</h2>
<p>This is my personal LLM/VLM trainings-based heaven and hell. YMMV.</p>
<section id="ml-engineers-heaven" class="level3">
<h3 class="anchored" data-anchor-id="ml-engineers-heaven">ML Engineer’s heaven</h3>
<ol type="1">
<li><p>A well built HPC, or a full service cloud based cluster, where someone diligently and timely takes care of the hardware and the systems.</p>
<p>I just need to bring my training software and do the training, which is already an insanely complicated job requiring special skills.</p></li>
<li><p>Lots of nodes available for exclusive unlimited use</p></li>
<li><p>Fast inter-node connectivity that doesn’t bottleneck the accelerators and which isn’t shared with other users</p></li>
<li><p>Huge local super-fast NVME based shared filesystem that can fit datasets and checkpoints</p></li>
<li><p>Barebones Linux w/ SLURM and minimal software to be able to launch training jobs</p></li>
<li><p><code>sudo</code>er access to ease the work with a team of people</p></li>
</ol>
</section>
<section id="ml-engineers-hell" class="level3">
<h3 class="anchored" data-anchor-id="ml-engineers-hell">ML Engineer’s hell</h3>
<ol type="1">
<li><p>A cloud or in-house cluster, where you have to do everything - sysadmining, replacing hardware, dealing with outages, etc. And to do the training on top of that.</p></li>
<li><p>A smallish slow shared filesystem (NFS?), with cloud to draw data from and checkpoint to</p></li>
<li><p>Slow inter-node leading to low accelerator utilization</p></li>
<li><p>Inter-node shared with other users which make the network erratic and unpredictable</p></li>
<li><p>Super-complicated cloud console with gazillion of screens and steps to set even simple things up</p></li>
<li><p>Not being able to swap out failing hardware fast</p></li>
<li><p>Needing to timeshare the nodes - with wait times between training jobs</p></li>
<li><p>Having other concurrent users who might use up the whole disk, leading to trainings crashing</p></li>
<li><p>Not being able to kill jobs others on the team started and went to sleep</p></li>
</ol>
</section>
</section>
<section id="getting-compute" class="level2">
<h2 class="anchored" data-anchor-id="getting-compute">Getting compute</h2>
<p>There are 3 main choices to where one gets compute:</p>
<ul>
<li>Rent on the cloud</li>
<li>Get a timeshare on an HPC</li>
<li>Buy it</li>
</ul>
<section id="renting-on-the-cloud" class="level3">
<h3 class="anchored" data-anchor-id="renting-on-the-cloud">Renting on the cloud</h3>
<p>This is currently the prevalent way of getting compute.</p>
<p>Pros:</p>
<ul>
<li>Easy to expand or contract the size of the cluster</li>
<li>Easy to upgrade from the old hardware generation to the new one in a few years</li>
<li>Cluster management could be easily outsourced</li>
</ul>
<p>Cons:</p>
<ul>
<li>Expensive, unless you negotiate a long term (1-3 year) contract for hundreds of accelerators</li>
<li>You will be tempted to buy many tools and services that you may or may not need</li>
<li>You always get charged whether you use your cluster fully or not</li>
</ul>
</section>
<section id="using-hpc" class="level3">
<h3 class="anchored" data-anchor-id="using-hpc">Using HPC</h3>
<p>There aren’t that many HPCs out there and so the amount of available resources is limited.</p>
<p>Pros: - Managed for you - all you need is your software to do the training and a bit of <a href="../orchestration/slurm">SLURM</a> know-how to launch jobs - Often sponsored by the local government/university - probably could get the job done for less $ or even free (e.g.&nbsp;we trained <a href="https://huggingface.co/bigscience/bloom">BLOOM-176B</a> for free on <a href="http://www.idris.fr/eng/jean-zay/">JeanZay HPC</a>!)</p>
<p>Cons: - needing to time share compute with other teams == short job times with possible long wait times in between - could be difficult to finish training quickly - The inter-node network is likely to be unstable as it’ll be used by other teams - Have to abide by the HPC’s rules (e.g.&nbsp;no <code>sudo</code> access and various other rules to follow) - In a way the HPC cluster will be what it’ll be - you can’t make the network faster and often even getting some software installed can be tricky.</p>
</section>
<section id="buying-hardware" class="level3">
<h3 class="anchored" data-anchor-id="buying-hardware">Buying hardware</h3>
<p>It’s mainly universities that buy and build their own clusters, and some big companies do that too.</p>
<p>Pros:</p>
<ul>
<li>If you can deploy the hardware 24/7 for more than a few years the total cost will be cheaper than renting</li>
<li>Easy to provide fast local storage - a good NVME raid would be much cheaper and faster than online storage</li>
</ul>
<p>Cons:</p>
<ul>
<li>You’re stuck with the outdated hardware just a few years after it was purchased - might be able to resell</li>
<li>Must buy more than needed - Hardware tends to break, especially when it’s used 24/7, RMA could take weeks</li>
<li>Have to hire talent to manage the in-house solution</li>
<li>Have to figure out cooling, electric costs, insurance, etc.</li>
</ul>
</section>
<section id="managing-compute" class="level3">
<h3 class="anchored" data-anchor-id="managing-compute">Managing compute</h3>
<ul>
<li>Unless you use a fully managed HPC compute you absolutely need to hire a sysadmin. It may feel that your ML engineers can swing that between their training jobs, but they will be losing a lot of time to managing disk space, dealing with problematic nodes, asking users to behave, etc.</li>
</ul>
</section>
</section>
<section id="the-needs-of-technology" class="level2">
<h2 class="anchored" data-anchor-id="the-needs-of-technology">The needs of technology</h2>
<section id="can-you-feed-the-furnace-fast-enough" class="level3">
<h3 class="anchored" data-anchor-id="can-you-feed-the-furnace-fast-enough">Can you feed the furnace fast enough?</h3>
<p>Imagine a steam locomotive - the engine is great, but if the <a href="https://en.wikipedia.org/wiki/Fireman_(steam_engine)">fireman</a> isn’t fast enough to shovel the coal in, the train won’t move fast.</p>
<p><img src="images/640px-Baureihe52Heizer.jpg" class="img-fluid"></p>
<p><a href="https://commons.wikimedia.org/wiki/File:Baureihe52Heizer.jpg">source</a></p>
<p>This is the current state of ML hardware: The bottleneck is in moving bits and not the compute.</p>
<ul>
<li>Accelerators get ~2x faster every 2 years (<a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore’s law</a>)</li>
<li>Network and memory are not! Already now both are compute bottlenecks</li>
<li>IO can be another bottleneck if your DataLoader has to pull data from the cloud</li>
<li>CPU is fine as long as it has enough cpu-cores for DataLoader workers, and main processes</li>
</ul>
<p>Corollary: research the whole machine and not just its engine.</p>
<p>a crazy idea: the older GPUs might do fine if you can actually feed them as fast as they can compute. And if you can get 3x of them at the same cost as the next generation GPU you might finish training sooner and a lower cost.</p>
</section>
<section id="tflops" class="level3">
<h3 class="anchored" data-anchor-id="tflops">TFLOPS</h3>
<ul>
<li><p>Once you choose the architecture and the size of the model and how many tokens you want to train the model for you immediately know how much compute will be required to accomplish this goal. Specifically you can now calculate <a href="../training/performance/README.md#tflops-as-a-performance-metric">how many floating point operations will be needed</a>.</p></li>
<li><p>All that is missing is comparing different compute providers to how many floating point operations their hardware can computes per secs (TFLOPS) and their cost per unit and now you can tell the total approximate cost of the training.</p></li>
</ul>
<ol type="1">
<li><p>Calculate the time needed to train given the TFLOPS of the considered solution:</p>
<p><code>total_tflops_required / tflops_of_this_compute_unit = time_in_seconds</code></p>
<p>Let’s say it came to be 604800 secs or 7 days.</p></li>
<li><p>Look at the cost of using this compute solution for 7 days and now you know the total $ to train this model.</p></li>
<li><p>Look at other proposals and calculate the same - chose the best option.</p></li>
</ol>
<ul>
<li>As mentioned earlier, time is of a huge importance, so you might still choose a more expensive solution if finishing the training sooner is important because you want to be first to market.</li>
</ul>
<p>Unfortunately, this math is only partially correct because the advertised peak TFLOPS are typically unachievable. The MFU section delves into it.</p>
</section>
<section id="model-flops-utilization-mfu" class="level3">
<h3 class="anchored" data-anchor-id="model-flops-utilization-mfu">Model Flops Utilization (MFU)</h3>
<p>As mentioned in the previous section, some (most?) vendors publish unrealistic peak performance TFLOPS - they aren’t possible to achieve.</p>
<p>Model Flops Utilization (MFU) is the metric that tells us how well the accelerator is utilized. Here is how it is calculated:</p>
<ol type="1">
<li>Measure the actual TFLOPS by calculating how many floating point operations a single training iteration takes and dividing that number by the number of seconds this iteration took.</li>
<li>Divide the actual TFLOPS by advertised TFLOPS to get the MFU</li>
</ol>
<p>Example: Let’s say you’re training in BFLOAT16 precision:</p>
<ul>
<li>If a single iteration requires 624 Tera floating point operations and it took 4 secs to run then we know that we get: <code>624/4=156</code> actual TFLOPS</li>
<li>now BF16@A100 is <a href="https://www.nvidia.com/en-us/data-center/a100/">advertised as 312TFLOPS</a> so <code>156/312=0.5</code> gives us 50% MFU.</li>
</ul>
<p>Practically: - with NVIDIA GPUs if you’re above 50% MFU on a multi-node setup with a large model you’re already doing fantastic - recent advancements in more efficient scalability solutions keep on increasing MFU - slow networks and inefficient frameworks or untuned configuration lower MFU</p>
<p>Therefore once you know the MFU you can now adjust the cost estimate from the previous section. In the example there we said it’ll take 7 days to train, but if MFU is 50%, it means it’ll take 14 days to train.</p>
</section>
<section id="moving-bits" class="level3">
<h3 class="anchored" data-anchor-id="moving-bits">Moving bits</h3>
<p>Why can’t the advertised TFLOPS achieved? It’s because it takes time to move data between accelerator memory and compute and additionally it takes even more time to move data from disk and other gpus to the accelerator’s memory.</p>
<ul>
<li><p>There is not much can be done about the accelerator memory since its bandwidth is what it is - one can only write more efficient software to make data move faster to/from the accelerator - hint: fused and custom written kernels (like <a href="https://pytorch.org/docs/stable/generated/torch.compile.html">torch.compile</a> and <a href="https://github.com/Dao-AILab/flash-attention">flash attention</a>)</p></li>
<li><p>If you only have a single GPU and the model fits its memory, you don’t need to worry about the network - accelerator memory is the only bottleneck. But if you have <a href="../training/model-parallelism">to shard the model across multiple GPUs</a> network becomes the bottleneck.</p></li>
<li><p>Intra-node Network - is very fast, but difficult to take advantage of for large models - <a href="../training/model-parallelism#tensor-parallelism">Tensor parallelism</a> and <a href="../training/model-parallelism#sequence-parallelism">sequence parallelism</a> address part of this problem. (<a href="../network/README.md#intra-node-networking">more</a>).</p></li>
<li><p>Inter-node Network - typically is too slow on most server setups - thus this is the key component to research! Efficient frameworks succeed to partially hide the comms overhead by overlapping compute and comms. But if comms take longer than compute, the comms are still the bottleneck. <a href="#inter-node-network">more</a>.</p></li>
<li><p>Storage IO is important primarily for feeding the DataLoader workers and saving the checkpoints. <a href="#storage">more</a>.</p>
<ol type="1">
<li>Typically with enough DL workers the DataLoader adds very little overhead.</li>
<li>While checkpoints are being saved the accelerators idle unless some async saving solution is used, so fast IO is crucial here</li>
</ol></li>
</ul>
</section>
</section>
<section id="key-hardware-components" class="level2">
<h2 class="anchored" data-anchor-id="key-hardware-components">Key hardware components</h2>
<section id="accelerators" class="level3">
<h3 class="anchored" data-anchor-id="accelerators">Accelerators</h3>
<p>As of this writing here are the most common accelerators that can be used for training, finetuning and inference ML models:</p>
<p>Widely available:</p>
<ul>
<li>NVIDIA A100 - huge availability across all clouds, but is already gradually being replaced by H100</li>
</ul>
<p>Available, but locks you in:</p>
<ul>
<li>Google TPUs - fast! but the cost is a lock-in into a single vendor and cloud</li>
</ul>
<p>Emerging to general availability:</p>
<ul>
<li><p>NVIDIA H100 - 2-3x faster than A100 (half precision), 6x faster for fp8</p></li>
<li><p>AMD MI250 ~= A100 - very few clouds have them and most likely MI300X will be the first mainstream AMD GPU</p></li>
<li><p>AMD MI300X ~= H100 - a few clouds will have those in March, 2024</p></li>
<li><p>Intel Gaudi2 ~= H100 - starting to slowly emerge on Intel’s cloud</p></li>
<li><p>GraphCore IPU - very difficult to find, paperspace has them</p></li>
<li><p>Cerebras WaferScale Engine - available on Cerebras’ cloud</p></li>
</ul>
<section id="accelerator-interoperability" class="level4">
<h4 class="anchored" data-anchor-id="accelerator-interoperability">Accelerator Interoperability</h4>
<p>In general most (all?) accelerators are supported by major frameworks like PyTorch or TensorFlow and the same code should run everywhere with small modifications as long as it doesn’t use any accelerator-specific functionality.</p>
<p>For example, if your PyTorch application includes custom CUDA kernels it’ll only work on NVIDIA GPUs and may be on AMD MI-series.</p>
<ul>
<li><p>NVIDIA GPUs: all based on <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, which most training frameworks support. You can easily moved between different NVIDIA GPUs and most things would work the same.</p></li>
<li><p>AMD MI250/MI300: with PyTorch using <a href="https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/">ROCm</a> you can run most CUDA-based software as is. This is really the only inter-operable accelerator with the NVIDIA stack.</p></li>
<li><p>Gaudi2: if you use HF Transformers/Diffusers you can use <a href="https://github.com/huggingface/optimum-habana">optimum-habana</a>. If you use HF Trainer with NVIDIA GPUs it should be relatively easy to switch to train/infer on Gaudi2.</p></li>
<li><p>GraphCore IPU: can also be run via PyTorch via <a href="https://github.com/graphcore/poptorch">poptorch</a></p></li>
<li><p>Cerebras: is also working on PyTorch support via <a href="https://www.cerebras.net/blog/supporting-pytorch-on-the-cerebras-wafer-scale-engine/">Cerebras Software Platform (CSoft) via XLA</a>.</p></li>
</ul>
<p>Also in general most ML code could be compiled into cross-platform formats like <a href="https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange">Open Neural Network Exchange (ONNX)</a> which can be run on a variety of accelerators. This approach is typically used more often for inference workloads.</p>
</section>
</section>
<section id="network" class="level3">
<h3 class="anchored" data-anchor-id="network">Network</h3>
<ul>
<li><p>If you want to train a large model that doesn’t fit onto a single accelerator’s memory you have to rely on the intra- and inter-node networks to synchronize multiple accelerators.</p></li>
<li><p>The biggest issue right now is that compute hardware advancements move faster than networking hardware, e.g.&nbsp;for NVIDIA NVLink intra-node:</p></li>
</ul>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">GPU</th>
<th style="text-align: right;">Compute<b>fp16<br>TFLOPS</b></th>
<th style="text-align: right;">Compute<br>speedup</th>
<th style="text-align: right;">Intra-node<br>GBps</th>
<th style="text-align: right;">Intra-node<br>speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">V100</td>
<td style="text-align: right;">125</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">300</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">A100</td>
<td style="text-align: right;">312</td>
<td style="text-align: right;">2.5</td>
<td style="text-align: right;">600</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">H100</td>
<td style="text-align: right;">989</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">900</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
<ul>
<li><p>You can see that A100 was 2.5 faster than V100, and H100 is ~3x faster than A100. But the intra-node speed of NVLink has only increased by 300GBps each generation.</p></li>
<li><p>Moreover, all 3 generations of NVLink use identical NICs of the same 50GBps duplex throughput. They have just doubled and tripled the number of links to speed things up. So there was 0 progress in that technology.</p></li>
<li><p>The inter-node situation isn’t any better with most NICs there doing 100 or 200Gbps, and some 400Gbps are starting to emerge. (correspondingly in GBps: 12.5, 25 and 50). It’s the same story here, some solutions provide dozens of NICs to get to higher speeds.</p></li>
<li><p>Also typically with LLMs the payload is so large that network latency is often negligible for training. It’s still quite important for inference.</p></li>
</ul>
<section id="intra-node-network" class="level4">
<h4 class="anchored" data-anchor-id="intra-node-network">Intra-node Network</h4>
<ul>
<li><p>Pay attention to bytes vs bits. 1Byte = 8bits. 1GBps = 8Gbps.</p></li>
<li><p>If you need to reduce bits (e.g.&nbsp;gradients) across multiple nodes, it’s the slowest link (Inter-node) that defines the overall throughput, so intra-node speed doesn’t matter then</p></li>
<li><p><a href="../training/model-parallelism#tensor-parallelism">Tensor parallelism</a> and <a href="../training/model-parallelism#sequence-parallelism">sequence parallelism</a> have to remain within the node to be efficient - only makes sense with fast intra-node speed</p></li>
</ul>
<p>NVIDIA:</p>
<ul>
<li><p>NVIDIA-based compute nodes come with 50GBps duplex NVLInk</p></li>
<li><p>Some have a lot of NVLinks, others less but typically plenty w/ at least 900GBps (5.6Tbps) duplex for H100, 600GBps for A100 nodes</p></li>
</ul>
<p>Intel Gaudi2:</p>
<ul>
<li>8x 21 NICs of 100GbE RoCE v2 ROMA for a total of 2.1TBps</li>
</ul>
<p><a href="../network/README.md#intra-node-networking">More details</a></p>
</section>
<section id="inter-node-network" class="level4">
<h4 class="anchored" data-anchor-id="inter-node-network">Inter-node Network</h4>
<ul>
<li><p>An order of magnitude slower than Intra-node</p></li>
<li><p>You will see a wide range of speeds from 50Gbps to 3200 Gbps</p></li>
<li><p>You need to reduce gradients and other bits faster than compute to avoid idling accelerators</p></li>
<li><p>You typically get at most 80% of advertised speed. e.g., if you are told you get 800Gbps, expect ~480Gbps.</p></li>
<li><p>If moving to fp8 H100 is 18x faster than V100</p></li>
<li><p>We are yet to see if 3200Gbps for H100s will be enough to keep high MFU.</p></li>
<li><p>Practically less than 3x but it’s a good estimate</p></li>
</ul>
<p><a href="../network/README.md#inter-node-networking">More details</a>.</p>
</section>
</section>
<section id="storage" class="level3">
<h3 class="anchored" data-anchor-id="storage">Storage</h3>
<p>There are 3 distinct Storage IO needs in the ML workload:</p>
<ol type="1">
<li>You need to be able to feed the DataLoader fast - (super fast read, don’t care about fast write) - requires sustainable load for hours and days</li>
<li>You need to be able to write checkpoints fast - (super fast write, fastish read as you will be resuming a few times) - requires burst writing - you want super fast to not block the training for long (unless you use some sort of cpu offloading to quickly unblock the training)</li>
<li>You need to be able to load and maintain your codebase - (medium speed for both reading and writing) - this also needs to be shared since you want all nodes to see the same codebase - as it happens only during the start or resume it’ll happen infrequently</li>
</ol>
<ul>
<li><p>Most of the time you’re being sold 80% of what you paid. If you want a reliable 100TBs you need to rent 125TBs or your application may fail to write long before the disk is full.</p></li>
<li><p>Shared Distributed Filesystem:</p>
<ol type="1">
<li>non-parallel shared file systems can be extremely slow if you have a lot of small files (=Python!)</li>
<li>You want Parallel FS like GPFS (IBM Spectrum Scale) or Lustre (Open Source)</li>
</ol></li>
</ul>
<p><a href="../storage/README.md">More details</a>.</p>
</section>
<section id="cpu-memory" class="level3">
<h3 class="anchored" data-anchor-id="cpu-memory">CPU Memory</h3>
<p>You need enough memory for:</p>
<ul>
<li><p>2-3 possibly DL workers per Accelerator (so 16-24 processes with 8 accelerators per node)</p></li>
<li><p>Even more memory for DL workers if you pull data from the cloud</p></li>
<li><p>Enough memory to load the model if you can’t load to accelerator directly</p></li>
<li><p>Often used for accelerator memory offloading - extends accelerator’s memory by swapping out the currently unused layers - if that’s the target use, then the more cpu memory is available - the better!</p></li>
</ul>
</section>
<section id="cpu" class="level3">
<h3 class="anchored" data-anchor-id="cpu">CPU</h3>
<p>This is probably the least worrisome component.</p>
<ul>
<li><p>Most clouds provide beefy CPUs with plenty of cpu cores</p></li>
<li><p>You need to have enough cores to run 2-3 DL workers +1 per gpu - so at least 30 cores</p></li>
<li><p>Even more cores for DL workers if you have complex and/or slow DL transforms (CV)</p></li>
<li><p>Most of the compute happens on GPUs</p></li>
</ul>
</section>
</section>
<section id="impress-others-with-your-ml-instant-math" class="level2">
<h2 class="anchored" data-anchor-id="impress-others-with-your-ml-instant-math">Impress others with your ML instant math</h2>
<section id="tell-how-many-gpus-do-you-need-in-5-secs" class="level3">
<h3 class="anchored" data-anchor-id="tell-how-many-gpus-do-you-need-in-5-secs">Tell how many GPUs do you need in 5 secs</h3>
<ul>
<li><p>Training in half mixed-precision: <code>model_size_in_B * 18 * 1.25 / gpu_size_in_GB</code></p></li>
<li><p>Inference in half precision: <code>model_size_in_B * 2 * 1.25 /  gpu_size_in_GB</code></p></li>
</ul>
<p>That’s the minimum, more to have a bigger batch size and longer sequence length.</p>
<p>Here is the breakdown:</p>
<ul>
<li><p>Training: 8 bytes for AdamW states, 4 bytes for grads, 4+2 bytes for weights</p></li>
<li><p>Inference: 2 bytes for weights (1 byte if you use quantization)</p></li>
<li><p>1.25 is 25% for activations (very very approximate)</p></li>
</ul>
<p>For example: Let’s take an 80B param model and 80GB GPUs and calculate how many of them we will need for:</p>
<ul>
<li>Training: at least 23 GPUs <code>80*18*1.25/80</code></li>
<li>Inference: at least 3 GPUs <code>80*2*1.25/80</code></li>
</ul>
<p><a href="../training/performance/README.md#anatomy-of-models-memory-usage">More details</a>.</p>
</section>
</section>
<section id="traps-to-be-aware-of" class="level2">
<h2 class="anchored" data-anchor-id="traps-to-be-aware-of">Traps to be aware of</h2>
<p>As you navigate this very complex AI industry here are some thing to be aware of:</p>
<section id="say-no-to-will-make-a-reasonable-effort-to-contracts" class="level3">
<h3 class="anchored" data-anchor-id="say-no-to-will-make-a-reasonable-effort-to-contracts">Say no to “will make a reasonable effort to …” contracts</h3>
<ul>
<li><p>If you contract doesn’t have clear deliverables (time and performance) don’t be surprised if you paid for something you won’t receive in time you need it or not at all</p></li>
<li><p>Be very careful before you sign a contract that includes clauses that start with “we will make a reasonable effort to …”.</p>
<p>When was the last time you went to the bread section of the supermarket and found a lump of half-baked dough with a note “we made a reasonable effort to bake this bread, but alas, what you see is what you get”?</p>
<p>But for whatever reason it’s acceptable to create a legal contract where the provider provides neither delivery dates nor performance metrics and doesn’t provide stipulations for what will they do in recompense when those promises aren’t fulfilled.</p></li>
</ul>
</section>
<section id="beware-of-hardware-and-software-lock-in-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="beware-of-hardware-and-software-lock-in-scenarios">Beware of hardware and software lock-in scenarios</h3>
<ul>
<li><p>Some cloud providers will make you use very proprietary tools or hardware that will make it very difficult for you to leave down the road because you will have to retool everything if you leave</p></li>
<li><p>Consider what would be the cost of moving to a different provider should this provider prove to be not satisfactory or if they don’t have a capacity to fulfill your growing needs.</p></li>
<li><p>If you rent a cluster with a generic Linux box with generic open source tools it should be trivial to move from one provider to another as almost everything would work out of the box</p></li>
<li><p>Obviously if you choose compute that requires custom software that works for that hardware only and you can’t rent this hardware anywhere else you’re setting yourself up for a lock-in</p></li>
</ul>
</section>
<section id="dont-buy-what-you-dont-really-need" class="level3">
<h3 class="anchored" data-anchor-id="dont-buy-what-you-dont-really-need">Don’t buy what you don’t really need</h3>
<ul>
<li><p>The cloud providers have mostly the same generic hardware, which leads to a very slim $ margin and so in order to make big $ they invent products and then try to convince you that you need to buy them. Sometimes you actually need those products, but very often not. See also the previous section on lock-in, since proprietary products usually mean a partial lock-in.</p></li>
<li><p>Often it’s easy to observe the 3 step marketing technique for solutions that seek a problem to solve:</p></li>
</ul>
<ol type="1">
<li>Convince a couple of well respected customers to use the provider’s proprietary products by giving them huge discounts or even pay them to use them</li>
<li>Use those in step 1 as the social approval lever to reel in more converts</li>
<li>Then scoop the rest of the strugglers by telling them that 80% of your customers (1+2) use these amazing products</li>
</ol>
<p>When marketing these products it’s important: - to mention how well they work with a dozen of other products, since now you’re not buying into a single product but into a whole proprietary product-sphere. - to use really nice looking complicated diagrams of how things plug into each other, and move really fast to the next slide before someone asks a difficult question.</p>
<p>HPCs are probably a good group of compute providers to learn from - they have no funds to create new products and so they creatively address all their needs using mostly generic open source tools with some custom written software added when absolutely needed.</p>
</section>
</section>
<section id="unsolicited-advice" class="level2">
<h2 class="anchored" data-anchor-id="unsolicited-advice">Unsolicited advice</h2>
<p>To conclude I thought I’d share some insights to how one could slightly improve their daily AI battlefield experience.</p>
<section id="fomo-and-avoiding-depression" class="level3">
<h3 class="anchored" data-anchor-id="fomo-and-avoiding-depression">FOMO and avoiding depression</h3>
<p>If you read Twitter and other similar ML-related feeds you’re guaranteed to feel the fear of missing out, since there is probably at least one new great model getting released weekly and multiple papers are getting published daily and your peers will publish their cool achievements hours.</p>
<p>We are dealing with <strong>very</strong> complicated technology and there is a small handful of people who can absorb that much new material and understand / integrate it.</p>
<p>This can be extremely depressing and discouraging.</p>
<p>I deal with it by looking at twitter about once or twice a week. I mostly use Twitter in broadcast mode - that is if I have something to share I post it and only watch for possible follow up questions.</p>
<p>Usually all the important news reach me through other people.</p>
</section>
<section id="dont-try-to-know-everything" class="level3">
<h3 class="anchored" data-anchor-id="dont-try-to-know-everything">Don’t try to know everything</h3>
<p>The pace of innovation in the field of AI is insane. It’s not possible to know all-things-AI. I’d dare to say it’s not possible to know even 10% of it for most of us.</p>
<p>I realized this very early one and I stopped paying attention to most announcements, tutorials, keynotes, etc. Whenever I have a new need I research it and I discover what I need and I have to be careful not to try to learn other things not pertinent to the goal at hand.</p>
<p>So I actually know very little, but what I have researched in depth I know quite well for some time and later I forget even that (that’s why I write these notes - so that I can easily find what I have already researched).</p>
<p>So if you ask me something, chances are that I don’t know it, but the saving grace for me is that if you give me time I can figure it out and give the answer or develop a solution.</p>
</section>
<section id="dont-beat-yourself-up-when-using-half-baked-software" class="level3">
<h3 class="anchored" data-anchor-id="dont-beat-yourself-up-when-using-half-baked-software">Don’t beat yourself up when using half-baked software</h3>
<p>Because the ML field is in a huge race, a lot of the open source software is half-baked, badly documented, badly tested, at times poorly supported. So if you think you can save time by re-using software written by others expect spending hours to weeks trying to figure out how to make it work. And then keeping it working when the updates break it.</p>
<p>The next problem is that most of this software depends on other software which often can be just as bad. It’s not uncommon where I start fixing some integration problem, just to discover a problem in a dependent package, which in its turn has another problem from another package. This can be extremely frustrating and discouraging. Once excepts to save time by reuse, but ends up spending a long time figuring out how to make it work. At least if I write my own software I have fun and it’s a creative process, trying to make other people’s software work is not.</p>
<p>So at the end of the day we are still better off re-using other people’s software, except it comes at an emotional price and exhaustion.</p>
<p>So first of all, try to find a way not to beat yourself up if the software you didn’t write doesn’t work. If you think about it, those problems aren’t of your creation.</p>
<p>Learning how to <a href="https://github.com/stas00/the-art-of-debugging/tree/master/methodology">debug efficiently</a> should also make this process much less painful.</p>
</section>
</section>
<section id="contributors" class="level2">
<h2 class="anchored" data-anchor-id="contributors">Contributors</h2>
<p><a href="https://github.com/msaroufim">Mark Saroufim</a>,</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bekman2024,
  author = {Bekman, Stas and Foreman, Sam},
  title = {ML {Engineering}},
  date = {2024-02-20},
  url = {https://saforem2.github.io/ml-engineering},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bekman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Bekman, Stas, and Sam Foreman. 2024. <span>“ML Engineering.”</span>
February 20, 2024. <a href="https://saforem2.github.io/ml-engineering">https://saforem2.github.io/ml-engineering</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a href="https://saforem2.github.io/ml-engineering">ML-Engineering</a></p>
</div>   
    <div class="nav-footer-center">
<p>2024</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/saforem2/ml-engineering/blob/main/old/ai-battlefield-old.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/saforem2/ml-engineering/edit/main/old/ai-battlefield-old.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/saforem2/ml-engineering/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p><a href="https://github.com/saforem2/ml-engineering"><i class="fa-brands fa-github" aria-label="github"></i></a></p>
</div>
  </div>
</footer>




</body></html>